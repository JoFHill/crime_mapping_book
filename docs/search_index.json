[["index.html", "Crime Mapping in R Prelude 0.1 Introduction 0.2 Disclaimer", " Crime Mapping in R Reka Solymosi and Juanjo Medina 2021-03-15 Prelude 0.1 Introduction This workbook contains the lab materials for our Crime Mapping module in Department of Criminology at the University of Manchester. This module is an optional unit open to 3rd year undergraduate and postgraduate students. It makes use of R, as it is a free, open source tool, that has tremendous community support, and great versatility in mapping applications. You can find more details about the advantages of R for geospatial work here Crime Mapping introduces students to the concepts of spatial data analysis. The aim is to familiarise students with basic concepts of GIS, and get acquainted with spatial statistics to be able to talk about data about crime, policing, and criminal justice topics situated in the places they occur. Details can be found in the Syllabus. 0.2 Disclaimer Please beware that: In making these notes, while we briefly cover some concepts, students are expected to do the weekly reading, and attend the weekly lectures, as well as participate in lab disucssions to receive a complete course experience. These notes are not intended to be a stand-alone reference or textbook, rather a set of exercises to gain hands-on practice with the concepts introduced during the course. These pages are the content of the university course mentioned above. They are meant to (very gently) introduce students to the concept of spatial data analysis, and cover descriptive statsitics and the key concepts required to build an understanding of quantitative data analysis in crime research. The handouts below use, among other data sets, dara from the UK data service such as the Crime Survey for England and Wales that is available under a Open Government Licence. This dataset is designed to be a learning resource and should not be used for research purposes or the production of summary statistics. "],["a-first-lesson-about-r.html", "Chapter 1 A first lesson about R 1.1 Install R &amp; R Studio 1.2 Open up and explore RStudio 1.3 Customising the RStudio look 1.4 Packages 1.5 Exploring data 1.6 Getting organised: R Projects", " Chapter 1 A first lesson about R In this lesson, you will be introduced to the programming language, R. After installing the related software and getting a basic idea of the R Studio interface, you will learn three R basics: operators, objects, and packages. Unlike other statistical software like SPSS and STATA, R is a free, open-source software for performing statistical analysis and data visualization. In addition, R offers more analytical solutions, flexibility, and customization than these commonly used statistical software, and its popularity has increased substantially over the years. We learn R because we hope that this is an important tool that you will continue to use in future. As it is free and has a community feel to it where anyone can create and upload new techniques, the idea is that you can use R long after this course. Even if data analysis is not in the future for you, learning how to conduct and interpret statistical output is a good skill to have – much of our knowledge of the world includes statistics, so understanding the numbers and how they were derived are advantages. R uses a language called object-oriented programming, and though it may seem daunting at first, practice makes familiarity. Also, you can impress your friends with all your coding. 1.1 Install R &amp; R Studio As R and R Studio are free software, you should be able to install these on your own machines at home. You may be working with different IT, so there are different options to allow a successful install. Our first activity will be to decide what approach to working with R and R Studio will be best for you. 1.1.1 Activity 1: Identifying your operating system In this activity, you need to answer a question about your computer/IT that you will be working with for this class. That question is: What is your operating system? Operating system refers to the software that your computer is running to deliver the basic functions. You may have, for example: Windows or Linux - if you have these, you are most likely going to have an easy time installing R and R Studio, so you should give the installation instructions below a try Apple - if you have a Mac, there are some extra steps to install R and R Studio. Specifically, there will be an additional programme to download called Xcode, and additional steps to take. Chromebook - Installing R and R Studio on a Chromebook involves installing Linux. Like with a Mac, there are some additional steps you will need to take, and some patience. In your group google sheets, write down which operating system you have. This will guide which directions to follow later. 1.1.2 Activity 2: Install R &amp; R Studio 1.1.2.1 Some notes specific to your operating system. Before you move on to the installation steps, look at your answer from Activity 1, and read or watch the advice specific to your operating system: Windows: click here for instructions Chromebook: read the tutorial here Mac, follow the guidance in the video here and then, you will also need to install command line tools, for that you can watch another video here Linux: for ubuntu see the video here and if you have questions, let the teaching team know! Once you have watched or read the instructions for your relevant operating system, you are now ready to actually have a go at downloading the software for yourself. Before you start, write in the google doc any questions or concerns, and once ready, install! Install R: Go to https://www.r-project.org/ Click the download R link under the Getting Started heading You will be asked to select a Comprehensive R Archive Network (CRAN) mirror. Click the URL closest to your location Click whichever download link is appropriate for your operating system (see above). Then click the install R for the first-time link and proceed to install R Install R Studio: Go to https://rstudio.com/ Click the Download link Scroll down and then click the DOWNLOAD button for the free version of RStudio You will be taken to a website to download the free version of RStudio that is appropriate for your computer. Download and then install it. 1.1.2.2 Plan B: accessing R Studio remotely throught a web browser It might be that the above does not work and you find that there are some issues specific to your computer, or something just is not working. In that case, you have two options: Option 1: You can remotely access one of the university PCs from your browser (Firefox, Chrome, Safari, etc). You can find instructions how to do this here , and the university IT helpdesk can help you access this too. If you do this, you will be able to use the version of RStudio installed in the computer clusters. Option 2: You can access an online version of R Studio, which you can access through any web browser (Firefox, Chrome, Safari, etc). To do this, you visit https://rstudio.cloud/, click on ‘get started for free,’ choose the free account, and click on ‘sign up.’ Then you can always visit this website and log in to use R Studio in the cloud. Note that you should start a New Project and name it Modelling Crime Data, and then all your work will be saved in this project. 1.2 Open up and explore RStudio In this session we will focus in developing basic familiarity with R Studio. You can use R without using R Studio, but R Studio is an app that makes it easier to work with R. R Studio is what we call an IDE, an integrated development environment. It is a fancy way of saying that it is a cool interface designed to write programming code. Every time you open up R Studio you are in fact starting a R session. R Studio automatically runs R in the background. We will be interacting with R in this course unit via R Studio. When you first open R Studio, you will see (as in the image above) that there are 3 main windows. The bigger one to your left is the console. If you read the text in the console you will see that R Studio is indeed opening R and you can see what version of R you are running. Since R is constantly being updated the version you installed is likely more recent that the one we used at time of writing. 1.2.0.1 Activity 3: Opening up the script pane The view in R Studio is structured so that you have 4 open windows in a regular session. However when you first open, you might be starting with only 3. To open the script pane (the one missing) click in the File drop down Menu, select New File, then R Script. You will now see the 4 window areas in display. On each of these areas you can shift between different views and panels. You can also use your mouse to re-size the different windows if that is convenient. 1.2.1 The four panes of R Studio The purposes of the four panes in Figure above are the following: Script and data view: where you type your programming code that tells R what you want to do. These are essentially instructions that you type and save as a script, so that you can return to it later to remember what you did and to share it with others so that they can reproduce what you did. Environment and history view: 2.1 Environment tab- gives you the names of all the (data) objects that you have defined during your current R session, including number of observations and rows in those objects. We learn more about objects later. 2.2 History tab- shows you a history of all the code you have previously evaluated in the main console. One of the key advantages of doing data analysis this way - with code versus with a point and click interface like Excel or ArcGIS is that you are producing a written record of every step you take in the analysis. First time around it will take you time to write these instructions, it may be slower than pointing and clicking. And unlike with pointing and clicking you need to know the “words” and “grammar” of this language. Main console: this is considered R's heart, and it is where R evaluates the codes that you run. You can type your codes directly in the console, but for the sake of good habits, type them in the script and data view so you can save a record of them. Only type and run code from here if you want to debug or do some quick analysis. File directory, Plots, Packages, Help: 4.1 Files tab- allows you to see the files in the folder that is currently set as your working directory. 4.2 Plots tab- you will see any data visualizations that you produce here. You have not produced any yet, so it is empty now. 4.3 Packages tab- you will see the packages that are currently available to install. We will explain what these are soon, but know that they are an essential feature when working with R. 4.4 Help tab- you can access further information on the various packages. 1.2.1.1 Activity 4: Interacting with the 4 panes In the previous activity, you opened up the ‘script’ pane. We now write some code in it, and see what happens. To do this, go to your open version of R Studio, and type in the script pane the following: print(&quot;Hello world!&quot;) When you have typed this, you will have typed your first bit of code. Yet nothing is happening? That is because you also have to RUN the code. You can do this by highlighting the code you wish to run, and clicking on ‘run’ in the top right hand corner: knitr::include_graphics(&#39;img/run_hello_world.gif&#39;) When you ‘run’ the code, it will print the text ‘Hello World!’ in the bottom pane, which is the console. That means you have written and executed your first line of code. In the rest of the session, we will be unpacking how this all works, and getting more familiar and comfortable with using R Studio. To recap: the script is where you write your programming code. A script is nothing but a text file with some code on it. Unlike other programs for data analysis you may have used in the past (Excel, SPSS), you need to interact with R by means of writing down instructions and asking R to evaluate those instructions. R is an interpreted programming language: you write instructions (code) that the R engine has to interpret in order to do something. And all the instructions we write can and should be saved in a script, so that you can return later to what you did. One of the key advantages of doing spatial data analysis this way - with code versus with a point and click interface like ArcGIS or MapInfo (or even QGIS) is that you are producing a written record of every step you take in the analysis. First time around it will take you time to write these instructions, it may be slower than pointing and clicking. And unlike with pointing and clicking you need to know the “words” and “grammar” of this language. The advantage of doing analysis this way is that once you have written your instructions and saved them in a file, you will be able to share it with others and run it every time you want in a matter of seconds. This creates a reproducible record of your analysis: something that your collaborators or someone else anywhere (including your future self, the one that will have forgotten how to do the stuff) could run and get the same results than you did at some point earlier. This makes science more transparent and transparency brings with it many advantages. For example, it makes your research more trustworthy. Don’t underestimate how critical this is. Reproducibility is becoming a key criteria to assess good quality research. The University of Manchester just joined the UK Reproducibility Network, and is promoting this good practice. If you’re interested you can join the Open Science working group or the reading club ReproducibiliTea. You can read up on reproducibility and its importance here or get Stuart Richie’s book Science Fictions for an engaging further read. 1.3 Customising the RStudio look R Studio allows you to customise the way it looks. Working with white backgrounds is not generally a good idea if you care about your eyesight. If you don’t want to end up with dry eyes not only it is good you follow the 20-20-20 rule (every 20 minutes look for 20 seconds to an object located 20 feet away from you), but it may also be a good idea to use more eye friendly screen displays. Click in the Tools menu and select Global options. This will open up a pop up window with various options. Select Appearance. In this section you can change the font type and size, but also the kind of theme background that R will use in the various windows. You can make any changes you’d like to here, including the theme background that R will use as the interface. For example, you may choose a darker theme like ‘tomorrow night bright.’ As indicated above, the window in the bottom left corner is the main console.You will see that the words “I love maps” appear printed there. If rather than using R Studio you were working directly from R, that’s all you would get: the main console where you can write code interactively (rather than all the different windows you see in R Studio). You can write your code directly in the main console and execute it line by line in an interactive fashion. However, we will be running code from scripts, so that you get used to the idea of properly documenting all the steps you take, 1.3.1 Functions Functions do things. They are called by a certain name, usually a name which represents what they do, and they are followed by brackets (). Within the brackets, you can put whatever it is that you want the function to work with. For example, the code we wrote in Activity 4 was the print() function. This function told R to print into the console whatever we put in the brackets (“Hello World!”). Same idea with a personalised greeting: if you want to print ‘Hello Reka,’ you will need to have “Hello Reka” inside the brackets: print(&quot;Hello Reka&quot;) ## [1] &quot;Hello Reka&quot; There are so many functions in R. We will be learning many of them throughout our class. Print is fun, but most of the time, we will be using functions to help us with our data analysis. For example, getting the minimum, maximum, or mean of a list of numbers. R does this using functions in a very similar way. For example, if we have a bunch of numbers, we just find the appropriate function to get the summary we want: mean(10, 34, 5, 3, 77) ## [1] 10 min(10, 34, 5, 3, 77) ## [1] 3 max(10, 34, 5, 3, 77) ## [1] 77 How can you find the function you need? Throughout this class, you will learn a list that appears at the top of each lesson. A recommendation is to also create a ‘function cookbook,’ where you write down a list of functions, what the functions do, and some examples. Here is an example: You can use google to make your cookbook, and the website stackoverflow, in particular, can help you find the function you need. But be wary, especially in the beginning, that you understand what the function does. There can be several different functions for the same action. One good approach is to add a function of interest to your cookbook and ask the teaching team about what it does, and how it might be different to other functions that do the same thing. 1.3.1.1 Activity 5: Play around with functions Have a guess (or google) about what you think is the function to get the median. Once you have your answer, write it in the shared google docs. Then, use it to get the median of the numbers 10, 34, 5, 3, 77. Write the answer in your shared google doc (or note it down for yourself if in the quiet room). The answer is further below, after the note: NOTE: R is case-sensitive! For example: # Calculating the logarithm Log(100) # ERROR! # Instead, it should be: log(100) ## [1] 4.60517 Okay, now you know these, the answer to Activity 6 was… median(10, 34, 5, 3, 77) ## [1] 10 Now let us move on to our second key topic: objects! 1.3.2 Objects Everything that exists in R is an object. Think of objects as boxes where you put things in. Imagine a big, empty cardboard box. We can create this big empty box in R by simply giving it a name. Usually, you want your object/box to have a good descriptive name, which will tell people what is in it. Imagine moving house. If you have a cardboard box full of places, you might want to label it “plates.” That way, when carrying, you know to be careful, and when unpacking, you know its contents will go in the kitchen. On the other hand, if you named it “box1,” then this is a lot less helpful when it comes to unpacking. 1.3.2.1 Activity 6: Creating an object Let us create an object called ‘plates.’ To do this, you go to your script, and type ‘plates.’ But if you run this code, you will get an error. Let’s see: knitr::include_graphics(&#39;img/error_no_plates.gif&#39;) You see the error ‘Error! Object plates not found.’ This is because you have not yet put anything inside the plates ‘box.’ Remember objects are like boxes,so there must be something inside our object ‘plates.’ In order for this object to exist, you have to put something inside it, or in R-speak assign it some value. Therefore, we make an object by using an assignment operator ( &lt;- ). In other words, we assign something to an object (i.e., put something in the box). For example: plates &lt;- &quot;yellow plate&quot; Now if we run this, we will see no error message, but instead, we will see the plates object appear in our environment pane: knitr::include_graphics(&#39;img/assign_plates.gif&#39;) Here are some more examples to illustrate: # Putting &#39;10&#39; in the &#39;a&#39; box a &lt;- 10 # Putting &#39;Hello!&#39; in the &#39;abc123&#39; box abc123 &lt;- &quot;Hello!&quot; In these examples, we are putting the value of 10 into the object a, and the value of ‘Hello!’ into the object abc123. Earlier, we introduced you to the Environment and History pane. We mentioned that it lists objects you defined. After making the ‘a’ and ‘abc123’ objects, they should appear in that very pane under the Environment tab. 1.3.2.2 Types of objects Why are objects important? We will be storing everything in our data analysis process in these objects. Depending on what is inside them, they can become a different type of object. Here are some examples: Data structures are important objects that store your data, and there are five main types but we focus on three for this course: (atomic) vector: an ordered set of elements that are of the same class. Vectors are a basic data structure in R. Below are five different classes of vectors: # 1. numeric vector with three elements my_1st_vector &lt;- c(0.5, 8.9, 0.6) # 2. integer vector with addition of L at the end of the value my_2nd_vector &lt;- c(1L, 2L, 3L) # 3. logical vector my_3rd_vector &lt;- c(TRUE, FALSE, FALSE) # &#39;my_4th_vector&#39; creates a logical vector using abbreviations of True and False, but you should use the full words instead my_4th_vector &lt;- c(T, F) # 4. character vector my_5th_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # 5. complex vector (we will not use this for our class) my_6th_vector &lt;- c(1+0i, 2+4i) lists: technically they, too, are vectors but they are more complex because they are not restricted on the length, structure, or class of the included elements. For example, to create a list containing strings, numbers, vectors and a logical, use the list() function, and inside the brackets, put everything tat you want to combine into a list: list_data &lt;- list(&quot;teal&quot;, &quot;sky blue&quot;, c(10, 5, 10), TRUE, 68.26, 95.46, 99.7) Above, we created list_data, an object that contains all those things that we put inside the list() function. This function serves to create a list from combining everything that is put inside its brackets. Use the class() function to confirm that the objects have been defined as a list class(list_data) ## [1] &quot;list&quot; data frames: also stores elements but differ from lists because they are defined by their number of columns and rows; the vectors (columns) must be of the same length. Data frames can contain different classes but each column must be of the same class. For example, if you want to combine some related vectors to make a data frame on violent American cities, use the function data.frame(): # Making some relevant vectors TopVioCities &lt;- c(&quot;St. Louis&quot;, &quot;Detroit&quot;, &quot;Baltimore&quot;) # some violent US cities VioRatePer1k = c(20.8, 20.6, 20.3) # their violence rates per 1,000 persons State &lt;- c(&quot;Missouri&quot;, &quot;Michigan&quot;, &quot;Maryland&quot;) # in what states are these cities found #Join them to make a data frame called &#39;df&#39; df&lt;-data.frame(TopVioCities, VioRatePer1k, State) We can then view the data frame, ‘df,’ with the View() function: View(df) 1.3.2.3 Activity 7: Doing things to objects We have learned what functions are (i.e., things that do things) and what are objects (i.e., the boxes that hold things). We also saw some functions which helped us create objects. Functions can also do things to objects. For example, we saw the function class() that told us about what kind of object list_data was, and View() which allowed us to have a look at our dataframe we called df. Let us look back at our plates object. Remember it was the object that held our kitchen items. We added ‘yellow plate’ to it. Now let us add some more items and let us use the concatenate c() function for this again: plates &lt;- c(&quot;yellow plate&quot;, &quot;purple plate&quot;, &quot;silver plate&quot;, &quot;orange bowl&quot;) Let us say that we suddenly forgot what was in our object called ‘plates.’ Like what we learned earlier, we use the function print() to see what is inside this object: print(plates) ## [1] &quot;yellow plate&quot; &quot;purple plate&quot; &quot;silver plate&quot; &quot;orange bowl&quot; This can apply to obtaining the mean, the minimum, and maximum. You could assign those statistics to an object this time: nums &lt;- c(10, 34, 5, 3, 77) Now if we want to know the mean, we can take the mean of the object nums, which we just created: mean(nums) ## [1] 25.8 The object we will use most frequently though is data frames. These hold your data in a format whereby each column represents a variable, and each row an observation. Just earlier, we had created a dataframe called df previously. If you have not yet copied this over into your own R Studio, do this now. You should have the object df in your environment. When you run View(df), you should see this dataset: To do something to an entire dataframe, we would use the name of the object (df) to refer to it. In the case of the View() function, we want to see the whole thing, so we will call View(df). On the other hand, if we want to refer to only one variable in the data, (remember back to term 1 - each varible is held in each column) there is a special notation to do this. To refer to a variable (column) inside a dataframe, you use: \\(dataframe name + \\$ + variable name\\) For example, to refer to the variable VioRatePer1k, we use the notation df$VioRatePer1k. And if we wanted to View only that column, we use: View(df$VioRatePer1k) You should see: Say we wanted to know the mean violence rate across our units of analysis, the cities, for example, we would take the numeric column to calculate this: mean(df$VioRatePer1k) ## [1] 20.56667 1.4 Packages Packages are a very important element of R. Packages are elements that add the functionality of R. What most packages do is they introduce new functions that allow you to ask R to do new different things. Anybody can write a package, so consequently R packages vary on quality and complexity. You can find packages in different places, as well, from official repositories (which means they have passed a minimum of quality control), something called Git Hub (a webpage where software developers post work in progress), to personal webpages (danger danger!). Throughout the course, and hopefully afterwards, you will find yourself installing numerous open source software packages that allow R to do new and different things. There are loads of packages out there. In early 2020, there were over 150,000 packages available. Anyone can write one, so you will need to be careful on which ones you use as the quality can vary. Official repositories, like CRAN, are your best bet for packages as they will have passed some quality controls. You can see what packages are available in your local install by looking at the packages tab in the File directory, Plots, Packages pane. A number of the packages we will use belong to a set of packages called tidyverse. These packages help make your data tidy. According to Statistician and Chief Scientist at RStudio, Hadley Wickham, transforming your data into tidy data is one of the most important steps of the data analysis process. It will ensure your data are in the format you need to conduct your analyses. We will also be using the simple features package sf and many more associated with spatial data analysis. Packages can be installed using the install.packages() function. Remember that while you only need to install packages once, they need to be loaded with the library()function each time you open up RStudio. Let us install the package dplyr from tidyverse and load it: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union A lot of code and activity appears in the console. Warnings may manifest. Most of the time, the warnings explain what is being loaded and confirm that the package is successfully loaded. If there is an error, you will have to figure out what the warnings are telling you to successfully load the package. This happens and is normal. To double check that you have actually installed dplyr, go to that File Directory, Plots, Packages pane and click on the Packages tab. The list of packages is in alphabetical order and dplyr should be there. If there is a tick in its box, it means that this package is currently loaded and you can use it; if there is no tick, it means that it is inactive, and you will have to bring it up with library(), or just tick its box (Figure 1.6). On masking: sometimes packages introduce functions that have the same name as those that are already loaded into your session. When that happens, the newly loaded ones will override the previous ones. You can still use them but you will have to refer to them explicitly by bringing them up by specifying to which package they belong with library(). How do you find out what a package does? You look at the relevant documentation. In the Packages window scroll down until you find the new package we installed listed. Here you will see the name of the package (dplyr), a brief description of what the program is about, and the version you have installed (an indication that a package is a good package is that it has gone through several versions, that means that someone is making sure the package gets regular updates and improvements). Click in the name dplyr. You will see that R Studio has now brought you to the Help tab. Here is where you find the help files for this package, including all the available documentation. Every beginner in R will find these help files a bit confusing. But after a while, their format and structure will begin to make sense to you. Click where it says User guides, package vignettes, and other documentation. Documentation in R has become much better since people started to write vignettes for their packages. They are little tutorials that explain with examples what each package does. Click in the cowsay::cowsay_tutorial that you see listed here (the html link). What you will find there is an html file that gives you a detailed tutorial on this package. You don’t need to read it now, but remember that this is one way to find help when using R. You will learn to love vignettes. 1.5 Exploring data Now that we know the basic component, let’s play around with using R as we will throughout the course, for some data analysis. We will get some data by installing a package which has data in it as well as functions, and then go on to produce some basic summaries. This should give some practice! 1.5.1 Activity 8: Playing around with data We are going to look at some data that are part of the fivethirtyeight package. This package contains data sets and code behind the stories in this particular online newspaper. This package is not part of the base installation of R, so you will need to install it first. I won’t give you the code for it. See if you can figure it out by looking at previous examples. Discuss and write in the google doc what you think the code will be. Done? Ok, now we are going to look at the data sets that are included in this package. Remember first we have to load the package if we want to use it: library(&quot;fivethirtyeight&quot;) ## Some larger datasets need to be installed separately, like senators and ## house_district_forecast. To install these, we recommend you install the ## fivethirtyeightdata package by running: ## install.packages(&#39;fivethirtyeightdata&#39;, repos = ## &#39;https://fivethirtyeightdata.github.io/drat/&#39;, type = &#39;source&#39;) data(package=&quot;fivethirtyeight&quot;) #This function will return all the data frames that are available in the named package. Notice that this package has some data sets that relate to stories covered in this newspaper that had a criminological angle. Let’s look for example at the hate_crimes data set. How do you that? First we have to load the data frame into our global environment. To do so use the following code: data(&quot;hate_crimes&quot;) This function will search among all the loaded packages and locate the hate_crimes data set. Notice that it now appears in the global environment, although it also says “promise” next to it. To see the data in full you need to do something to it first. So let’s do that. Every object in R can have attributes. These are: names; dimensions (for matrices and arrays: number of rows and columns) and dimensions names; class of object (numeric, character, etc.); length (for a vector this will be the number of elements in the vector); and other user-defined. You can access the attributes of an object using the attributes() function. Let’s query R for the attributes of this data frame. attributes(hate_crimes) ## $row.names ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ## [51] 51 ## ## $class ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## ## $names ## [1] &quot;state&quot; &quot;state_abbrev&quot; ## [3] &quot;median_house_inc&quot; &quot;share_unemp_seas&quot; ## [5] &quot;share_pop_metro&quot; &quot;share_pop_hs&quot; ## [7] &quot;share_non_citizen&quot; &quot;share_white_poverty&quot; ## [9] &quot;gini_index&quot; &quot;share_non_white&quot; ## [11] &quot;share_vote_trump&quot; &quot;hate_crimes_per_100k_splc&quot; ## [13] &quot;avg_hatecrimes_per_100k_fbi&quot; This prints out the row names (not very exciting here..) the class (see above when we used class() function) and the names, which are the column headers - or the names of the variables within this data set. You can see there are things like state, and share who voted trump (in the 2016 election, not most recent!). Now use the View() function to glance at your data frame. What you get there is a spreadsheet with 12 variables and 51 observations. Each variable in this case is providing you with information (demographics, voting patterns, and hate crime) about each of the US states. Ok, let’s now have a quick look at the data. There are so many different ways of producing summary stats for data stored in R that is impossible to cover them all! We will just introduce a few functions that you may find useful for summarising data. Before we do any of that it is important you get a sense for what is available in this data set. Go to the help tab and in the search box input the name of the data frame, this will take you to the documentation for this data frame. Here you can see a list of the available variables. Let’s start with the mean. This function takes as an argument the numeric variable for which you want to obtain the mean. You have done this above, so it should be familiar now! If you want to obtain the mean of the variable that gives us the proportion of people that voted for Donald Trump you can use the following expression: mean(hate_crimes$share_vote_trump) ## [1] 0.49 Another function you may want to use with numeric variables is summary(): summary(hate_crimes$share_vote_trump) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.040 0.415 0.490 0.490 0.575 0.700 This gives you the five number summary (minimum, first quartile, median, third quartile, and maximum, plus the mean and the count of missing values if there are any). You don’t have to specify a variable you can ask for these summaries from the whole data frame: summary(hate_crimes) ## state state_abbrev median_house_inc share_unemp_seas ## Length:51 Length:51 Min. :35521 Min. :0.02800 ## Class :character Class :character 1st Qu.:48657 1st Qu.:0.04200 ## Mode :character Mode :character Median :54916 Median :0.05100 ## Mean :55224 Mean :0.04957 ## 3rd Qu.:60719 3rd Qu.:0.05750 ## Max. :76165 Max. :0.07300 ## ## share_pop_metro share_pop_hs share_non_citizen share_white_poverty ## Min. :0.3100 Min. :0.7990 Min. :0.01000 Min. :0.04000 ## 1st Qu.:0.6300 1st Qu.:0.8405 1st Qu.:0.03000 1st Qu.:0.07500 ## Median :0.7900 Median :0.8740 Median :0.04500 Median :0.09000 ## Mean :0.7502 Mean :0.8691 Mean :0.05458 Mean :0.09176 ## 3rd Qu.:0.8950 3rd Qu.:0.8980 3rd Qu.:0.08000 3rd Qu.:0.10000 ## Max. :1.0000 Max. :0.9180 Max. :0.13000 Max. :0.17000 ## NA&#39;s :3 ## gini_index share_non_white share_vote_trump hate_crimes_per_100k_splc ## Min. :0.4190 Min. :0.0600 Min. :0.040 Min. :0.06745 ## 1st Qu.:0.4400 1st Qu.:0.1950 1st Qu.:0.415 1st Qu.:0.14271 ## Median :0.4540 Median :0.2800 Median :0.490 Median :0.22620 ## Mean :0.4538 Mean :0.3157 Mean :0.490 Mean :0.30409 ## 3rd Qu.:0.4665 3rd Qu.:0.4200 3rd Qu.:0.575 3rd Qu.:0.35694 ## Max. :0.5320 Max. :0.8100 Max. :0.700 Max. :1.52230 ## NA&#39;s :4 ## avg_hatecrimes_per_100k_fbi ## Min. : 0.2669 ## 1st Qu.: 1.2931 ## Median : 1.9871 ## Mean : 2.3676 ## 3rd Qu.: 3.1843 ## Max. :10.9535 ## NA&#39;s :1 There are multiple ways of getting results in R. Particularly for basic and intermediate-level statistical analysis many core functions and packages can give you the answer that you are looking for. For example, there are a variety of packages that allow you to look at summary statistics using functions defined within those packages. You will need to install these packages before you can use them. I am only going to introduce one of them here skimr. You will need to install it before anything else. Once you have loaded the skimr package you can use it. Its main function is skim. Like summary for data frames, skim presents results for all the columns and the statistics will depend on the class of the variable. skim(hate_crimes) Hopefully in your statistical modules you had taken previously, you have learned some things about how to graphically display variables. So you may have some memory about the amount of work involved with this. Hopefully R will offer some respite. Of course, there are many different ways of producing graphics in R. In this course we rely on a package called ggplot2, which is part of the tidyverse set of packages mentioned earlier. library(ggplot2) Then we will use one of its functions to create a scatterplot. ggplot(hate_crimes, aes(x=share_vote_trump, y=avg_hatecrimes_per_100k_fbi)) + geom_point(shape=1) + geom_smooth(method=lm) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). What do you think this graphic is telling you? Discuss and write in your shared google docs. Graphing is very powerful in R, and much of the spatial visualisation we will produce throughout the module will build on this. If you are not already familiar with this, I recommend a read of the data visualisation chapter of R for Data Science https://r4ds.had.co.nz/data-visualisation.html 1.6 Getting organised: R Projects One thing that can help you tremendously throughout this module is keeping your code organised.R Studio helps with this by virtue of something called R Projects. Technically, a R Studio project is just a directory with the name of the project, and a few files and folders created by R Studio for internal purposes. This is where you should hold your scripts, your data, and reports. You can manage this folder with your own operating system manager (discussed earlier, e.g., Windows) or through the R Studio file manager (that you access in the bottom right corner set of windows in R Studio). When a project is reopened, R Studio opens every file and data view that was open when the project was closed last time around. Trust me, this is a really helpful thing! If you create a project for this module, you can keep everything in once place, and refer back to your old code and your learnings throughout the module. 1.6.0.1 Activity 9: Saving your work and projects First things first, hopefully you have already created a separate folder on your desktop, or dropbox, or something like this. Now save the script you’ve been working on into this folder. By clicking on “File” and “Save as….” Then navigate to your folder for this module, and for your script make sure to give it some meaningful name like week1lab.R or something like this. Then click ‘save.’ Now, go back to “File” and select “New project…” Then in the options that appear choose “Existing Directory.” This is because you already have a folder for this work, this is where you saved your script just before. For me this was my folder called ‘crime_mapping’ you saw above. So select ‘Existing Directory,’ and on the next page use the “Browse” button to select this folder (the directory) where you saved the script earlier. Once you have done this, click on ‘Create Project’ on the bottom. This will now open up a new R Studio window with your project. In the future, you can start right back up where you finished last time by navigating to the .Rproj file, and double clicking it. It helps you keep everything in one place, and lets R read everything from that folder. With simple projects a single script file and a data file is all you may have. But with more complex projects, things can rapidly become messy. So you may want to create subdirectories within this project folder. I typically use the following structure in my own work to put all files of a certain type in the same subdirectory: Scripts and code: Here I put all the text files with my analytic code, including rmarkdown files which is something we will introduce much later in the semester. Source data: Here I put the original data. I tend not to touch this once I have obtained the original data. Documentation: This is the subdirectory where I place all the data documentation (e.g., codebooks, questionnaires, etc.) Modified data: All analysis involve doing transformations and changing things in the original data files. You don’t want to mess up the original data files, so what you should do is create new data files as soon as you start changing your source data. I go so far as to place them in a different subdirectory. Literature: Analysis is all about answering research questions. There is always a literature about these questions. I place the relevant literature for the analytic project I am conducting in this subdirectory. Reports and write up: Here is where I file all the reports and data visualisations that are associated with my analysis. If you come to my office, you will see it is a very messy place. But my computer is, in contrast, a very tidy environment. You should aim for your computer workspace to be very organised as well. You can read up here on why projects are useful here: https://www.r-bloggers.com/2020/01/rstudio-projects-and-working-directories-a-beginners-guide/ "],["making-maps-in-r.html", "Chapter 2 Making Maps in R 2.1 A quick introduction of terms 2.2 Getting some spatial data to put on a map 2.3 From dataframes to spatial objects: finding spatial information in our data 2.4 Putting crime on the map - simple features 2.5 Summary", " Chapter 2 Making Maps in R This week we will start making some maps in R, and learn about how we can take regular crime data, and assign the appropriate geometry for our chosen unit of analysis. We will produce some maps, using possibly familiar ggplot notation, and learn some key terms around projectsion and coordinate reference systems which will be essential for your work in the coming weeks. Today we will use the following packages, so make sure you have them downloaded: reader tibble janitor sf ggplot2 ggspatial dplyr 2.1 A quick introduction of terms 2.1.1 Geospatial Perspective - The Basics Geospatial analysis provides a distinct perspective on the world, a unique lens through which to examine events, patterns, and processes that operate on or near the surface of our planet. Ultimately geospatial analysis concerns what happens where, and makes use of geographic information that links features and phenomena on the Earth’s surface to their locations. We can talk about a few different concepts when it comes to spatial information. These are: Place Attributes Objects 2.1.1.1 Place At the center of all spatial analysis is the concept of place. People identify with places of various sizes and shapes, from the room with the parcel of land, to the neighbourhood, to the city, the country, the state or the nation state. Places often have names, and people use these to talk about and distinguish names. Names can be official. Places also change continually as people move. The basis of rigorous and precise definition of place is a coordinate system, a set of measurements that allows place to be specified unambiguously and in a way that is meaningful to everyone. 2.1.1.2 Attributes Attribute has become the preferred term for any recorded characteristic or property of a place. A place’s name is an obvious example of an attribute. But there can be other pieces of information, such as number of crimes in a neighbourhood, or the GDP of a country. Within GIS the term ‘attributes’ usually refers to records in a data table associated with individual elements in a vector map or cells in a grid (raster or image file). These data behave exactly as data you have encountered in your past experience. The rows represent observations, and the columns represent variables. The variables can be numeric or categorical, and depending on what they are, you can apply different methods to making sense of them. The difference with other kind of data table is that the observations, your rows, correspond to places or locations. 2.1.1.3 Objects In spatial analysis it is customary to refer to places as objects. These objects can be a whole country, or a road. In forestry, the objects of interest might be trees, and their location will be represented as points. On the other hand, studies of social or economic patterns may need to consider the two-dimensional extent of places, which will therefore be represented as areas. These representations of the world are part of what is called the vector data model: A representation of the world using points, lines, and polygons. Vector models are useful for storing data that have discrete boundaries, such as country borders, land parcels, and streets. This is made up of points, lines, and areas (polygons): Points Points are pairs of coordinates, in latitude/longitude or some other standard system Lines Lines are ordered sequences of points connected by straight lines Areas (polygons) Areas are ordered rings of points, also connected by straight lines to form polygons. It can contain holes, or be linked with separate islands. Objects can also be raster data. Raster data is made up of pixels (or cells), and each pixel has an associated value. Simplifying slightly, a digital photograph is an example of a raster dataset where each pixel value corresponds to a particular colour. In GIS, the pixel values may represent elevation above sea level, or chemical concentrations, or rainfall etc. The key point is that all of this data is represented as a grid of (usually square) cells. 2.1.1.4 Networks We already mentioned lines that constitute objects of spatial data, such as streets, roads, railroads, etc. Networks constitute one-dimensional structures embedded in two or three dimensions. Discrete point objects may be distributed on the network, representing phenomena such as landmarks, or observation points. Mathematically, a network forms a graph, and many techniques developed for graphs have application to networks. These include various ways of measuring a network’s connectivity, or of finding the shortest path between pairs of points on a network. You can have a look at the lesson on network analysis in the QGIS documentation 2.1.1.5 Maps: reference and thematic maps Historically maps have been the primary means to store and communicate spatial data. Objects and their attributes can be readily depicted, and the human eye can quickly discern patterns and anomalies in a well-designed map. In GIS we distinguish between reference and thematic maps. A reference map places the emphasis on the location of spatial objects such as cities, mountains, rivers, parks, etc. You use these maps to orient yourself in space and find out the location of particular places. Thematic maps, on the other hand, are about the spatial distribution of attributes or statistics. For example, the number of crimes across different neighbourhouds. Our focus in this book is on thematic maps. 2.1.1.6 Map projections and geographic coordinate systems Whenever we put something into a map we need some sort of system to pinpoint the location. A coordinate system allows you to integrate any dataset with other geographical datasets within a common framework. There are hundreds of them. It is common to distinguish between geographic coordinate systems and projected coordinate systems. A geographic coordinate system is a three dimensional reference system that enables you to locate any location on earth. Often this is done with longitude, latitute and elevation. Projected coordinate systems or map projections, on the other hand, try to portray the surface of the earth or a portion of the earth on a two dimensional flat piece of paper or computer screen. All projections of a sphere like the earth in a two dimensional map involve some sort of distortion. You can’t fit a three dimensional object into two dimensions without doing so. Projections differ to a large extent on the kind of distortion that they introduce. The decision as to which map projection and coordinate reference system to use, depends on the regional extent of the area you want to work in, on the analysis you want to do and often on the availability of data. Knowing the system you use would allow you to translate your data into other systems whenever this may be necessary. Often you may have to integrate data that is provided to you in different coordinate or projected systems. As long as you know the systems, you can do this. [Footnote to this for more detail: https://www.youtube.com/watch?v=6tmDxTAjux0] A traditional method of representing the earth’s shape is the use of globes. When viewed at close range the earth appears to be relatively flat. However when viewed from space, we can see that the earth is relatively spherical. Maps, are representations of reality. They are designed to not only represent features, but also their shape and spatial arrangement. Each map projection has advantages and disadvantages. The best projection for a map depends on the scale of the map, and on the purposes for which it will be used. For your purposes, you just need to understand that essentially there are different ways to flatten out the earth, in order to get it into a 2-dimensional map. The process of creating map projections can be visualised by positioning a light source inside a transparent globe on which opaque earth features are placed. Then project the feature outlines onto a two-dimensional flat piece of paper. Different ways of projecting can be produced by surrounding the globe in a cylindrical fashion, as a cone, or even as a flat surface. Each of these methods produces what is called a map projection family. Therefore, there is a family of planar projections, a family of cylindrical projections, and another called conical projections. figure_projection_families With the help of coordinate reference systems (CRS) every place on the earth can be specified by a set of three numbers, called coordinates. In general CRS can be divided into projected coordinate reference systems (also called Cartesian or rectangular coordinate reference systems) and geographic coordinate reference systems. The use of Geographic Coordinate Reference Systems is very common. They use degrees of latitude and longitude and sometimes also a height value to describe a location on the earth’s surface. The most popular is called WGS 84. This is the one you will most likely be using, and if you get your data in latitude and longitude, then this is the CRS you are working in. It is also possible that you will be using a projected CRS. This two-dimensional coordinate reference system is commonly defined by two axes. At right angles to each other, they form a so called XY-plane. The horizontal axis is normally labelled X, and the vertical axis is normally labelled Y. Working with data in the UK, on the other hand, you are most likely to be using British National Grid (BNG). The Ordnance Survey National Grid reference system is a system of geographic grid references used in Great Britain, different from using Latitude and Longitude. In this case, points will be defined by “Easting” and “Northing” rather than “Longitude” and “Latitude.” It basically divides the UK into a series of squares, and uses references to these to locate something. The most common usage is the six figure grid reference, employing three digits in each coordinate to determine a 100 m square. For example, the grid reference of the 100 m square containing the summit of Ben Nevis is NN 166 712. Grid references may also be quoted as a pair of numbers: eastings then northings in meters, measured from the southwest corner of the SV square. For example, the grid reference for Sullom Voe oil terminal in the Shetland Islands may be given as HU396753 or 439668,1175316 BNG This will be important later on when we are linking data from different projections, or when you look at your map and you try to figure out why it might look “squished.” 2.1.1.7 Density estimation One of the more useful concepts in spatial analysis is density - the density of humans in a crowded city, or the density of retail stores in a shopping centre. Mathematically, the density of some kind of object is calculated by counting the number of such objects in an area, and dividing by the size of the area. To read more about this, I recommend Silverman, Bernard W. Density estimation for statistics and data analysis. Vol. 26. CRC press, 1986. 2.1.2 Summary Right so hopefully this gives you a few things to think about. Be sure that you are confident to know about: Spatial objects - what they are and how they are represented Attributes - the bits of information that belong to your spatial objects Maps and projections - especially what WSG84 and BNG mean, and why it’s important that you know what CRS your data have 2.2 Getting some spatial data to put on a map Alright let’s get some practical experience where we take some crime data, and find out how we can put it on the map! 2.2.1 Find some relevant data to show: obtaining data on crime For your first crime map, we better get some real world crime data. This can be done for the UK easily, as anonymised open crime data are released for the public to use. We can play around with police recorded crime data, which can be downloaded from the police.uk website. Let’s download some data for crime in Manchester. 2.2.2 Activity 1: Get some crime data To do acquire the data, open the data.police.uk/data website. Choose the data tab, in order to download some data. In Date range just select one month of data. Choose whatever month you like EXCEPT nothing more recent than June 2019. Unfortunately, for GMP there is no more recent data available than that. This is because they are having some serious IT issues see this article which apparently have disrupted this flow of data. In Force find Greater Manchester Police, and tick the box next to it. In Data sets tick Include crime data. Finally click on Generate File button. This will take you to a download page, where you have to click the Download now button. This will open a dialogue to save a .zip file. Navigate to the project directory folder you’ve created and save it there, ideally in a subfolder. I call my subfolder ‘data.’ Unzip the file. You can use the unzip function for this. A cool function you may want to use as well is file.choose(). If we pass this function as an argument to unzip(), we will get a pop window where we will be able to select our file using familiar point and click. Ideally, you want to rather write down the path to your file. But sometimes these shortcuts are convenient. unzip(file.choose()) If you look at the Files window in the bottom right corner of RStudio you should see now a new subdirectory that contains a .csv file with the data that we need. Since I downloaded the data from June 2019 in my case this subdirectory is called 2019-06. Before we can use this data we need to read it or import it into R and turn it into a dataframe object. To read in the .csv file, which is the format we just downloaded, the command is read_csv() from the readr package. You’ll need to load this package first. library(readr) Again there are two ways to read in the data, if you want to open a window where you can manually navigate and open the file, you can pass file.choose() argument to the read_csv() function as illustrated earlier. #This code creates a dataframe object called crimes which will include the spreadsheet in the file we have downloaded. In my case, that is 2007-11-greater-manchester-street.csv. crimes &lt;- read_csv(file.choose()) m # Print the map Or, if you know the path to your file, you can code it in there, within quotation marks: crimes &lt;- read_csv(&quot;data/2019-06-greater-manchester-street.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## `Crime ID` = col_character(), ## Month = col_character(), ## `Reported by` = col_character(), ## `Falls within` = col_character(), ## Longitude = col_double(), ## Latitude = col_double(), ## Location = col_character(), ## `LSOA code` = col_character(), ## `LSOA name` = col_character(), ## `Crime type` = col_character(), ## `Last outcome category` = col_character(), ## Context = col_logical() ## ) You might notice that the object crimes has appeared in your work environment window. It will tell you how many observations (rows - and incidentally the number of recorded crimes in June 2019 within the GMP jurisdiction) and how many variables (columns) your data has. Let’s have a look at the crimes dataframe with the View() function. This will open the data browser in RStudio View(crimes) If you rather just want your results in the console, you can use the glimpse() function from the tibble package. This function does just that, it gives you a quick glimpse of the first few cases in the dataframe. Notice that there are two columns (Longitude and Latitude) that provide the require geographical coordinates that we need to plot this data. library(tibble) glimpse(crimes) ## Rows: 32,058 ## Columns: 12 ## $ `Crime ID` &lt;chr&gt; NA, &quot;aa1cc4cb0c436f463635890bcb4ff2cba08f59925… ## $ Month &lt;chr&gt; &quot;2019-06&quot;, &quot;2019-06&quot;, &quot;2019-06&quot;, &quot;2019-06&quot;, &quot;2… ## $ `Reported by` &lt;chr&gt; &quot;Greater Manchester Police&quot;, &quot;Greater Manchest… ## $ `Falls within` &lt;chr&gt; &quot;Greater Manchester Police&quot;, &quot;Greater Manchest… ## $ Longitude &lt;dbl&gt; -2.464422, -2.441166, -2.444807, -2.444807, -2… ## $ Latitude &lt;dbl&gt; 53.61250, 53.61604, 53.61151, 53.61151, 53.606… ## $ Location &lt;chr&gt; &quot;On or near Parking Area&quot;, &quot;On or near Pitcomb… ## $ `LSOA code` &lt;chr&gt; &quot;E01004768&quot;, &quot;E01004768&quot;, &quot;E01004768&quot;, &quot;E01004… ## $ `LSOA name` &lt;chr&gt; &quot;Bolton 001A&quot;, &quot;Bolton 001A&quot;, &quot;Bolton 001A&quot;, &quot;… ## $ `Crime type` &lt;chr&gt; &quot;Anti-social behaviour&quot;, &quot;Violence and sexual … ## $ `Last outcome category` &lt;chr&gt; NA, &quot;Unable to prosecute suspect&quot;, &quot;Unable to … ## $ Context &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… You may notice that a lot of the variable names are messy in that they have a space in them - this can cause issues, so before playing around too much with the data we want to clean this up. Luckily there is a very handy package you can use for this called janitor which contains the function clean_names(). library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test crimes &lt;- clean_names(crimes) Now the names are much neater. You can print them all for a view using the names() function: names(crimes) ## [1] &quot;crime_id&quot; &quot;month&quot; &quot;reported_by&quot; ## [4] &quot;falls_within&quot; &quot;longitude&quot; &quot;latitude&quot; ## [7] &quot;location&quot; &quot;lsoa_code&quot; &quot;lsoa_name&quot; ## [10] &quot;crime_type&quot; &quot;last_outcome_category&quot; &quot;context&quot; So what you see above is what we can call a basemap. The term basemap is seen often in GIS and refers to a collection of GIS data and/or orthorectified imagery that form the background setting for a map. The function of the basemap is to provide background detail necessary to orient the location of the map. Basemaps also add to the aesthetic appeal of a map. 2.3 From dataframes to spatial objects: finding spatial information in our data Having had a chance to inspect the data set you’ve downloaded, let’s consider what sort of spatial information we might be able to use. 2.3.1 Activity 2: Find the spatial data If you have a look at the column names, what are some of the variables which you think might have some spatial component? Have a think about each column, and how you think it may help to put these crimes on the map. Discuss in your groups, and add the answer to the shared google doc. Once you are done, read on. So what did you decide in your discussion? There are a few answers here. In fact there are one each to map onto point, line, and polygon, which we read about earlier. 2.3.2 The point First, and possibly most obvious, are the coordinates provided with each crime incident recorded. You can find this in the two columns - Longitude and Latitude. These two column help put each crime incident on a specific point on a map. For example, let’s take the very first crime incident. Here we use the head() function and specify that we want the first 1 rows only with n=1 parameter. head(crimes, n = 1) ## # A tibble: 1 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 2019-… Greater Ma… Greater Man… -2.46 53.6 On or n… E01004768 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You can see that the values are for Longitude and for Latitude. These two numbers allow us to put this point on a map. 2.3.3 The line Another column which contains information about where the crime happened is the aptly named location variable. This shows you a list of locations related to where the crimes happened. You may see a few values such as on or near XYZ street. Let’s look again at the first entry. head(crimes, n = 1) ## # A tibble: 1 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 2019-… Greater Ma… Greater Man… -2.46 53.6 On or n… E01004768 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You can see that the value is this isn’t great, as we might struggle to identify which parking area… Some other ones are more useful, let’s look at the last entry for example with the tail() function. tail(crimes, n = 1) ## # A tibble: 1 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 75e215f9… 2019… Greater Ma… Greater Man… -2.57 53.5 On or n… E01006347 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You can see that the value is . This makes our crime much easier to find, we just need to locate where is . We might have a shapefile of lines of all the roads of Manchester, and if we did, we can link the crime to that particular road, in order to map it. 2.3.4 The polygon What more? You may also have seen the column lsao_name seems to have some spatial component, Let’s have a look at the first crime again. head(crimes, n = 1) ## # A tibble: 1 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 2019-… Greater Ma… Greater Man… -2.46 53.6 On or n… E01004768 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; You see the value for LSOA name is - Bolton we know is a Borough of Greater Manchester, but what is the 001 for ? Well it denotes a sparticular geographical sub-unit within Bolton called a Lower Layer Super Output Area. This is a unit of UK Census Geography. The basic unit for Census Geography is an ‘Outout area’ - this is the resolution at which we can access data from the UK Census. We will be making use of census data later in the course. The Ourput Area (OA) is therefore the smallest unit we could use. Normally, in this module, we will be using a slightly larger version - the Lower Super Output Area (LSOA). There are now 181,408 OAs, 34,753 lower layer super output areas (LSOA) and 7,201 middle layer super output areas (MSOA) in England and Wales. The neat thing about these Census geographies is the idea is that they don’t change (unlike administrative boundaries such as wards) and were created with statistical analysis in mind. The less neat thing is that although we use them to operationalise the concept of neighbourhood a lot, they may not bear much resemblance to what residesnts might think of as their neighbourhood. Think for a minute - do you know what LSOA you live in? Most likely you answered no. If you answered yes, I am impressed and you get geography nerd cred. Well done. Anyway back to our crime data. You see we have two columns that reference LSOAs, lsoa_name and lsoa_code. We can use these to link our crime data to a shapefile of a polygon which contains the geometries needed to put the crime data on the map. 2.4 Putting crime on the map - simple features So how can we use these spatial details to put our crimes on the map? We need to somehow specify a geometry for our data, which links each unit of analysis (whether that is the point, line, or polygon) to a relevant geographical representation, which allows us to put this thing on the map. How you add geographical information will vary with the type of information we have, but in all of these, we will use the simple features framework. (Illustration (c) by Allison Horst) What are simple features? sf package author Edzer Pebesma describes simple features as a standardized way of encoding spatial vector data (points, lines, polygons) in computers. The sf package is an R package for reading, writing, handling, and manipulating simple features in R, implementing the vector (points, lines, polygons) data handling functionality. Traditionally spatial analysis in R were done using the sp package which creates a particular way of storing spatial objects in R. When most packages for spatial data analysis in R and for thematic cartography were first developed sp was the only way to work with spatial data in R. There are more than 450 packages rely on sp, making it an important part of the R ecosystem. More recently a new package, sf (which stands for “simple features”), is revolutionising the way that R does spatial analysis. This new package provides a new way of storing spatial objects in R and most recent R packages for spatial analysis and cartography are using it as the new default. It is easy to transform sf objects into sp objects, so that those packages that still don’t use this new format can be used. But in this course we will emphasise the use of sf whenever possible. You can read more about the history of spatial packages and the sf package in the first two chapters of this book. Features can be thought of as “things” or objects that have a spatial location or extent; they may be physical objects like a building, or social conventions like a political state. Feature geometry refers to the spatial properties (location or extent) of a feature, and can be described by a point, a point set, a linestring, a set of linestrings, a polygon, a set of polygons, or a combination of these. The simple adjective of simple features refers to the property that linestrings and polygons are built from points connected by straight line segments. Features typically also have other properties (temporal properties, color, name, measured quantity), which are called feature attributes. For more detailed insight I recommend reading the paper Simple Features for R: Standardized Support for Spatial Vector Data 2.4.1 Mapping points with sf Let’s get started with making some maps using sf. First, make sure you install the package, and then load with library() function. On Mac and Linux a few requirements must be met to install sf. These are described in the package’s README at github.com/r-spatial/sf. library(sf) ## Linking to GEOS 3.8.1, GDAL 3.1.4, PROJ 6.3.1 Now we can use the functions of sf in order to introduce geometries. Let’s start with the points 2.4.1.1 Activity 3: Creating a sf object of points WE know that we have two columns one for Longitude and one for Latitude, which pinpoint each crime event to a specific point, close to where it happened. Not quite where it happened, as the data are anonymised (more on this later), but for our purposes here, we can assume this is the location of the crime. To map these points, we can transform our ordinary dataframe into a simple features object. To do so, we can use the st_as_sf() function, into which we need to specify what we are to transform (our dataframe), where the spatial data can be found (our columns which hold the latitude and longidude information), and also what coordinate reference system the object has (see above our discussion about projections and coordinate reference systems). Latitude longitude coordinates specify location on the WGS 84 CRS (remember I said to keep this one in mind!). We can tell R that this is our CRS of choice by including it’s [EPSG identifier] (https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset) as a parameter in our function. It is handy to know the more common EPSG identifiers. For example, for WGS84 the EPSG identifier is 4326. For British National Grid, the identifier is 27700. We will be making use of these numbers, so do note them down somewhere. Putting it all together in practice, we can create a simple features object from our dataframe using the latitude and longitude columns: crimes_sf &lt;- st_as_sf(crimes, #dataframe coords = c(&quot;longitude&quot;, &quot;latitude&quot;), #columns with coordinates crs = 4326) #crs is WGS84 We can see that this is now a simple features object using the class() function the we see the result “sf”: class(crimes_sf) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; You might also notice something else that is different between crimes and crimes_sf. Have a look at the dimension (hint look in your ‘Environment’ tab). Do you see? Discuss what you think is different, and write your answers in the shared google doc. 2.4.1.2 Activity 4: Mapping our points Now that we have this sf object, how can we map it? I mentioned before about the graphical package ggplot2. We can use this, and its syntax, in order to map spatial data using the geom_sf() geometry. First, a quick refresher on ggplot and the grammar of graphics. The grammar of graphics upon which this package is based on defines various components of a graphic. Some of the most important are: -The data: For using ggplot2 the data has to be stored as a data frame or tibble. -The geoms: They describe the objects that represent the data (e.g., points, lines, polygons, etc..). This is what gets drawn. And you can have various different types layered over each other in the same visualisation. -The aesthetics: They describe the visual characteristics that represent data (e.g., position, size, colour, shape, transparency). -Facets: They describe how data is split into subsets and displayed as multiple small graphs. -Stats: They describe statistical transformations that typically summarise data. Let’s take it one step at the time. Essentially the philosophy behind this is that all graphics are made up of layers. The package ggplot2 is based on the grammar of graphics, the idea that you can build every graph from the same few components: a data set, a set of geoms—visual marks that represent data points, and a coordinate system. Take this example (all taken from Wickham, H. (2010). A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19(1), 3-28.) You have a table such as: You then want to plot this. To do so, you want to create a plot that combines the following layers: This will result in a final plot: Taking our crime data as an example, we would build up our plot as follows: Load ggplot2 package library(ggplot2) Data: ggplot(crimes, aes(x = crime_type)) Geometry: ggplot(crimes, aes(x = crime_type)) + geom_bar() Aesthetics: ggplot(crimes, aes(x = crime_type, fill = last_outcome_category)) + geom_bar() And then you could add any facets (for example if we had more than one month of data) or any statistics (for example error bars) with facet and stats layers. One more thing I do want to show is tidying up your chart, you can add theme and clean up your labels and titles and colour scheme. ggplot(crimes, aes(x = crime_type, fill = last_outcome_category)) + geom_bar() + coord_flip() + theme_minimal() + xlab(&quot;Crime Type&quot;) + ylab(&quot;Number of cases&quot;) + scale_fill_brewer(type = &quot;qual&quot;, palette = 3, name = &quot;Outcome&quot;) This is not the greatest graph you’ll ever see, but it illustrates the process. Do read up on ggplot2 for example in Hadley Wickham’s book R4DS chapter on data visualisation. So how can we use this for spatial data? We can use the geom_sf() function to do so. 2.4.1.3 Activity 5: Mapping our points Using geom_sf is slightly different to other geometries, for example how we used geom_bar() above. First we initiate the plot with the ggplot() function but don’t include the data in there. Instead, it is in the geometry where we add the data. And second we don’t need to specify the mapping of x and y. Since this is in the geometry column of our spatial object. Like so: ggplot() + geom_sf(data = crimes_sf) And here we have a map of each point in our data set, each recorded crime in June 2019 in Greater Manchester. Would you call this a map though? While it is presenting spatial data, there is not a lot of meaning being communicated. Point maps generally can be messy and their uses are specific to certain situations and cases, usually when you have fewer points, but here, these points are especially devoid of any meaning, as they are floating in a graph grid. SO let’s give it a basemap. We can do this by adding a layer to our graph object. Specifically we will use the annotation_map_tile() from the ggspatial package. This provides us with a static Open Street Map layer behind our data, giving it (some) more context. Remember to load the package (and install if you haven’t already) library(ggspatial) And then use the annotation_map_tile() function, making sure to place it before the geom_sf points layer, so the background map is placed first, and the points on top of that: ggplot() + annotation_map_tile() + geom_sf(data = crimes_sf) ## Zoom: 9 So what you see above is what we can call a basemap. The term basemap is seen often in GIS and refers to a collection of GIS data and/or orthorectified imagery that form the background setting for a map. The function of the basemap is to provide background detail necessary to orient the location of the map. Basemaps also add to the aesthetic appeal of a map. In the lecture and some readings these are described as reference maps. We often may want to use these reference maps as basemaps for our thematic and point maps. They may give us context and help with the interpretation. You can see above that you are seeing the Open Street Map Basemap. This is one option but there are others. Anyway let’s leave out points for now, and move on to how we might map our lines and polygons. 2.4.2 Mapping data by joining it to sf objects What about our other two columnds, location, and LSOAs? Well to put these on the map, we need a geometry representation of them. We will learn in this section where you may find these, how to download them, turn them into sf objects, and how to link your data to them to be able to map them. 2.4.2.1 Activity 6: Finding shapefiles In this section you are going to learn how you take one of the most popular data formats for spatial objects, the shapefile, and read it into R. The shapefile was developed by ESRI, the developers and vendors or ArcGIS. And although many other formats have developed since and ESRI no longer holds the same market position it once occupied (though they’re still the player to beat), shapefiles continue to be one of the most popular formats you will encounter in your work. You can read more about shapefiles here. We are going to learn here how to obtain shapefiles for British census geographies. In the class today we talked about the idea of neighborhouds and we explained how a good deal of sociological and criminological work traditionally used census geographies as proxies for neighbourhouds. As of today, they still are the geographical subdivisions for which we can obtain a greater number of attribute information (e.g., sociodemographics, etc.). For this activity we will focus on the polygon (the LSOA) rather than the lines of the streets, but the logic is more or less the same. Remember above we talked about what is a census geography. You can read more about census boundary data here. “Boundary data are a digitised representation of the underlying geography of the census.” Census Geography is often used in research and spatial analysis because it is divided into units based on population counts, created to form comparable units, rather than other administrative boundaries such as wards or police force areas. However depending on your research question and the context for your analysis, you might be using different units. The hierarchy of the census geographies goes from Country to Local Authority to Middle Layer Super Output Area (MSOA) to Lower Layer Super Output Area (LSOA) to Output Area: Here we will get some boundaries for Manchester. Let’s use the LSOA level, so that we can link back to our crime data earily. These are geographical regions designed to be more stable over time and consistent in size than existing administrative and political boundaries. LSOAs comprise, on average, 600 households that are combined on the basis of spatial proximity and homogeneity of dwelling type and tenure. So to get some boundary data, you can use the UK Data Service website. There is a simple Boundary Data Selector tool which we can use. When you get to the link, you will see on the top there is some notification to help you with the boundary data selector. If in the future you are looking for boundary data and you are feeling unsure at any point, feel free to click on that note “How to use Boundary Data Selector” which will help to guide you. For now, let’s focus on the selector options. Here you can choose the country you want to select shapefiles for. We select “England.” You can also choose the type of geography we want to use. Here we select “Statistical Building Block,” as discussed above. And finally you can select when you want it for. If you are working with historical data, it makes sense to find boundaries that match the timescale for your data. Here we will be dealing with contemporary data, and therefore we want to be able to use the newest available boundary data. Once you have selected these options, click on the “Find” button. That will populate the box below: Here you can select the boundaries we want. As discussed, we want the census lower super output areas. But again, your future choices here will depend on what data you want to be mapping. Once you’ve made your choice, click on “List Areas.” This will now populate the box below. We are here concerned with Manchester. However you can select more than one if you want boundaries for more than one area as well. Just hold down “ctrl” to select multiple areas individually, or the shift key to select everything in between. Once you’ve made your decision click on the “Extract Boundary Data” button. You will see the following message: You can bookmark, or just stay on the page and wait. How long you have to wait will depend on how much data you have requested to download. When your data is read, you will see the following message: You have to right click on the “BoundaryData.zip,” and hit Save Target as on a PC or Save Link As on a Mac: Navigate to the folder you have created for this analysis, and save the .zip file there. Extract the file contents using whatever you like to use to unzip compressed files. # For example, unzip(&quot;BoundaryData.zip&quot;, exdir = &quot;BoundaryData&quot;) You should end up with a folder called “BoundaryData.” Have a look at its contents: So you can see immediately that there are some documentations around the usage of this shapefile, in the readme and the terms and conditions. Have a look at these as they will contain information about how you can use this map. For example, all your maps will have to mention where you got all the data from. So since you got this boundary data from the UKDS, you will have to note the following: “Contains National Statistics data © Crown copyright and database right [year] Contains OS data © Crown copyright [and database right] (year)” You can read more about this in the terms and conditions document. But then you will also notice that there are 4 files with the same name “england_oac_2011.” It is important that you keep all these files in the same location as each other! They all contain different bits of information about your shapefile (and they are all needed): .shp — shape format; the feature geometry itself - this is what you see on the map .shx — shape index format; a positional index of the feature geometry to allow seeking forwards and backwards quickly .dbf — attribute format; columnar attributes for each shape, in dBase IV format. .prj — projection format; the coordinate system and projection information, a plain text file describing the projection using well-known text format Sometimes there might be more files associated with your shapefile as well, but we will not cover them here. So unlike when you work with spreadsheets and data in tabular form, which typically is just all included in one file; when you work with spatial data, you have to live with the required information living in separate files that need to be stored together. So, being tidy and organised is even more important when you carry out projects that involve spatial data. Please do remember the suggestions we provided last week as to how to organise your RStudio project directories. 2.4.2.2 Activity 7: Reading shapefiles into R To read in your data, you will need to know the path to where you have saved it. Ideally this will be in your data folder in your project directory. Let’s create an object and assign it our shapefile’s name: # Remember to use the appropriate pathfile in your case shp_name &lt;- &quot;data/BoundaryData/england_lsoa_2011.shp&quot; Make sure that this is saved in your working directory, and you have set your working directory. Now use the st_read() function to read in the shapefile: manchester_lsoa &lt;- st_read(shp_name) ## Reading layer `england_lsoa_2011&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/BoundaryData/england_lsoa_2011.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 282 features and 3 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 378833.2 ymin: 382620.6 xmax: 390350.2 ymax: 405357.1 ## projected CRS: OSGB 1936 / British National Grid Now you have your spatial data file. You can have a look at what sort of data it contains, the same way you would view a dataframe, with the View() function: View(manchester_lsoa) ## Rows: 282 ## Columns: 4 ## $ label &lt;chr&gt; &quot;E08000003E02001062E01005066&quot;, &quot;E08000003E02001092E01005073&quot;,… ## $ name &lt;chr&gt; &quot;Manchester 018E&quot;, &quot;Manchester 048C&quot;, &quot;Manchester 018A&quot;, &quot;Man… ## $ code &lt;chr&gt; &quot;E01005066&quot;, &quot;E01005073&quot;, &quot;E01005061&quot;, &quot;E01005062&quot;, &quot;E0100506… ## $ geometry &lt;POLYGON [m]&gt; POLYGON ((384850 397432, 38..., POLYGON ((382221.1 38… And of course, since it’s spatial data, you can map it using the geom_sf() function, as we did with our points: ggplot() + geom_sf(data = manchester_lsoa) Great, we now have an outline of the LSOAs in Manchester. Notice how the shape is different to that of the points in our crime data. Why do you think this is? Discuss and write your thoughts in the shared google doc. 2.4.2.3 Activity 8: Data wrangling with dplyr In order to map crimes to LSOAs we might want to take a step back and think about unit of analysis at which our data are collected. In our original dataframe of crimes, we saw that each crime incident is one row. So the unit of anlysis is each crime. Since we were looking to map each crime at the location it happened, we used the latitude and longitude supplied with each one, and this supplied a geometry each for each crime type. However, when we are looking to map our data to LSOA level, we need to match the crime data to the geometry we wish to display. Have a look at the manchester_lsoa object we mapped above. How many rows (observations) does it have? You can check this by looking in the Environment pane, or by using the nrow() function. nrow(manchester_lsoa) ## [1] 282 You can see this has 282 rows. This means we have geometries for 282 LSOAs. On the other hand, our crimes dataframe has 32058 rows, one for each crime (observation). So how can we match these up? The answer lies in thinking about what it is that our map using LSOAs as our unit of analysis will be able to tell us. Think of other maps of areas - what are they usually telling you? Usually we expect to see crimes per neighbourhood - something like this. So our unit of analysis needs to be LSOA, and for each one we need to know how many crimes occurred in that area. To achieve this, we will wrangle our data using functions from the dplyr package. This is a package for conducting all sorts of operations with data frames. We are not going to cover the full functionality of dplyr (which you can consult in this tutorial), but we are going to cover three different very useful elements of dplyr: the select function, the group_by function, and the piping operator. First load the library: library(dplyr) The select() function provides you with a simple way of subsetting columns from a data frame. So, say we just want to use one variable, lsoa_code, from the crimes dataframe and store it in a new object we could write the following code: new_object &lt;- select(crimes, lsoa_code) We can also use the group_by() function for performing group operations. Essentially this function ask R to group cases within categories and then do something with those grouped cases. So, say, we want to count the number of cases within each LSOA, we could use the following code: #First we group the cases by LSOA code and stored this organised data into a new object grouped_crimes &lt;- group_by(new_object, lsoa_code) #Then we could count the number of cases within each category and use the summarise function to print the results summarise(grouped_crimes, count = n()) #We could infact create a new dataframe with these results crime_per_LSOA &lt;- summarise(grouped_crimes, count = n()) As you can see we can do what we wanted, create a new dataframe with the required info, but if we do this we are creating many objects that we don’t need, one at each step. Instead there is a more efficient way of doing this, without so many intermediate steps clogging up our environment with unnecessary objects. That’s where the piping operator comes handy. The piping operator is written like %&gt;% and it can be read as “and then.” Look at the code below: #First we say create a new object called crime_per_lsoa, and then select only the LSOA.code column to exist in this object, and then group this object by the LSOA.code, and then count the number of cases within each category, this is what I want in the new object. crimes_per_lsoa &lt;- crimes %&gt;% group_by(lsoa_code) %&gt;% summarise(count=n()) Essentially we obtain the same results but with more streamlined and elegant code, and not needing additional objects in our environment. And now we have a new object, crimes_per_lsoa if we have a look at this one, we can now see what each row represents one LSOA, and next to it we have a variable for the number of crimes from each area. We created a new dataframe from a frequency table, and as each row of the crimes data was one crime, the frequency table tells us the number of crimes which occurred in each LSOA. Those of you playing close attention might note that there are still more observations in this dataframe (1671) than in the manchester_lsoas one (282). Why do you think that might be? Look back at how you answered the final question for the previous activity for a hint. 2.4.2.4 Activity 9: Join data to sf object Our next task is to link our crimes data to our sf spatial object to help us map this. Notice anything similar between the data from the shapefile and the frequency table data we just created? Do they share a column? Yes! You might notice that the lsoa_code field in the crimes data matches the values in the code field in the spatial data. In theory we could join these two data tables. So how do we do this? Well what you can do is to link one data set with another. Data linking is used to bring together information from different sources in order to create a new, richer dataset. This involves identifying and combining information from corresponding records on each of the different source datasets. The records in the resulting linked dataset contain some data from each of the source datasets. Most linking techniques combine records from different datasets if they refer to the same entity (an entity may be a person, organisation, household or even a geographic region.) You can merge (combine) rows from one table into another just by pasting them in the first empty cells below the target table—the table grows in size to include the new rows. And if the rows in both tables match up, you can merge columns from one table with another by pasting them in the first empty cells to the right of the table—again, the table grows, this time to include the new columns. Merging rows is pretty straightforward, but merging columns can be tricky if the rows of one table don’t always line up with the rows in the other table. By using left_join() from the dplyr package, you can avoid some of the alignment problems. left_join() will return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. So we’ve already identified that both our crimes data, and the spatial data contain a column with matching values, the codes for the LSOA that each row represents. You need a unique identifier to be present for each row in all the data sets that you wish to join. This is how R knows what values belong to what row! What you are doing is matching each value from one table to the next, using this unique identified column, that exists in both tables. For example, let’s say we have two data sets from some people in Hawkins, Indiana. In one data set we collected information about their age. In another one, we collected information about their hair colour. If we collected some information that is unique to each observation, and this is the same in both sets of data, for example their names, then we can link them up, based on this information. Something like this: And by doing so, we produce a final table that contains all values, lined up correctly for each individual observation, like this: This is all we are doing, when merging tables, is we are making use that we line up the correct value for all the variables, for all our observations. Why are we using left join though? There is a whole family of join functions as part of dplyr which join data sets. There is also a right_join, and an inner_join and an outer_join and a full_join. But here we use left join, because that way we keep all the rows in x (the left-hand side dataframe), and join to it all the matched columns in y (the right-hand side dataframe). So let’s join the crimes data to the spatial data, using left_join(): We have to tell left_join what are the dataframes we want to join, as well as the names of the columns that contain the matching values in each one. This is “code” in the manchester_lsoa dataframe and “lsoa_code” in the crimes_per_lsoa dataframe. Like so: manchester_lsoa &lt;- left_join(manchester_lsoa, crimes_per_lsoa, by = c(&quot;code&quot;=&quot;lsoa_code&quot;)) Now if you have a look at the data again, you will see that the column of number of crimes (n) has been added on. 2.4.2.5 Activity 10: Mapping our data at polygon level Now that we have joined the crimes data to the geometry, you can use this to make our map! Remember our original empty map: ggplot() + geom_sf(data = manchester_lsoa) Well now, since we have the column (varaible) for number of crimes here, we can use that to share the polygons based on how many crimes there are in each LSOA. We can do this by specifying the fill= parameter of the geom_sf function. ggplot() + geom_sf(data = manchester_lsoa, aes(fill = count)) We can add a basemap, and adjust the colour scheme, and even the opacity to see under our shape files. ggplot() + annotation_map_tile() + # add basemap geom_sf(data = manchester_lsoa, aes(fill = count), alpha = 0.7) + # alpha sets the opacity scale_fill_gradient2(name =&quot;Number of crimes&quot;) #use scale_fill_gradient2() for a different palette and name the variable on the legend ## Zoom: 10 2.5 Summary In this session we had a play around with some regular old crime data and discovered how we can use the sf package in R to assign it a geometry (both at point and polygon level), and how that can help us visualise our results. We covered some very important concepts such as projections and coordinate reference systems, and we had a play at acquiring shapefiles which can help us visualise our data. We had a think about units of analysis, and how that will affect how we visualise our data. Make sure to complete the homework tasks available on Blackboard, and once you have done that, and read all the readings and watched all the videos, complete your homework quiz. Next week we will spend a bit of more time discussing how to make good choices when producing maps. "],["thematic-maps-in-r.html", "Chapter 3 Thematic maps in R 3.1 Intro and recap 3.2 Creating thematic maps 3.3 Classification systems for thematic maps 3.4 Using graduated symbols 3.5 Mapping rates rather than counts 3.6 Summary", " Chapter 3 Thematic maps in R 3.1 Intro and recap Last week we showed you fairly quickly how to create maps by understanding how data may have spatial elements, and how that can be linked to geometries. This week we will get to know how to think about thematic maps, and how to apply your learning to creating your own maps of this variety. In our lecture videos this week we discuss in detail issues with choropleth maps. So the focus of today’s lab is going to be around thematic maps and some of the choices we discussed. We will also introduce faceting and small multiples, which is a format for comparing the geographical distribution of different social phenomena. For this session we will be using the spatial object that you created last week and complement it with additional information from the census. So first of all you will have to rerun the code you used to create the manchester_lsoa sf object. Apart from doing so, you want to start your session loading the libraries you know for sure you will need: library(readr) library(sf) library(janitor) library(tmap) library(dplyr) You may not remember all of what you did to generate that file so let’s not waste time and just cut and paste from below (but try to remember what each of the lines of code is doing and if you are not clear look at the notes from last week). Imagine you had to do all of this again by pointing and clicking in a graphical user interface rather than just sending the code to the console! As you will see time and time again, code in the end is a much more efficient way of talking to a computer. crimes &lt;- read_csv(&quot;data/2019-06-greater-manchester-street.csv&quot;) #The following assumes you have a subdirectory called BoundaryData in your data folder, if not then you will need to change to the pathfile where you store your LSOA shapefile shp_name &lt;- &quot;data/BoundaryData/england_lsoa_2011.shp&quot; manchester_lsoa &lt;- st_read(shp_name) ## Reading layer `england_lsoa_2011&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/BoundaryData/england_lsoa_2011.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 282 features and 3 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 378833.2 ymin: 382620.6 xmax: 390350.2 ymax: 405357.1 ## projected CRS: OSGB 1936 / British National Grid crimes_per_lsoa &lt;- crimes %&gt;% clean_names() %&gt;% select(lsoa_code) %&gt;% group_by(lsoa_code) %&gt;% summarise(count=n()) manchester_lsoa &lt;- left_join(manchester_lsoa, crimes_per_lsoa, by = c(&quot;code&quot;=&quot;lsoa_code&quot;)) You may not want to have to go through this process all the time. One thing you could do is to save the manchester_lsoa object as a physical file in your machine. You can use the st_write() function from the sf package to do this. If we want to write into a shapefile format we would do as shown below: st_write(manchester_lsoa, &quot;data/BoundaryData/manchester_crime_lsoa.shp&quot;) Notice how four files have appeared in your working directory, in your “BoundaryData”\" subdirectory or whatever you called it. Remember what we said last week about shapefiles, there are a collection of files that need to be kept together. If you wanted to bring this shapefile back into R at any future point, you would only need to use the st_read() function. manchester_crime_lsoa &lt;- st_read(&quot;data/BoundaryData/manchester_crime_lsoa.shp&quot;) ## Reading layer `manchester_crime_lsoa&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/BoundaryData/manchester_crime_lsoa.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 282 features and 4 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 378833.2 ymin: 382620.6 xmax: 390350.2 ymax: 405357.1 ## projected CRS: Transverse_Mercator 3.1.0.1 Activity 1: Spot the difference Before we carry on, can you tell what is different between manchester_lsoa.shp and manchester_crime_lsoa.shp that you now have saved for working with? Discuss in your groups, and write your thoughts on the shared google doc. 3.2 Creating thematic maps Today we are going to introduce the tmap package. This package was developed to easily produce thematic maps. It is inspired by the ggplot2 package and the layered grammar of graphics. It was written by Martjin Tennekes a Dutch data scientist. There are a number of vignettes in the CRAN repository and the GitHub repo for this package that you can explore. GitHub is a collaborative website used by software developers and data scientist, also contains a useful readme section with additional resources to familiarise yourself with this package. Each map can be plotted as a static map (plot mode) and shown interactively (view mode) as we briefly saw last week. We will start by focusing on static maps. Every time you use this package you will need a line of code that specifies the spatial object you will be using. Although originally developed to handle sp objects only, it now also has support for sf objects. For specifying the spatial object we use the tm_shape() function and inside we specify the name of the spatial object we are using. On its own, this will do nothing apparent. No map will be created. We need to add additional functions to specify what we are doing with that spatial object. If you try to run this line on its own, you’ll get an error telling you you must “Specify at least one layer after each tm_shape.” tm_shape(manchester_crime_lsoa) The main plotting method consists of elements that we can add. The first element is the tm_shape() function specifying the spatial object, and then we can add a series of elements specifying layers in the visualisation. They can include polygons, symbols, polylines, raster, and text labels as base layers. We will add a polygon using tm_polygon(). As noted, with tmap you can produce both static and interactive maps. The interactive maps rely on leaflet. You can control whether the map is static or interactive with the tmap_mode() function. If you want a static map you pass plot as an argument, if you want an interactive map you pass view as an argument. Let’s create a static map first. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting tm_shape(manchester_crime_lsoa) + tm_polygons() Given that we are not passing any additional arguments all we are getting is a map with the shape of the geographies that we are representing, the census LSOAs for Manchester city. We can, however, ask R to produce a choropleth map by mapping the values of a variable in our data table using colour. In tmap we need to denote our variables between quotes. The first argument we pass then would be the name of the variable we want to visualise. If you remember we have a count for crimes (“count”), so let’s visualise that by creating a thematic map. tm_shape(manchester_crime_lsoa) + tm_polygons(&quot;count&quot;) We have been using tm_polygons() but we can also add the elements of a polygon map using different functions that break down what we represent here. In the map above you see the polygons have a dual representation, the borders are represented by lines and the colour is mapped to the intensity of the quantitative variable we are representing. With darker colours representing more of the variable, the areas with more crimes. Instead of using tm_polygon() we can use the related functions tm_fill(), for the colour inside the polygons, and tm_borders(), for the aesthetics representing the border of the polygons. Say we find the borders distracting and we want to set them to be transparent. In that case we could just use tm_fill(). tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;) As you can see here, the look is a bit cleaner. We don’t need to get rid of the borders completely. Perhaps we want to make them a bit more translucent. We could do that by adding the border element but making the drawing of the borders less pronounced. tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;) + tm_borders(alpha = 0.1) The alpha parameter that we are inserting within tm_borders() controls the transparency of the borders, we can go from 0 (totally transparent) to 1 (not transparent). You can play around with this value and see the results. Notice in the last few maps we did not have to specify whether we wanted the map to be static or interactive. When you use tmap, R will remember the mode you want to use. So once you specify tmap_mode(\"plot\"), all the subsequent maps will be static. It is only when you want to change this behaviour that you would need another tmap_mode call. Notice as well that the legend in this map is (a) not very informative and (b) located in a place that is less than optimal, since it covers part of the map. We can add a title within the tm_fill to clarify what count is and we can use the tm_layout() function to control the appearance of the legend. This later function tm_layout allows you to think about many of the more general cosmetics of the map. tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, title = &quot;Crime counts&quot;) + tm_borders(alpha = 0.1) + tm_layout(main.title = &quot;Crime in Manchester City, Nov/2017&quot;, main.title.size = 0.7 , legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8) We are also going to change the current style of the maps by making them more friendly to colour blind people. We can use the tmap_style() function to do so. current_style &lt;- tmap_style(&quot;col_blind&quot;) ## tmap style set to &quot;col_blind&quot; ## other available styles are: &quot;white&quot;, &quot;gray&quot;, &quot;natural&quot;, &quot;cobalt&quot;, &quot;albatross&quot;, &quot;beaver&quot;, &quot;bw&quot;, &quot;classic&quot;, &quot;watercolor&quot; See how the map changes. tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, title = &quot;Crime counts&quot;) + tm_borders(alpha = 0.1) + tm_layout(main.title = &quot;Crime in Manchester City, Nov/2017&quot;, main.title.size = 0.7 , legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8) 3.3 Classification systems for thematic maps In thematic mapping, you have to make some key decisions, the most important one being how to display your data. When mapping a quantitative varaible, we have to “bin” this variable into groups. For example in the map we made below, the default binning applied was to display LSOAs grouped into those with 1 -200, 201-400, 401-600 and 601-800 crimes. But why these? How were these groupings decided upon? The quantitative information, being directly measured (e.g. total population) or derived (e.g. population density), is usually classified before its symbolization in a thematic map. Theoretically, accurate classes that best reflect the distributional character of the data set can be calculated. The equal interval (or equal step) classification method divides the range of attribute values into equally sized classes. What this means is that the values are divided into equal groups. This approach is best for continuous data. The quantile map bin the same count of features into each of its classes. This classification method places equal numbers of observations into each class. This method is best for data that is evenly distributed across its range. The natural breaks (or Jenks) classification method utilizes an algorithm to group values in classes that are separated by distinct break points. It is an optimisation method which takes an iterative approach to its groupings to achieve least variation within each class. This method is best used with data that is unevenly distributed but not skewed toward either end of the distribution. The standard deviation map uses the standard deviation (standardised measure of observations’ deviation from the mean) to bin the observations into classes. This classification method forms each class by adding and subtracting the standard deviation from the mean of the dataset. It is best suited to be used with data that conforms to a normal distribution. The above should be familiar from your reading, but if you would like another angle I recommend a browse of this guide which has some nice visualisations. 3.3.0.1 Activity 2: Comparing classifications For comparing the effects of using different methods we can use small multiples. Small multiples is simply a way of reproducing side by sides similar maps for comparative purposes. To be more precise small multiples are sets of charts of the same type, with the same scale, presented together at a small size and with minimal detail, usually in a grid of some kind. The term was at least popularized by Edward Tufte, appearing first in his Visual Display of Quantitative Information in 1983. There are different ways of creating small multiples with tmap as you could see in the vignettes for the package, some of which are quicker but a bit more restricted. Here we are going to use tmap_arrange(). With tmap_arrange() first we need to create the maps we want and then we arrange them together. Let’s make four maps, each one using a different classification method: Equal interval, QUantile, Natural breaks (Jenks), and Standard Deviation. For each map, instead of visualising them one by one, just assign them to a new object. Let’s call them map1, map2, map3 and map4. So let’s make map1. This will create a thematic map using equal intervals: map1 &lt;- tm_shape(manchester_crime_lsoa) + #use tm_shape function to specify spatial object tm_fill(&quot;count&quot;, style=&quot;equal&quot;, title = &quot;Equal&quot;) + #use tm_fill to specify variable, classification method, and give the map a title tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;), #use tm_layout to make the legend look nice legend.title.size = 0.8, legend.text.size = 0.5) Now create map2, with the jenks method often preferred by geographers: map2 &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, style=&quot;jenks&quot;, title = &quot;Jenks&quot;) + tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) Now create map3, with the quantile method often preferred by epidemiologists: map3 &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, style=&quot;quantile&quot;, title = &quot;Quantile&quot;) + tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) And finally make map4, standard deviation map, which maps the values of our variable to distance to the mean value. map4 &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, style=&quot;sd&quot;, title = &quot;Standard Deviation&quot;) + tm_borders(alpha=0.1) + tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) Notice that we are not plotting the maps, we are storing them into R objects (map1 to map4). This way they are saved, and you can call them later, which is what we need in order to plot them together using the tmap_arrange() function. So if you wanted to map just map3 for example, all you need to do, is call the map3 object. Like so: map3 But now we will plot all 4 maps together, arranged using the tmap_arrange() function. Like so: #And now we deploy tmap_arrange to plot these maps together tmap_arrange(map1, map2, map3, map4) There are some other classification methods built into tmap which you can experiment with if you’d like. Your discrete gradient options are “cat,” “fixed,” “sd,” “equal,” “pretty,” “quantile,” “kmeans,” “hclust,” “bclust,” “fisher,” “jenks,” “dpih,” “headtails,” and “log10_pretty.” A numeric variable is processed as a categorical variable when using “cat,” i.e. each unique value will correspond to a distinct category. Taken from the help file we can find more information about these, for example the “kmeans” style uses kmeans clustering technique (a form of unsupervised statistical learning) to generate the breaks. The “hclust” style uses hclust to generate the breaks using hierarchical clustering and the “bclust” style uses bclust to generate the breaks using bagged clustering. These approaches are outisde the scope of what we cover, but just keep in mind that there are many different ways to classify your data, and you must think carefully about the choice you make, as it may affect your readers’ conclusions from your map. 3.4 Using graduated symbols Some of the literature on thematic cartography highlights how counts, like the ones above, are best represented using graduated symbols rather than choropleth maps (using colour, as we did above). So let’s try to go for a more appropriate representation. In tmap you can use tm_symbols for this. We will use tm_borders to provide some context. tm_shape(manchester_crime_lsoa) + tm_bubbles(&quot;count&quot;) First thing you see is that we loose the context (provided by the polygon borders) that we had earlier. The border.lwd argument set to NA in the tm_symbols() is asking R not to draw a border to the circles. Whereas tm_borders() brings back a layer with the borders of the polygons representing the different LSOAs in Manchester city. Notice how I am modifying the transparency of the borders with the alpha parameter. tm_shape(manchester_crime_lsoa) + #use tm_shape function to specify spatial object tm_bubbles(&quot;count&quot;, border.lwd=NA) + #use tm_bubbles to add the bubble visualisation, but set the &#39;border.lwd&#39; parameter to NA, meaning no symbol borders are drawn tm_borders(alpha=0.1) + #add the LSOA border outlines using tm_borders, but set their transparency using the alpha parameter (0 is totally transparent, 1 is not at all) tm_layout(legend.position = c(&quot;right&quot;, &quot;bottom&quot;), #use tm_layout to make the legend look nice legend.title.size = 0.8, legend.text.size = 0.5) 3.5 Mapping rates rather than counts In much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that population is not equally distributed in space. That means that if we do not account for how many people are somewhere, we end up mapping population size rather than our topic of interest. As always, there is a relevant xkcd for that: https://xkcd.com/1138/ In specific to crime mapping, there is an ongoing issue of the denominators dilemma which has been cropping up in your reading. This is concerned with choosing the most appropriate measure for calculating crime rates. The best measure is one which captures opportunities. You read about some approaches to capturing ambient population for example to estimate risk for on-street crimes. Whatever denominator you choose, you will usually want to make a case as to why that is the best representation of the opportunities for the crime type you’re interested in. 3.5.0.1 Activity 3: Getting population data from the census Last week you learned how to obtain crime data from the police UK website and you also developed the skills to obtain shapefiles with the boundaries for the UK census geographies. Specifically you learnt how to obtain LSOAs boundaries. Then we taught you how to join these data tables using dplyr. If you open your manchester_lsoa object you will see that at the moment you only have one field in this dataframe providing you with statistical information. However, there is a great deal of additional information that you could add to these data frame. Given that you are using census geographies you could add to it all kind of socio demographic variables available from the census. You may want to watch this 4 minute video to get a sense for how to obtain the data. If you don’t have headphones make sure you read this brief tutorial before carrying on. We are going to get some data for Manchester city LSOAs. Let me warn you though, the census data portal is one of the closest things to hell you are going to come across on the internet. Using it will be a good reminder of why point and click interfaces can suck the life out of you. From the main Infuse portal select the 2011 census data then when queried pick selection by geography: Expand the local authorities and select Manchester. Expand Manchester and select LSOAs: At the bottom of the page click in Add and then where it says Next. Now big tip. Do not press back in your browser. If you need to navigate back once you get to that point use the previous button at the bottom of the screen. You will regret it if you don’t do this. Now you will need to practice navigating the Infuse system to generate a data table that has a number of relevant fields we are going to use today and at a later point this semester. I want you to create a file with information about: the resident population, the workday population, and the number of deprivation households. This will involve some trial and error but you should end up with a selection like the one below: Once you have those fields click next to get the data and download the file. Unzip them and see you have your .csv file. Save this into your data subfolder in your project directory. Use read_csv() function from the readr package to import this data. ## Warning: Missing column names filled in: &#39;X14&#39; [14] library(readr) census_lsoa_m &lt;- read_csv(&quot;data/Data_AGE_APPSOCG_DAYPOP_UNIT_URESPOP.csv&quot;) Notice that even all the variables that begin with “f” are numbers they have been read into R as characters. This is to do with the fact the first two lines do not represent cases and do have characters. R is coercing everything into character vectors. Let’s clean this a bit. First we will get rid of the first two rows. In particular we will use the slice() function from dplyr. We can use slice to select cases based on row number. We don’t need the first two rows so we can select rows 3 to 284. census_lsoa_m &lt;- slice(census_lsoa_m, 3:284) There are also fields that we don’t need. We only need the variables beginning with F for those have the information about population and deprivation, and the GEO_CODE tag which will allow us to link this table to the manchester_lsoa file. census_lsoa_m &lt;- select(census_lsoa_m, GEO_CODE, F996:F323339) We also want to convert the character variables into numeric ones, whilst preserving the id as a character variable. For this we will use the lapply function. This is a convenient function that will administer a function to the elements we pass as an argument. In this case we are asking to apply the as.numeric() function to the columns 2 to 9 of the census_lsoa_m data frame. This is turning into numeric all those character columns. census_lsoa_m[2:9] &lt;- lapply(census_lsoa_m[2:9], as.numeric) The only problem we have now is that the variable names are not very informative. If you look at the metadata file that came along you can see that there is a key there to understand what these variables mean. We could use that information to create more meaningful names for the variables we have. We will use the rename() function from the dplyr package to do the renaming: census_lsoa_m &lt;- rename(census_lsoa_m, tothouse = F996, notdepr = F997, depriv1 = F998, depriv2 = F999, depriv3 = F1000, depriv4 = F1001, respop = F2384, wkdpop = F323339) The rename function takes as the first argument the name of the dataframe. Then for each variable you want to change you write down the new name followed by the old name. Now that we have the file ready we can link it to our manchester_lsoa file using code we learned last week. We use again the left_join() function to add to the manchester_lsoa dataframe the variables that are present in the census_lsoa_m. The first argument in the function is the name of the dataframe to which we want to add fields, the second argument the name of the dataframe from which those fields come, and then you need to specify using “by” the name of the variables on each of these two dataframes that have the id variable that will allow us to ensure that we are linking the information across the same observations. manchester_crime_lsoa &lt;- left_join(manchester_crime_lsoa, census_lsoa_m, by = c(&quot;code&quot;=&quot;GEO_CODE&quot;)) And there you go… Now you have a datafile with quite a few pieces of additional information about LSOAs in Manchester. The next step is to use this information. 3.5.0.2 Activity 4: Computing crime rates Ok, so now we have a field that provides us with the number of crimes and two alternative counts of population for each LSOA in Manchester in the same dataframe. We could compute the rate of crime in each using the population counts as our denominator. Let’s see how the maps may compare using these different denominators. But first we need to create new variables. For this we can use the mutate() function from the dplyr package. This is a very helpful function to create new variables in a dataframe based on transformations or mathematical operations performed in other variables within the dataframe. In this function, the first argument is the name of the data frame, and then we can pass as arguments all new variables we want to create as well as the instructions as to how we are creating those variables. First we want to create a rate using the usual residents, since crime rates are often expressed by 100,000 inhabitants we will multiply the division of the number of crimes by the number of usual residents by 100,000. We will then create another variable, crimr2, using the workday population as the denominator. We will store this new variables in our existing manchester_lsoa dataset. You can see that below then I specify the name of a new variable crimr1 and then I tell the function I want that variable to equal (for each case) the division of the values in the variable count (number of crimes) by the variable respop (number of people residing in the area) and then we multiply the result of this division by 100,000 to obtain a rate expressed in those terms. Then we do likewise for the alternative measure of crime. manchester_crime_lsoa &lt;- mutate(manchester_crime_lsoa, crimr1 = (count/respop)*100000, crimr2 = (count/wkdpop)*100000) And now we have two new variables, one for crime rate with residential population as a denominator, and another with workplace population as a denominator. 3.5.0.3 Activity 5: Mapping crime rates Now that we have our variables for crime rate per population, we can use this to produce our crime maps! Let’s first map crime count, next to residential population, and then the crime rate. We can do this by creating two maps, and then using our trusty tmap_arrange() to put them next to one another. Let’s also use a different palette for each map, that is a different fill colour. To change the colours for the fill of the polygons you can use the palette argument within the tm_fill() function. You can explore different palettes running the following code: tmaptools::palette_explorer() Pick the ones you like, and use them. Here I will use Blues, Greens and Reds crime_count_map &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;count&quot;, style=&quot;quantile&quot;, palette= &quot;Blues&quot;, title = &quot;Crime count&quot;) + tm_layout(panel.labels = &quot;Crime count&quot;, legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) res_pop_map &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;respop&quot;, style=&quot;quantile&quot;, palette= &quot;Greens&quot;, title = &quot;Residential population&quot;) + tm_layout(panel.labels = &quot;Residential population&quot;, legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) crime_rate_map &lt;- tm_shape(manchester_crime_lsoa) + tm_fill(&quot;crimr1&quot;, style=&quot;quantile&quot;, palette= &quot;Reds&quot;, title = &quot;Crime rate&quot;) + tm_layout(panel.labels = &quot;Crime per 100,00 population&quot;, legend.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) tmap_arrange(crime_count_map, res_pop_map, crime_rate_map) What do you think about these three maps? How do you think this might be different if we were to look at workday population instead of residential population as a denominator? Discuss in your groups and make notes in your shared google doc. Once you have completed this activity, let’s explore your map with the crime rate using the usual residents as the denominator using the interactive way. Assuming you name that visualisation map5 you could use the following code. tmap_mode(&quot;view&quot;) map5 You may find it useful to shift to the OpenStreetMap view by clicking in the box to the left, since it will give you a bit more contextual information than the default CartoDB basemap. In the first lecture we spoke a bit about Open Street Map, but if you’re interested it’s definitely worth reading up on. As I mentioned, Open Street Map is a non-profit foundation whose aim is to support and enable the development of freely-reusable geospatial data, and relies heavily on volunteers participating in this project to map their local areas. You can have a look here for ongoing humanitarian projects, or read here about the mapping parties I told you about. At the very least though, in the spirit of open source and crowdsourcing, take a moment to appreciate that all these volunteers of people just like you have contributed to creating such a detailed geographical database of our world. That’s definitely something kinda cool to think about! 3.6 Summary This week we learned some basic principles of thematic maps. We learned how to make them using the tmap package, we learned about the importance of classification schemes, and how each one may produce a different looking map, which may tell a different story. We learned how to access population data from the UK census, and how we can use that to calculate crime rates instead of crime counts. Taking this forward, make sure to think critically about the decisions that go into producing your thematic maps "],["performing-spatial-operations-in-r.html", "Chapter 4 Performing spatial operations in R 4.1 Getting some (more) data 4.2 Making interactive maps with leaflet 4.3 Spatial operations 4.4 Recap", " Chapter 4 Performing spatial operations in R By now you have come a long way in terms of taking your spatial data, and visualising it using maps, and being able to present the values of a variable using thematic maps. You have had some practice in taking data which has a spatial component, and joining it to a shapefile, using the common column, in order to be able to visually demonstrate variation on something, such as the crime rate, across space. I hope that you are finding this to be really exciting stuff, and an opportunity to get yourselves accustomed to spatial data. If there is anything you are unsure about, or want to catch up on, please do not hesitate to revisit older material, and ask us questions about it. We build on each week acquiring knowledge cumulatively, so don’t let yourself get stuck anywhere down the line. But, if you’re ready, today we will go a step further, and get your hands dirty with spatial manipulation of your data. Thus far, our data manipulation exercises were such that you might be familiar with, from your earlier exposures to data analysis. Linking datasets using a common column, calculating a new variable (new column) from values of existing variables, these are all tasks which you can perform on spatial or non-spatial data. However today we will explore some exercises in data manipulation which are specific to spatial data analysis. After this session you can truly say you are masters of spatial data manipulation. So let’s get started with that! The main objectives for this session are that by the end you will have: used geocoding methods to translate postcodes into geographic coordinates made interactive point map with leaflet met a new format of spatial shape file called geojson subset points that are within a certain area using a spatial operation created new polygons by generating buffers around points counted the number of points that fall within a polygon (known as points in polygon) These are all very useful tools for the spatial crime analyst, and we will hope to demonstrate this by working through an example project, where you would make use of all of these tools. The packages we will use today are: readr dplyr rjson purrr leaflet RColorBrewer sf janitor Let’s consider the assumption that licenced premises which serve alcohol are associated with increased crimes. We might have some hypotheses about why this may be. One theory might be that some of these serve as crime attractors. Crime attractors are particular places, areas, neighbourhoods, districts which create well-known criminal opportunities to which strongly motivated, intending criminal offenders are attracted because of the known opportunities for particular types of crime. Examples might include bar districts; prostitution areas; drug markets; large shopping malls, particularly those near major public transit exchanges; large, insecure parking lots in business or commercial areas. The intending offender goes to rough bars looking for fights or other kinds of ‘action.’ On the other hand, it is possible that these areas are crime generators. Crime generators are particular areas to which large numbers of people are attracted for reasons unrelated to any particular level of criminal motivation they might have or to any particular crime they might end up committing. Typical examples might include shopping precincts; entertainment districts; office concentrations; or sports stadiums. (To read further in crime attractors vs crime generators turn to your recommended reading of Brantingham, P., &amp; Brantingham, P. (1995). Criminality of place. European journal on criminal policy and research, 3(3), 5-26.). There have since been more developments, for example about crime radiators and absorbers as well (watch this Risky Places lecture from Kate Bowers: to learn more!) It’s possible that some licensed premises attract crimes, due to their reputation. However it is also possible that some of them are simply located in areas that are busy, attracts lots of people for lots of reasons, and crimes occurr as a result of an abundance of opportunities instead. In any case, what we want to do is to examine whether certain outlets have more crimes near them than others. We can do this using open data, some R code, and the spatial operations discussed above. So let’s get to it! 4.1 Getting some (more) data Manchester City Council have an Open Data Catalogue on their website, which you can use to browse through what sorts of data they release to the public. There are a some more and some less interesting data sets made available here. It’s not quite as impressive as the open data from some of the cities in the US such as New York or Dallas but we’ll take it. One interesting data set, especially for our questions about the different alcohol outlets is the Licensed Premises data set. This details all the currently active licenced premises in Manchester. You can see there is a link to download now. As always, there are a few ways you can download this data set. On the manual side of things, you can simply right click on the download link from the website, save it to your computer, and read it in from there, by specifying the file path. Remember, if you save it in your working directory, then you just need to specify the file name, as the working directory folder is where R will first look for this file. 4.1.0.1 Activity 1: Reading data in from the web But, programmers are lazy, and the whole point of using code-based interfaces is that we get to avoid doing unneccessary work, like point-and-click downloading of files. And when data exists online in a suitable format, we can tell R to read the data in from the web directly, and cut out the middle man (that being ourseves in our pointing-and-clicking activity). How can we do this? Well think about what we do when we read in a file. We say, hello R, i would like to create a new object please and I will call this new object my_data. We do this by typing the name we are giving the object and the assignment function &lt;-. Right? Then on the right hand side of the assignment function, there is the value that we are assigning the variable. So it could be a bit of text (such as when you’re creating a shp_name object and you pass it the string \"path to my file\"), or it could be some function, for example when you read a csv file with the read.csv() function. So if we’re reading a csv, we also need to specity where to read the csv from. Where should R look to find this data? This is where normally you are putting in the path to your file, right? Something like: my_data &lt;- read_csv(&quot;path to my file here&quot;) Well what if your data does not live on your laptop or PC? Well, if there is a way that R can still access this data just by following a path, then this approach will still work! So how can we apply this to getting the Licensed Premises data from the web? You know when you right click on the link, and select “Save As…” or whatever you click on to save? You could, also select “Copy Link Address.” This just copies the webpage where this data is stored. Give it a go! Copy the address, and then paste it into your browser. It will take you to a blank page where a forced download of the data will begin. So what if you pasted this into the read.csv() function? my_data &lt;- read_csv(&quot;www.data.com/the_data_i_want&quot;) Well in this case, the my_data object would be assigned the value returned from the read.csv() function reading in the file from the url you provided. File path is no mysterious thing, file path is simply the path to the file you want to read. If this is a website, then so be it. So without dragging this on any further, let’s read in the licensed premises data directly from the web: library(readr) lic_prem &lt;- read_csv(&quot;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## CASEREFERENCE = col_double(), ## LATENIGHTREFRESHMENT = col_logical(), ## ALCOHOLSUPPLY = col_logical(), ## OPENINGHOURS = col_logical(), ## LICENCEENDDATE = col_logical(), ## ONPREMISESALCOHOLSALE = col_logical(), ## OFFPREMISESALCOHOLSALE = col_logical(), ## SUPERVISORADDRESS = col_logical(), ## SUPERVISORPOSTCODE = col_logical(), ## CONDITIONS = col_logical(), ## DECIDEDBY = col_logical(), ## LICENCEURL = col_logical(), ## UPRN = col_logical() ## ) ## ℹ Use `spec()` for the full column specifications. ## Warning: 63264 parsing failures. ## row col expected actual file ## 2272 -- 36 columns 23 columns &#39;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&#39; ## 2273 -- 36 columns 23 columns &#39;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&#39; ## 2274 -- 36 columns 23 columns &#39;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&#39; ## 2275 -- 36 columns 23 columns &#39;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&#39; ## 2276 -- 36 columns 23 columns &#39;http://www.manchester.gov.uk/open/download/downloads/id/169/licensed_premises.csv&#39; ## .... ... .......... .......... ................................................................................... ## See problems(...) for more details. You can always check if this worked by looking to your global environment on the righ hand side and seeing if this ‘lic_prem’ object has appeared. If it has, you should see it has 65535 observations (rows), and 36 variables (columns). Let’s have a look at what this data set looks like. You can use the View() function for this: View(lic_prem) We can see there are some interesting and perhaps less interesting columns in there. There are quite a lot of venues in this list as well. Let’s think about subsetting them. Let’s say we’re interested in city centre manchester. We can see that there is a column for postcodes. We know (from our local domain knowledge) That city centre postcodes are M1-M4. So let’s start by subsetting the data to include these. 4.1.1 Subsetting using pattern matching We could use spatial operations here, and geocode all the postcodes at this point, then use a spatial file of city centre to select only the points contained in this area. The only reason we’re not doing this is because the geocode function takes a bit of time to geocode each address. It would only be about 10 - 15 minutes, but we don’t want to leave you sitting around in the lab for this long, so instead we will try to subset the data using pattern matching in text. In particular we will be using the grepl() function. This function takes a pattern and looks for it in some text. If it finds the pattern, it returns TRUE, and if it does not, it returns FALSE. So you have to pass two parameters to the grepl() function, one of them being the pattern that you want it to look for, and the other being the object in which to search. So for example, if we have an object that is some text, and we want to find if it contains the letter “a,” we would pass those inside the grepl() function, which would tell us TRUE (yes it’s there) or FALSE (no it’s not there): some_text &lt;- &quot;this is some text that has some letter &#39;a&#39;s&quot; grepl(&quot;a&quot;, some_text) ## [1] TRUE You can see this returns TRUE, because there is at least one occurrence of the letter a. If there wasn’t, we’d get FALSE: some_text &lt;- &quot;this is some text tht hs some letters&quot; grepl(&quot;a&quot;, some_text) ## [1] FALSE 4.1.1.1 Activity 2: Pattern matching to find city centre premises So we can use this, to select all the cases where we find the pattern “M1” in the postcode. NOTICE the space in our search pattern. It’s not “M1” it’s “M1.” Can you guess why? Well, M1 will be found in M1 but also in M13, which is the University of Manchester’s postcode, and not the part of city centre in which we are interested. So let’s subset our data by creating a new object city_centre_prems, and using the piping (%&gt;%) and filter() functions from the dplyr package: #remember to load dplyr package if you haven&#39;t already: library(dplyr) #then create the city_centre_prems object: city_centre_prems &lt;- lic_prem %&gt;% filter(grepl(&quot;M1 &quot;, POSTCODE) ) Now we only have 353 observations (see your global environment), which is a much more manageable number. 4.1.2 Geocoding from an address Great OK so we have this list of licensed premises, and we have their address, which is clearly some sort of spatial information, but how would you put this on a map? Any ideas? We can, at the most basic, geocode the postcode. This will put all the establisments to the centroid of the postcode. Postcodes are used in the United Kingdom as alphanumeric codes, that were devised by Royal Mail. A full postcode is known as a “postcode unit” and designates an area with a number of addresses or a single major delivery point. You can search the Royal Mail for information on post codes here.. Here is a map of the postcode areas in Greater Manchester: Now the centroid of the post code area represents the central point of the shapefile. For example, here you can see some polygons with their centroids illustrated by points: This is not quite as precise as geocoding the actual address, but let’s just stick with this approach for now. So we need something that will help us get the coordinates for the relevant post code centroid. For this we can use the Open postcode geo from data.gov.uk. Open Postcode Geo is a postcode dataset and API optimised for geocoding applications. You can use Open Postcode Geo to geocode a dataset, geocode user input, and therefore build a proximity search. Data is derived from the Office for National Statistics postcode database and is free to use, subject to including attributions to ONS, OS (Ordinance Survey) and Royal Mail. Postcodes can be entered at area, district, sector, and unit level - see Postcode map for the geographical relationship between these. We can use the Application Programme Interface (API) to query postcodes and read them directly into R, attaching a latitude and a longitude to our dataframe. 4.1.2.1 Activity 3: Getting address from post code using an API What is an API? I once gave an hour long demo on using APIs, if you would like to watch you can see here: APIs demo. But essentially, the way we use it here, an API is a way for us to query data from the web using an url. In this case, we will use the Open Postcode Geo API (detailed above), and give it an address. In the URL. Then, it will return to us the coordinates of that address. Try this in a browser. Open up chrome, or firefox, or whatever you use and type “http://api.getthedata.com/postcode/” plus your post code, but instead of the space add a plus sign. For example, the uni postcode is “M13 9PL.” So to query the coordinates for the university we use the url: “http://api.getthedata.com/postcode/M13+9PL” You should see a result like this: This is called a JSON file, you can see it has lots of info about this post code, for example it tells us the country is England, and it gives us a value for latitude and for longitude. It’s all very well seeing this in a browser, but how can we get this into R? Well we can use a function called fromJSON() from the rjson package, which reads in JSON files and turns them into the nice dataframes we know and love. Inside the fromJSON() we use the readlines() function to get the data from the URL. We save all this into a geocode_result object: library(rjson) geocode_result &lt;- fromJSON(readLines(&quot;http://api.getthedata.com/postcode/M13+9PL&quot;)) ## Warning in readLines(&quot;http://api.getthedata.com/postcode/M13+9PL&quot;): incomplete ## final line found on &#39;http://api.getthedata.com/postcode/M13+9PL&#39; We get a warning about an incomplete final line. For now ignore this, as it seems we get our data anyway. So what do we get? Well let’s see this geocode_result object: geocode_result ## $status ## [1] &quot;match&quot; ## ## $match_type ## [1] &quot;unit_postcode&quot; ## ## $input ## [1] &quot;M13 9PL&quot; ## ## $data ## $data$postcode ## [1] &quot;M13 9PL&quot; ## ## $data$status ## [1] &quot;live&quot; ## ## $data$usertype ## [1] &quot;large&quot; ## ## $data$easting ## [1] 384591 ## ## $data$northing ## [1] 396711 ## ## $data$positional_quality_indicator ## [1] 1 ## ## $data$country ## [1] &quot;England&quot; ## ## $data$latitude ## [1] &quot;53.466926&quot; ## ## $data$longitude ## [1] &quot;-2.233578&quot; ## ## $data$postcode_no_space ## [1] &quot;M139PL&quot; ## ## $data$postcode_fixed_width_seven ## [1] &quot;M13 9PL&quot; ## ## $data$postcode_fixed_width_eight ## [1] &quot;M13 9PL&quot; ## ## $data$postcode_area ## [1] &quot;M&quot; ## ## $data$postcode_district ## [1] &quot;M13&quot; ## ## $data$postcode_sector ## [1] &quot;M13 9&quot; ## ## $data$outcode ## [1] &quot;M13&quot; ## ## $data$incode ## [1] &quot;9PL&quot; ## ## ## $copyright ## [1] &quot;Contains OS data (c) Crown copyright and database right 2021&quot; ## [2] &quot;Contains Royal Mail data (c) Royal Mail copyright and database right 2021&quot; ## [3] &quot;Contains National Statistics data (c) Crown copyright and database right 2021&quot; Ahh it contains all the information we saw earlier in the browser. How nice! This is great for one postcode at a time, but remember what I said about being lazy, and automating. We want the computer to do the work for us. To do this, we need to set up our query (the url) in a way that lets us give many postcodes in, and get many results out. For that we can use the paste0() function and the gsub() function. paste0() can be used to paste together different bits of text, while gsub() substitutes certain characters (in this case spaces) for other characters (in this case +). So to get from M13 9PL to M13+9PL we use gsub(\" \", \"+\", \"M13 9PL\"), first saying what to replace, then what to replace it with, and finally in what object (or character string). So let’s say we assign our postcode to an object called “address”: address &lt;- &quot;M13 9PL&quot; We can then use this in the combination of paste0 and gsub to build the URL for our query: geocode_result &lt;- fromJSON(readLines(paste0(&quot;http://api.getthedata.com/postcode/&quot;,gsub(&quot; &quot;, &quot;+&quot;, address)))) ## Warning in readLines(paste0(&quot;http://api.getthedata.com/postcode/&quot;, gsub(&quot; &quot;, : ## incomplete final line found on &#39;http://api.getthedata.com/postcode/M13+9PL&#39; You can see we get the same result as above. And if we want only the coordinates we can call them: geocode_result$data$latitude ## [1] &quot;53.466926&quot; geocode_result$data$longitude ## [1] &quot;-2.233578&quot; So how can we apply this to a whole dataframe? Well here I build two functions geocode_addys_getlng() to get the longitudes and geocode_addys_getlat() to get the latitudes. You can unpick this code if you like, but if you want, you’re welcome to just run these to create the functions in your environment and use geocode_addys_getlng &lt;- function(x){ geocode_result &lt;- fromJSON(readLines(paste0(&quot;http://api.getthedata.com/postcode/&quot;,gsub(&quot; &quot;, &quot;&quot;, x)))) return(ifelse(!is.null(geocode_result$data$longitude), geocode_result$data$longitude, NA)) } geocode_addys_getlat &lt;- function(x){ geocode_result &lt;- fromJSON(readLines(paste0(&quot;http://api.getthedata.com/postcode/&quot;,gsub(&quot; &quot;, &quot;&quot;, x)))) return(ifelse(!is.null(geocode_result$data$latitude), geocode_result$data$latitude, NA)) } Now to apply these functions to the whole dataframe, we will use the mutate() function to create a new column for longitue and one for latitude, and for each one apply the relevant function with the map_chr function from the purr package. library(purrr) city_centre_prems &lt;- city_centre_prems %&gt;% mutate(longitude = map_chr(POSTCODE, geocode_addys_getlng), latitude = map_chr(POSTCODE, geocode_addys_getlat)) Be patient, this will take a while, each postcode has to be referenced against their database and the relevant coordinates extracted. For each point you will see a note appear in red, and while R is working you will see the red stop sign on the top right corner of the Console window: Also think about how incredibly fast and easy this actually is, if you consider a potential alternative where you have to manualy find some coordinates for each address. That sounds pretty awful, doesn’t it? Compared to that, setting the above functions running, and stepping away to make a cup of tea is really a pretty excellent alternative, no? Right so hopefully that is done now, and you can have a look at your data again to see what this new column looks like. Remember you can use the View() function to make your data appear in this screen. View(city_centre_prems) And now we have a column called longitude for longitude and a column called latitude for latitude. Neat! I know there was a lot in there, so don’t worry about asking lots of questions on this, but also don’t worry too much if you just run the functions and get the coordinates, as long as you know where the coords come from! 4.2 Making interactive maps with leaflet Thus far we have explored a few approaches to making maps. We made great use of the tmaps package for example in the past few weeks. But now, we are going to step up our game, and introduce Leaflet as one way to easily make some neat maps. It is the leading open-source JavaScript library for mobile-friendly interactive maps. It is very most popular, used by websites ranging from The New York Times and The Washington Post to GitHub and Flickr, as well as GIS specialists like OpenStreetMap, Mapbox, and CartoDB, some of who’s names you’ll recognise from the various basemaps we played with in previous labs. In this section of the lab we will learn how to make really flashy looking maps using leaflet. If you haven’t already, you will need to have installed the following packages to follow along: install.packages(&quot;leaflet&quot;) #for mapping install.packages(&quot;RColorBrewer&quot;) #for getting nice colours for your maps Once you have them installed, load them up with the library() function: 4.2.1 Activity 4: Making an interactive map To make a map, just load the leaflet library: library(leaflet) You then create a map with this simple bit of code: m &lt;- leaflet() %&gt;% addTiles() And just print it: m Not a super usseful map, definitely won’t win map of the week, but it was really easy to make! You might of course want to add some content to your map. 4.2.1.1 Adding points manually: You can add a point manually: m &lt;- leaflet() %&gt;% addTiles() %&gt;% addMarkers(lng=-2.230899, lat=53.464987, popup=&quot;You would be here if it weren&#39;t 2021&quot;) m Or many points manually, with some popup text as well: latitudes = c(53.464987, 53.472726, 53.466649) longitudes = c(-2.230899, -2.245481, -2.243421) popups = c(&quot;You are (not) here&quot;, &quot;Here is another point&quot;, &quot;Here is another point&quot;) df = data.frame(latitudes, longitudes, popups) m &lt;- leaflet(data = df) %&gt;% addTiles() %&gt;% addMarkers(lng=~longitudes, lat=~latitudes, popup=~popups) m 4.2.1.2 Adding data from elsewhere Last time around we added crime data to our map. In this case, we want to be mapping our licensed premises in the city centre, right? So let’s do this: city_centre_prems$latitude &lt;- as.numeric(city_centre_prems$latitude) city_centre_prems$longitude &lt;- as.numeric(city_centre_prems$longitude) m &lt;- leaflet(data = city_centre_prems) %&gt;% addProviderTiles(&quot;Stamen.Toner&quot;) %&gt;% addMarkers(lng=~longitude, lat=~latitude, popup=~as.character(PREMISESNAME), label = ~as.character(PREMISESNAME)) ## Warning in validateCoords(lng, lat, funcName): Data contains 4 rows with either ## missing or invalid lat/lon values and will be ignored m Should be looking familiar as well. Now let’s say you wanted to save this map. You can do this by clicking on the export button at the top of the plot viewer, and choose the Save as Webpage option saving this as a .html file: Then you can open this file with any type of web browser (safari, firefox, chrome) and share your map that way. You can send this to your friends not on this course, and make them jealous of your fancy map making skills. One thing you might have noticed is that we still have some points that are not in Manchester. This should illustrate that the pattern matching approach is really just a work-around. Instead, what we really should be doing to subset our data spatially is to use spatial operations. So now we’ll learn how to do some of these in the next section. 4.3 Spatial operations Spatial operations are a vital part of geocomputation. Spatial objects can be modified in a multitude of ways based on their location and shape. For a comprehensive overview of spatial operations in R I would recommend the relevant chatper Chapter 4: Spatial Operations from the project of Robin Lovelace and Jakub Nowosad, Geocomputation with R. Spatial operations differ from non-spatial operations in some ways. To illustrate the point, imagine you are researching road safety. Spatial joins can be used to find road speed limits related with administrative zones, even when no zone ID is provided. But this raises the question: should the road completely fall inside a zone for its values to be joined? Or is simply crossing or being within a certain distance sufficent? When posing such questions it becomes apparent that spatial operations differ substantially from attribute operations on data frames: the type of spatial relationship between objects must be considered. (Lovelace &amp; Nowosad, 2018) So you can see we can do exciting spatial operations with our spatial data, which we cannot with the non-spatial stuff. For our spatial operations we will be using functions that belong to the sf package. So make sure you have this loaded up: library(sf) 4.3.1 Coordinate reference systems revisited One important note before we begin to do this brings us back to some of the learning from the second session on map projections and coordinate reference systems, like we discussed in the lecture today. We spoke about all the ways of flattening out the earth, and ways of making sense what that means for the maps, and also how to be able to point to specific locations within these. The latter refers to the Coordinate Reference System or CRS the most common ones we will use are WGS 84 and British National Grid. So why are we talking about this? It is important to note that spatial operations that use two spatial objects rely on both objects having the same coordinate reference system If we are looking to carry out operations that involve two different spatial objects, they need to have the same CRS!!! Funky weird things happen when this condition is not met, so beware! So how do we know what CRS our spatial objects are? Well the sf package contains a handy function called st_crs() which let’s us check. All you need to pass into the brackets of this function is the name of the object you want to know the CRS of. So let’s check what is the CRS of our licenced premises: st_crs(city_centre_prems) ## Coordinate Reference System: NA You can see that we get the CRS returned as NA. Can you think of why? Have we made this into a spatial object? Or is this merely a dataframe with a latitude and longitude column? The answer is really in the question here. So we need to convert this to a sf object, or a spatial object, and make sure that R knows that the latitude and the longitude columns are, in fact, coordinates. In the st_as_sf() function we specify what we are transforming (the name of our dataframe), the column names that have the coordinates in them (longitude and latitude), the CRS we are using (4326 is the code for WGS 84, which is the CRS that uses latitude and longitude coordinates (remember BNG uses Easting and Northing)), and finally agr, the attribute-geometry-relationship, specifies for each non-geometry attribute column how it relates to the geometry, and can have one of following values: “constant,” “aggregate,” “identity.” “constant” is used for attributes that are constant throughout the geometry (e.g. land use), “aggregate” where the attribute is an aggregate value over the geometry (e.g. population density or population count), “identity” when the attributes uniquely identifies the geometry of particular “thing,” such as a building ID or a city name. The default value, NA_agr_, implies we don’t know. cc_spatial &lt;- st_as_sf(city_centre_prems, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326, agr = &quot;constant&quot;, na.fail = FALSE) Now let’s check the CRS of this spatial version of our licensed premises: st_crs(cc_spatial) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;World&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] We can now see that we have this coordinate system as WGS 84. We need to then make sure that any other spatial object with which we want to perform spatial operations is also in the same CRS. 4.3.2 Meet a new format of shapefile: geojson GeoJSON is an open standard format designed for representing simple geographical features, along with their non-spatial attributes. It is based on JSON, the JavaScript Object Notation. It is a format for encoding a variety of geographic data structures. Geometries are shapes. All simple geometries in GeoJSON consist of a type and a collection of coordinates. The features include points (therefore addresses and locations), line strings (therefore streets, highways and boundaries), polygons (countries, provinces, tracts of land), and multi-part collections of these types. GeoJSON features need not represent entities of the physical world only; mobile routing and navigation apps, for example, might describe their service coverage using GeoJSON. To tinker with GeoJSON and see how it relates to geographical features, try geojson.io, a tool that shows code and visual representation in two panes. Let’s read in a geoJSON spatial file, again from the web. This particular geojson represents the wards of Greater Manchester. manchester_ward &lt;- st_read(&quot;https://raw.githubusercontent.com/RUMgroup/Spatial-data-in-R/master/rumgroup/data/wards.geojson&quot;) ## Reading layer `wards&#39; from data source `https://raw.githubusercontent.com/RUMgroup/Spatial-data-in-R/master/rumgroup/data/wards.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 215 features and 12 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 351664 ymin: 381168.6 xmax: 406087.5 ymax: 421039.8 ## projected CRS: OSGB 1936 / British National Grid Let’s select only the city centre ward, using the filter() function from dplyr city_centre &lt;- manchester_ward %&gt;% filter(wd16nm == &quot;City Centre&quot;) Let’s see how this looks, using the plot() function: plot(st_geometry(city_centre)) Now we could use this to make sure that our points included in cc_spatial are in fact only licensed premises in the city centre. This will be your first spatial operation. Excited? Let’s do this! 4.3.2.1 Activity 5: Subset points to those within a polygon So we have our polygon, our spatial file of the city centre ward. We now want to subset our point data, the cc_spatial data, which has points representing licensed premises. First things first, we check whether they have the same crs. st_crs(city_centre) == st_crs(cc_spatial) ## [1] FALSE Uh oh! They do not! So what can we do? Well we already know that cc_spatial is in WGS 84, because we made it so a little bit earlier. What about this new city_centre polygon? st_crs(city_centre) ## Coordinate Reference System: ## User input: OSGB 1936 / British National Grid ## wkt: ## PROJCRS[&quot;OSGB 1936 / British National Grid&quot;, ## BASEGEOGCRS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB 1936&quot;, ## ELLIPSOID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4277]], ## CONVERSION[&quot;British National Grid&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,49, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-2, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996012717, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,400000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,-100000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;UK - Britain and UKCS 49°46&#39;N to 61°01&#39;N, 7°33&#39;W to 3°33&#39;E&quot;], ## BBOX[49.75,-9.2,61.14,2.88]], ## ID[&quot;EPSG&quot;,27700]] Aha, the key is in the 27700. This code in fact stands for…. British National Grid…! So what can we do? We can transform our spatial object. Yepp, we can convert between CRS. So let’s do this now. To do this, we can use the st_transform() function. cc_WGS84 &lt;- st_transform(city_centre, 4326) Let’s check that it worked: st_crs(cc_WGS84) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;unknown&quot;], ## AREA[&quot;World&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] Looking good. Triple double check: st_crs(cc_WGS84) == st_crs(cc_spatial) ## [1] TRUE YAY! Now we can move on to our spatial operation, where we select only those points within the city centre polygon. To do this, we can use the st_intersects() function: # intersection cc_intersects &lt;- st_intersects(cc_WGS84, cc_spatial) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar # subsetting cc_intersects &lt;- cc_spatial[unlist(cc_intersects),] have a look at this new cc_intersects object in your environment. How many observations does it have? Is this now fewer than the previous cc_spatial object? Why do you think this is? (hint: you’re removing everything that is outside the city centre polygon) We can plot this too to have a look: # plot plot(st_geometry(cc_WGS84), border=&quot;#aaaaaa&quot;) plot(st_geometry(cc_intersects), col = &quot;red&quot;, add=T) COOL, we have successfully performed our first spatial operation, we managed to subset our points data set to include only those points which are inside the polgon for city centre. See how this was much easier, and more reliable than the hacky workaround using pattern matching? Yay! 4.3.2.2 Activity 6: Building buffers Right, but what we want to do really to go back to our original question. We want to know about crime in and around out areas of interest, in this case our licensed premises. But how can we count this? Well first we will need crime data. Let’s use the same data set from last week. I’m not going over the detail of how to read this in, if you forgot, go back to the notes from last week. crimes &lt;- read_csv(&quot;data/2019-06-greater-manchester-street.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## `Crime ID` = col_character(), ## Month = col_character(), ## `Reported by` = col_character(), ## `Falls within` = col_character(), ## Longitude = col_double(), ## Latitude = col_double(), ## Location = col_character(), ## `LSOA code` = col_character(), ## `LSOA name` = col_character(), ## `Crime type` = col_character(), ## `Last outcome category` = col_character(), ## Context = col_logical() ## ) Notice that in this case the columns are spelled with upper case “L.” You should always familiarise yourself with your data set to make sure you are using the relevant column names. You can see just the column names using the names() function like so : names(crimes) ## [1] &quot;Crime ID&quot; &quot;Month&quot; &quot;Reported by&quot; ## [4] &quot;Falls within&quot; &quot;Longitude&quot; &quot;Latitude&quot; ## [7] &quot;Location&quot; &quot;LSOA code&quot; &quot;LSOA name&quot; ## [10] &quot;Crime type&quot; &quot;Last outcome category&quot; &quot;Context&quot; Arg so messy! Let’s use our handy helpful clean_names() function from the janitor package: library(janitor) crimes &lt;- crimes %&gt;% clean_names() names(crimes) ## [1] &quot;crime_id&quot; &quot;month&quot; &quot;reported_by&quot; ## [4] &quot;falls_within&quot; &quot;longitude&quot; &quot;latitude&quot; ## [7] &quot;location&quot; &quot;lsoa_code&quot; &quot;lsoa_name&quot; ## [10] &quot;crime_type&quot; &quot;last_outcome_category&quot; &quot;context&quot; Or you can have a look at the first 6 lines of your dataframe with the head() function: head(crimes) ## # A tibble: 6 x 12 ## crime_id month reported_by falls_within longitude latitude location lsoa_code ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;NA&gt; 2019… Greater Ma… Greater Man… -2.46 53.6 On or n… E01004768 ## 2 aa1cc4cb… 2019… Greater Ma… Greater Man… -2.44 53.6 On or n… E01004768 ## 3 e513df63… 2019… Greater Ma… Greater Man… -2.44 53.6 On or n… E01004768 ## 4 6ed763df… 2019… Greater Ma… Greater Man… -2.44 53.6 On or n… E01004768 ## 5 780d55b8… 2019… Greater Ma… Greater Man… -2.45 53.6 On or n… E01004768 ## 6 753fa25f… 2019… Greater Ma… Greater Man… -2.44 53.6 On or n… E01004768 ## # … with 4 more variables: lsoa_name &lt;chr&gt;, crime_type &lt;chr&gt;, ## # last_outcome_category &lt;chr&gt;, context &lt;lgl&gt; Or you can view, with the View() function. Now, we have our points that are crimes, right? Well… How do we connect them to our points that are licensed premises? First things first, let’s make sure again that R is aware that this is a spatial set of points, and that the columns latitude and longitude are used to create a geometry. crimes_spatial &lt;- st_as_sf(crimes, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326, agr = &quot;constant&quot;) Next, we should find a way to link each crime to the licenced premise which we might count it against. One approach is to build a buffer around our licensed premises, and say that we will count all the crimes which fall within a specific radius of this licensed premise. What should this radius be? Well this is where your domain knowledge as criminologist comes in. How far away would you consdier a crime to still be related to this pub? 400 meters? 500 meters? 900 meters? 1 km? What do you think? This is again one of them it depends questions. Whatever buffer you choose you should justify, and make sure that you can defend when someone might ask about it, as the further your reach obviously the more crimes you will include, and these might alter your results. So, let’s say we are interested in all crimes that occur within 400 meters of each licensed premise. We chose 400m here as this is the recommended distance for accessible bus stop guidance, so basically as far as people should walk to get to a bus stop (TfL, 2008). So in this case, we want to take our points, which represent the licensed premises, and build buffers of 400 meters around them. You can do with the st_buffer() function: prem_buffer &lt;- st_buffer(cc_intersects, 1) ## Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs, endCapStyle = ## endCapStyle, : st_buffer does not correctly buffer longitude/latitude data ## dist is assumed to be in decimal degrees (arc_degrees). You should get a warning here, like I did above. This message indicates that sf assumes a distance value is given in degrees. This is because we have lat/long data (WSG 48) One quick fix to avoid this message, is to convert to BNG: prem_BNG &lt;- st_transform(cc_intersects, 27700) Now we can try again, with meters prem_buffer &lt;- st_buffer(prem_BNG, 400) Let’s see how that looks: plot(st_geometry(prem_buffer)) plot(st_geometry(prem_BNG), add = T) That should look nice and squiggly. But also it looks like there is quite a lot of overlap here. Should we maybe consider smaller buffers? Let’s look at 100 meter buffers: prem_buffer_100 &lt;- st_buffer(prem_BNG, 100) plot(st_geometry(prem_buffer_100)) plot(st_geometry(prem_BNG), add = T) Still quite a bit of overlap, but this is possibly down to all the licensed premises being very densely close together in the city centre. Well now let’s have a look at our crimes. I think it might make sense (again using domain knowledge) to restrict the analysis to violent crime. So let’s do this: violent_spatial &lt;- crimes_spatial %&gt;% filter(crime_type==&quot;Violence and sexual offences&quot;) Now, remember the CRS is WGS 48 here, so we will need to convert our buffer layer back to this: buffer_WGS84 &lt;- st_transform(prem_buffer_100, 4326) Now let’s just have a look: plot(st_geometry(buffer_WGS84)) plot(st_geometry(violent_spatial), col = &#39;red&#39;, add = T) OKAY, so some crimes fall inside some buffers, others not so much. Well, let’s get to our last spatial operation of the day, the famous points in polygon, to get to answering which licensed premises have the most violent crimes near them. 4.3.2.3 Activity 7: Counting Points in Polygon When you have a polygon layer and a point layer - and want to know how many or which of the points fall within the bounds of each polygon, you can use this method of analysis. In computational geometry, the point-in-polygon (PIP) problem asks whether a given point in the plane lies inside, outside, or on the boundary of a polygon. As you can see, this is quite relevant to our problem, wanting to count how many crimes (points) fall within 100 meters of our licensed premises (our buffer polygons). crimes_per_prem &lt;- violent_spatial %&gt;% st_join(buffer_WGS84, ., left = FALSE) %&gt;% count(PREMISESNAME) You now have a new dataframe, crimes_per_prem which has a column for the name of the premises, a column for the number of violend crimes that fall within the buffer, and a column for the geometry. Take a moment to look at this table. Use the View() function. Which premises have the most violent crimes? Are you surprised? Now as a final step, let’s plot this, going back to leaflet. We can shade by the number of crimes within the buffer, and include a little popup label with the name of the establishment: pal &lt;- colorBin(&quot;RdPu&quot;, domain = crimes_per_prem$n, bins = 5, pretty = TRUE) leaflet(crimes_per_prem) %&gt;% addTiles() %&gt;% addPolygons(fillColor = ~pal(n), fillOpacity = 0.8, weight = 1, opacity = 1, color = &quot;black&quot;, label = ~as.character(PREMISESNAME)) %&gt;% addLegend(pal = pal, values = ~n, opacity = 0.7, title = &#39;Violend crimes&#39;, position = &quot;bottomleft&quot;) It’s not the neatest of maps, with all these overlaps, but we can talk about prettifying maps another day. You’ve done enough today. 4.4 Recap Today we learned to: use geocoding methods to translate postcodes into geographic coordinates make interactive point map with leaflet about a new format of spatial shape file called geojson subset points that are within a certain area using a spatial operation create new polygons by generating buffers around points count the number of points that fall within a polygon (known as points in polygon) "],["more-on-thematic-maps.html", "Chapter 5 More on thematic maps 5.1 Introduction 5.2 Mapping rates, learning from yield mapping 5.3 Binning points 5.4 A note of caution: MAUP 5.5 Replacing polygons with grid or hex shapes 5.6 References and further reading", " Chapter 5 More on thematic maps 5.1 Introduction In this session we are going to discuss some additional features around thematic maps we did not cover in week 3. We are going to discuss how to address some of the problems we confront when we are trying to use use choropleth maps, as well as some alternatives to point based maps. We will also introduce the modifiable area unit problem. Before we do any of this, we need to load the libraries we will use today: library(sf) library(tmap) library(sp) library(spdep) library(DCluster) library(cartogram) library(ggplot2) library(dplyr) library(readxl) library(readr) library(janitor) library(hexbin) library(ggspatial) library(geogrid) Some of the above are new libraries we will use for the first time this week. Remember, if you don’t already have these you will need to install them. 5.1.1 Pro-tip: do I need to install this package? You might have noticed that in your list of available packages you might see more than you remember downloading. The idea of dependencies has come up throughout the semester. Packages have dependencies when their code is dependent on (uses code from) another package. For example, if I write some code that I think will be useful, so I release this in the form of the package “rekaR,” but I use ggplot2 in the code, then ggplot2 will be a dependency of rekaR. As a default, R will install all the dependencies for a package when you install your package. So this way you might end up with some packages there that you didn’t realise you had. Why am I telling you this? Well you should always check if you have a package, before installing it. And I wanted to share with you some neat code from a Stackoverflow discussion (if you are not yet familiar with Stackoverflow you have not been Google-ing your error messages enough) here to do this. I’ll comment it a bit, so you can follow along what it does but you don’t have to if you don’t want to. This is just an optional extra. So as a first step, you have to assign a list of all the packages you have to check to an object. Let’s say I tell you that today we will be using the following packaes: “sp,” “rgdal,” “classInt,” “RColorBrewer,” “ggplot2,” “hexbin,” “ggmap,” “XML,” and “dplyr.” Then you can add these to an object called libs, using the c() function: libs &lt;- c(&quot;sf&quot;, &quot;tmap&quot;, &quot;sp&quot;, &quot;spdep&quot;, &quot;DCluster&quot;, &quot;cartogram&quot;) Now you can run the below bit of code, and you will see in the console an output of what is and isn’t installed, as well as install the packages that are not! for (x in libs){ #cycle through each item in libs object if(x %in% rownames(installed.packages()) == FALSE) { #if the package is not installed print(paste0(&quot;installing &quot;, x, &quot;...&quot;)) #print a message to tell me install.packages(x) #and then install the packages } else{ #otherwise (if it is installed) print (paste0(x, &quot; is already installed &quot;)) #print a message to tell me } library(x, character.only = TRUE) #and then load the packages } ## [1] &quot;sf is already installed &quot; ## [1] &quot;tmap is already installed &quot; ## [1] &quot;sp is already installed &quot; ## [1] &quot;spdep is already installed &quot; ## [1] &quot;DCluster is already installed &quot; ## [1] &quot;cartogram is already installed &quot; As you can see if you read through the comments there, this bit of code checks each package in the list you pass to tbe libs object when you create it, and if it is not installed it installs for you, and if it is, it just loads it for you. It can be a handy bit of code to keep around. 5.1.1.1 Activity 1: Getting some messy data So far, we’ve really been spoilt by using neat and easy-to-access data typical of ‘western’ countries. This week we will have a little exercise in using data when there may not be such clear ways to access neat and tidy data. Specifically, we’re going to look at some data from India! We’ll be using two datasets: A shapefile of India (including boundaries of ‘states’ and ‘districts’) District-level data on households and crop yields First of all, a number of the big universities in the US have repositories (online collections) of GIS data (like shapefiles). There are a few of them that have India shapefiles (Harvard, Stanford) but you need to request access to them for research purposes. If you’re doing your own research and need the shapefile of a country, it’s worth looking on these repositories for it. You can then email them to request access and usually they’ll give access no problem if you’re using it for research. But for now, I’ve already requested access and downloaded the India shapefiles, but please note that these shapefiles are for the purposes of this workshop only. Here’s the link to download the shapefiles: https://www.dropbox.com/sh/6b1wiz3k7v1nl80/AABjYJdC0ETcCNVdbOqbcZxPa?dl=0. TIP: Use this button to select all files at once, then just hit ‘Download!’ (if you are struggling with this, the files are in a .zip file on Blackboard as well) You’ll notice that there are different shapefiles there, download them all for now. We will use them to demonstrate a good example of the problems you encounter when researching countries with less infrastructure. The UK has these ’LSOA’s we covered previously, which are areas that don’t change over time. In India, districts (like counties) are merged and split quite frequently and entire states (collections of districts) can merge and split too! So each shapefile we’ve downloaded will have different boundaries and different names for districts - so which one do we use?! 5.1.1.2 Activity 2: Getting some crop data (attributes) You may be in a situation like the one described above, and you might want to consider for yourself - how can you decide what is the best shapefile to use? Well the answer will really depend on your data set! WHat is it that you want to represent, and what is the best matched shapefile for the job? One way of deciding is to match each shapefile up with our data (like we did for crime in Manchester before) and checking which shapefile has the most matching district and state names with the dataset. So we’ll start by downloading the dataset. There’s an amazing dataset called VDSA (Village Dynamics in South Asia) which covers much of India, containing both agricultural and socio-economic information. Unfortunately, the website interface isn’t great and downloading the data requires you to make a free account and download individual files for each state, which is a massive faff. So instead, I’ve already downloaded the crop production data for just one state, Bihar, and you can download it here: https://www.dropbox.com/sh/mdwuut7mtqafnyt/AAAiqzYm9eK2A8KAwlU4F93-a?dl=0 (this too is also on BlackBoard) Great, now let’s read that data into R and have a look! Just like we read ‘.csv’ files with read.csv, we can read ‘.xlsx’ (Excel) files with the read_xlsx() function from the readxl package: yield_data &lt;- read_xlsx(&#39;data/Bihar.xlsx&#39;) glimpse(yield_data) ## Rows: 688 ## Columns: 59 ## $ STCODE &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ STNAME &lt;chr&gt; &quot;Bihar&quot;, &quot;Bihar&quot;, &quot;Bihar&quot;, &quot;Bihar&quot;, &quot;Bihar&quot;, &quot;Bihar&quot;, &quot;Bihar&quot;… ## $ DIST &lt;dbl&gt; 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 9… ## $ DISTNAME &lt;chr&gt; &quot;Champaran&quot;, &quot;Champaran&quot;, &quot;Champaran&quot;, &quot;Champaran&quot;, &quot;Champara… ## $ YEAR &lt;dbl&gt; 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1… ## $ RICE_TA &lt;dbl&gt; 389.49, 430.45, 450.45, 467.07, 425.50, 447.35, 435.38, 433.3… ## $ RICE_TQ &lt;dbl&gt; 159.79, 338.03, 371.31, 286.41, 346.06, 373.39, 358.86, 322.5… ## $ WHT_TA &lt;dbl&gt; 76.45, 98.85, 113.13, 131.65, 131.65, 140.52, 306.58, 129.65,… ## $ WHT_TQ &lt;dbl&gt; 41.47, 84.97, 108.83, 159.96, 142.44, 229.13, 328.86, 257.18,… ## $ SORG_KA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.02, 0.00, 0.00, 0.02, 0… ## $ SORG_KQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.01, 0… ## $ SORG_RA &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ SORG_RQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ SORG_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.02, 0.00, 0.00, 0.02, 0… ## $ SORG_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.01, 0… ## $ PMLT_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.07, 0.07, 0.07, 0.14, -… ## $ PMLT_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.03, 0.04, 0.02, 0.03, 0.08, 0… ## $ MAIZ_TA &lt;dbl&gt; 33.56, 43.37, 42.01, 41.80, 44.71, 36.64, 39.89, 44.81, 38.19… ## $ MAIZ_TQ &lt;dbl&gt; 22.18, 37.56, 33.87, 28.77, 52.13, 17.61, 58.23, 46.04, 27.10… ## $ FMLT_TA &lt;dbl&gt; 1.65, 1.82, 1.60, 1.56, 1.48, 1.28, 1.60, 1.25, 1.28, 1.67, 1… ## $ FMLT_TQ &lt;dbl&gt; 0.61, 0.99, 0.56, 0.46, 0.85, 0.60, 0.92, 0.79, 0.62, 0.62, 1… ## $ BRLY_TA &lt;dbl&gt; 71.94, 73.87, 60.16, 54.02, 45.74, 39.25, 32.00, 34.79, 34.00… ## $ BRLY_TQ &lt;dbl&gt; 28.53, 47.83, 32.78, 29.92, 31.88, 32.38, 19.30, 15.84, 27.18… ## $ CERL_TA &lt;dbl&gt; 582.15, 666.16, 679.80, 708.12, 718.94, 740.40, 878.08, 702.4… ## $ CERL_TQ &lt;dbl&gt; -1.00, -1.00, -1.00, -1.00, -1.00, 697.53, 803.27, 671.87, 63… ## $ CPEA_TA &lt;dbl&gt; 8.29, 8.49, 6.10, 6.10, 3.71, 3.29, 4.17, 3.13, 2.65, 2.06, 1… ## $ CPEA_TQ &lt;dbl&gt; 4.11, 4.03, 2.59, 2.56, 1.40, 1.56, 2.08, 1.78, 1.85, 1.12, 0… ## $ PPEA_TA &lt;dbl&gt; 9.55, 11.29, 10.51, 10.51, 10.29, 8.93, 8.71, 7.56, 7.01, 6.1… ## $ PPEA_TQ &lt;dbl&gt; 6.53, 11.43, 10.57, 10.57, 6.90, 8.83, 9.57, 5.03, 5.64, 6.12… ## $ MPUL_TA &lt;dbl&gt; 63.32, 61.15, 56.50, 52.53, 50.19, 53.93, 149.34, -1.00, 47.5… ## $ PULS_TA &lt;dbl&gt; 81.16, 80.93, 73.11, 69.14, 64.19, 66.16, 162.22, -1.00, 57.1… ## $ PULS_TQ &lt;dbl&gt; -1.00, -1.00, -1.00, -1.00, -1.00, 34.15, 26.57, 21.64, 27.10… ## $ GNUT_TA &lt;dbl&gt; 0.05, 0.08, 0.07, 0.03, 0.01, 0.02, 0.09, 0.03, 0.07, 0.03, 0… ## $ GNUT_TQ &lt;dbl&gt; 0.03, 0.07, 0.03, 0.02, 0.03, 0.02, 0.07, 0.02, 0.07, 0.03, 0… ## $ SESA_TA &lt;dbl&gt; 0.99, 1.03, 1.77, 1.89, 1.01, 1.07, 1.42, 1.59, 1.20, 1.11, 1… ## $ SESA_TQ &lt;dbl&gt; 0.33, 0.39, 0.45, 0.60, 0.39, 0.54, 0.62, 0.59, 0.40, 0.24, 0… ## $ RM_TA &lt;dbl&gt; 8.17, 8.94, 10.27, 10.98, 10.98, 10.73, 10.60, 8.00, 9.42, 8.… ## $ RM_TQ &lt;dbl&gt; 2.57, 4.97, 4.48, 7.27, 7.28, 6.86, 6.16, 4.04, 4.87, 6.98, 7… ## $ SAFF_TA &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ SAFF_TQ &lt;dbl&gt; -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -… ## $ CAST_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.16, 0.06, -1.00, … ## $ CAST_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.10, 0.04, -1.00, … ## $ LINS_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 8.13, 9.32, -1.00, … ## $ LINS_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 5.21, 4.37, -1.00, … ## $ SUNF_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ SUNF_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ SOYA_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ SOYA_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ OILS_TA &lt;dbl&gt; 19.33, 20.51, 22.06, 21.67, 20.44, 20.51, 19.75, 18.65, 19.90… ## $ OILS_TQ &lt;dbl&gt; -1.00, -1.00, -1.00, -1.00, -1.00, -1.00, -1.00, 9.93, 9.66, … ## $ SCAN_TA &lt;dbl&gt; 46.20, 38.05, 46.11, 52.69, 56.30, 51.10, 50.20, 48.43, 50.54… ## $ SGUR_TQ &lt;dbl&gt; 150.80, 127.60, 193.30, 216.80, 251.50, 183.90, 200.70, 165.1… ## $ COTN_TA &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… ## $ COTN_TQ &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.25, 0.00, 0.00, 0… ## $ FRUT_TA &lt;dbl&gt; 17.14, 16.09, 17.35, 16.09, 18.38, 18.08, 16.63, 17.42, 17.34… ## $ VEGT_TA &lt;dbl&gt; 16.58, 18.91, 21.81, 19.39, 21.10, 20.50, 21.76, 20.26, 21.55… ## $ TFV_TA &lt;dbl&gt; 33.72, 35.00, 39.16, 35.48, 39.48, 38.58, 38.39, 37.68, 38.89… ## $ POTA_TA &lt;dbl&gt; 4.28, 4.79, 7.04, 7.10, 7.30, 6.96, 7.66, 7.23, 8.01, 8.36, 8… ## $ ONIO_TA &lt;dbl&gt; 1.22, 1.39, 1.69, 1.63, 1.78, 1.56, 1.76, 1.74, 1.64, 1.95, 1… After looking at the yield data, you can see it’s quite massive! Each column represents the production in weight (’_TQ’ columns) and the area farmed (’_TA’ columns) for each crop, per district, per year. That’s too much information for us! We just want rice production and area, so we’ll use the ‘select’ function to just pick the columns we want: names of states and districts, the year, and the rice information. yield_data &lt;- yield_data %&gt;% dplyr::select(STNAME, DISTNAME, YEAR, RICE_TA, RICE_TQ) 5.1.1.3 Activity 3: Comparing shapefiles Now we’ve read in our rice yield data, we can test which shapefile will work best with the district names in our yield data. For this, we’ll use a basic ‘for’ loop which will test each shapefile (1951/61/71/81/91/2001) individually. This will show us which shapefile has the most matching district names (across India) with the yield data. Don’t worry about loops in R, they’re something you can learn if you’re interested but otherwise you can copy + paste the code below - so long as you understand what it’s doing overall. Chris’ advanced R will cover loops this week. N.B. You may need to change ‘shapefile_location &lt;-’ to tell R where you have saved your shapefiles! # Make a list of years between 1951 and 2001 at intervals every 10 years and store them as # characters instead of numbers (so we can use them as names of files in the loop) shapefile_years &lt;- lapply(seq(1951, 2001, 10), as.character) # Get a list of district names in the yield data to compare to each shapefile yield_dist_names &lt;- make_clean_names(unique(yield_data$DISTNAME)) # Prepare an output variable district_match_results &lt;- tibble(shapefile_year = character(), district_match_percent = numeric()) # Loop through each item in the &#39;shapefile_years&#39; list and call each item &#39;each_year&#39; for (each_year in shapefile_years){ # Change this if you need to, give the path to the directory with your shapefiles in shapefile_location &lt;- &#39;data/IndiaBoundaryData/&#39; # Read in each shapefile each_shapefile &lt;- st_read(paste0(shapefile_location, each_year, &#39;.shp&#39;)) # Get lists of each shapefile&#39;s district names shpfile_dist_names &lt;- make_clean_names(unique(each_shapefile$NAME)) # Get the percentage of matching district names between yield data and each shpfile dist_match &lt;- round(length(dplyr::intersect(yield_dist_names, shpfile_dist_names))/length(yield_dist_names)*100, 0) # Store results in output variable district_match_results &lt;- bind_rows(district_match_results, tibble(shapefile_year = each_year, district_match_percent = dist_match)) } ## Reading layer `1951&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1951.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 316 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 ## Reading layer `1961&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1961.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 341 features and 50 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 ## Reading layer `1971&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1971.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 360 features and 70 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 ## Reading layer `1981&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1981.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 426 features and 59 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 ## Reading layer `1991&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1991.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 468 features and 144 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 ## Reading layer `2001&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/2001.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 595 features and 170 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 And now to check the output to see which shapefile has the highest proportion of district name matches: print(district_match_results) ## # A tibble: 6 x 2 ## shapefile_year district_match_percent ## &lt;chr&gt; &lt;dbl&gt; ## 1 1951 72 ## 2 1961 72 ## 3 1971 89 ## 4 1981 61 ## 5 1991 61 ## 6 2001 56 # And plot it with ggplot ggplot(district_match_results) + geom_bar(aes(x = shapefile_year, y = district_match_percent), stat = &#39;identity&#39;) So, before continuing you should look at the table and graph you’ve just made and decide which shapefile we should use now. Which one will be the best for representing your attribute table? 5.1.1.4 Activity 4: Joining datasets One last thing before we can continue mapping - joining our yield and shapefile datasets together! Some name cleaning is required first, to make sure they match up. Hopefully you found before that the 1971 shapefile has the highest percentage of district name matches with our yield data - so we’ll read that one in now: # Read in the 1971 shapefile shapefile &lt;- st_read(&#39;data/IndiaBoundaryData/1971.shp&#39;) ## Reading layer `1971&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/IndiaBoundaryData/1971.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 360 features and 70 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 68.11009 ymin: 6.755698 xmax: 97.4091 ymax: 37.0503 ## geographic CRS: WGS 84 # Clean the shapefile state and district names shapefile$NAME &lt;- make_clean_names(shapefile$NAME) shapefile$STATE_UT &lt;- tolower(shapefile$STATE_UT) # Clean the yield data&#39;s state and district names in the same way yield_data$DISTNAME &lt;- gsub(&#39; &#39;, &#39;_&#39;, tolower(yield_data$DISTNAME)) yield_data$STNAME &lt;- tolower(yield_data$STNAME) # Join the two datasets together, the &#39;by&#39; argument in right_join describes which columns # are equivalent in each dataset district_yields &lt;- shapefile %&gt;% right_join(yield_data, by = c(&#39;NAME&#39; = &#39;DISTNAME&#39;, &#39;STATE_UT&#39; = &#39;STNAME&#39;)) %&gt;% dplyr::select(YEAR, NAME, STATE_UT, RICE_TA, RICE_TQ) %&gt;% # &#39;select&#39; only selects the columns we&#39;re interested in (specify select from dplyr!) mutate(rice_yield = RICE_TQ/RICE_TA) %&gt;% rename(district = NAME, state = STATE_UT) # Check output head(district_yields) ## Simple feature collection with 6 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 84.68945 ymin: 24.95576 xmax: 86.07065 ymax: 25.73259 ## geographic CRS: WGS 84 ## YEAR district state RICE_TA RICE_TQ rice_yield geometry ## 1 1966 patna bihar 138.92 16.21 0.1166859 MULTIPOLYGON (((85.02075 25... ## 2 1967 patna bihar 220.10 211.13 0.9592458 MULTIPOLYGON (((85.02075 25... ## 3 1968 patna bihar 263.87 231.91 0.8788798 MULTIPOLYGON (((85.02075 25... ## 4 1969 patna bihar 262.85 178.89 0.6805783 MULTIPOLYGON (((85.02075 25... ## 5 1970 patna bihar 229.71 231.55 1.0080101 MULTIPOLYGON (((85.02075 25... ## 6 1971 patna bihar 189.13 320.56 1.6949188 MULTIPOLYGON (((85.02075 25... 5.2 Mapping rates, learning from yield mapping In previous sessions we discussed how to map rates. It seems a fairly straightforward issue, you calculate a rate by dividing your numerator (eg: number of crimes or rice production) by your denominator (eg: daytime population or farming area). You get your variable with the relevant rate and you map it using a choropleth map. However, things are not always that simple. Rates are funny animals. This popped up in your preparation material as the problem “insensitivity to sample size” and we will demonstrate in the next activity. 5.2.1 Activity 4: The case of Saharsa Let’s look at our yield data, here yield means the amount of rice produced (in tonnes, 1 tonne = 1000kg) per hectare (a square of land with 100 metre-long sides) of rice farmland. summary(district_yields$rice_yield) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.7690 0.9935 1.0560 1.2223 5.5423 We can see that the district with the highest rice production rate in 1971 had a rate of 48,309 tonnes of rice per 100,000 individuals. That is very high. Just to put it into context in the UK is about 0.92. Where is that place? I can tell you is a place call Saharsa. Check it out: # This will show us the row with the highest yield district_yields %&gt;% filter(rice_yield == max(rice_yield)) ## Simple feature collection with 1 feature and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 86.31699 ymin: 25.44194 xmax: 87.12818 ymax: 26.5624 ## geographic CRS: WGS 84 ## YEAR district state RICE_TA RICE_TQ rice_yield geometry ## 1 1971 saharsa bihar 204.86 1135.4 5.542322 MULTIPOLYGON (((87.04089 26... # Subset the &#39;Saharsa&#39; district data saharsa &lt;- subset(district_yields, district == &quot;saharsa&quot;) Saharsa district in Bihar, India is known for high quality corn, much of which is exported here to the UK! But Saharsa isn’t particularly known for its great rice production. So, what’s going on here? Why do we have a high production rate (yield) in Saharsa that makes it look like the rice capital of India? Well, we can see from the output above that this happened in 1971 in Saharsa, so let’s map out the yields of all districts in Bihar in 1971 to investigate: bihar_1971 &lt;- filter(district_yields, YEAR == &#39;1971&#39;) tm_shape(bihar_1971)+ tm_fill(&#39;rice_yield&#39;) ## Warning: The shape bihar_1971 contains empty units. Saharsa is the darkly-shaded district in the top-right, it stands out massively from all other districts! Let’s plot the following year, 1972, for comparison: bihar_1972 &lt;- filter(district_yields, YEAR == &#39;1972&#39;) tm_shape(bihar_1972)+ tm_fill(&#39;rice_yield&#39;) ## Warning: The shape bihar_1972 contains empty units. Now Saharsa is in the lowest yield bracket - 0.2 to 0.4, pretty far off 5.5! Why does its yield value swing so far? print(filter(district_yields, district == &#39;saharsa&#39;, YEAR == &#39;1971&#39;) %&gt;% pull(RICE_TA)) ## [1] 204.86 area_comparison &lt;- district_yields %&gt;% group_by(district) %&gt;% summarise(mean_area = mean(RICE_TA)) %&gt;% dplyr::select(district, mean_area) ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar ## although coordinates are longitude/latitude, st_union assumes that they are planar print(area_comparison) ## Simple feature collection with 18 features and 2 fields (with 2 geometries empty) ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 83.32587 ymin: 21.9699 xmax: 88.29327 ymax: 27.51885 ## geographic CRS: WGS 84 ## # A tibble: 18 x 3 ## district mean_area geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 bhagalpur 151. (((87.1004 25.49808, 87.10085 25.49706, 87.10083 25.… ## 2 champaran 434. (((84.12354 27.51036, 84.12808 27.50942, 84.13081 27… ## 3 darbhanga 361. (((86.07361 26.65632, 86.07475 26.65469, 86.07544 26… ## 4 dhanbad 52.0 (((86.3877 24.05193, 86.38903 24.04741, 86.38946 24.… ## 5 gaya 439. (((85.00572 25.29162, 85.00678 25.29108, 85.00809 25… ## 6 hazaribag 188. (((85.75342 24.81269, 85.7547 24.81224, 85.75471 24.… ## 7 monghyar 180. (((86.13477 25.76138, 86.1353 25.7611, 86.13603 25.7… ## 8 muzaffarpur 324. (((85.66128 26.84723, 85.67135 26.84497, 85.67583 26… ## 9 muzzafarpur 346. EMPTY ## 10 palamau 79.0 (((84.02914 24.62704, 84.03024 24.62684, 84.03216 24… ## 11 patna 217. (((85.02075 25.73202, 85.02852 25.73189, 85.03005 25… ## 12 purnia 478. (((87.06237 26.58623, 87.06268 26.58621, 87.06619 26… ## 13 ranchi 399. (((84.99077 23.69956, 84.99183 23.69665, 84.99219 23… ## 14 saharsa 186. (((87.04089 26.56165, 87.04169 26.56154, 87.04329 26… ## 15 santhal_para… 354. EMPTY ## 16 saran 261. (((84.09505 26.6367, 84.09814 26.63578, 84.09865 26.… ## 17 shahabad 476. (((84.69971 25.75515, 84.7004 25.7505, 84.7004 25.74… ## 18 singhbhum 289. (((85.8843 23.16295, 85.88689 23.16146, 85.88792 23.… Although it doesn’t have the smallest rice farmland area, Saharsa’s rice land is pretty small. Remember that the rate is simply dividing the number of relevant events (rice production) by the exposure variable (in this case area of rice farmland). Most times Saharsa has very low yields ( &lt; 1 tonne per hectare): print(filter(district_yields, district == &#39;saharsa&#39;)) ## Simple feature collection with 44 features and 6 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 86.31699 ymin: 25.44194 xmax: 87.12818 ymax: 26.5624 ## geographic CRS: WGS 84 ## First 10 features: ## YEAR district state RICE_TA RICE_TQ rice_yield ## 1 1966 saharsa bihar 126.51 41.03 0.32432219 ## 2 1967 saharsa bihar 190.54 143.14 0.75123334 ## 3 1968 saharsa bihar 200.53 157.23 0.78407221 ## 4 1969 saharsa bihar 209.53 121.28 0.57881926 ## 5 1970 saharsa bihar 199.40 136.47 0.68440321 ## 6 1971 saharsa bihar 204.86 1135.40 5.54232159 ## 7 1972 saharsa bihar 166.16 85.27 0.51318007 ## 8 1973 saharsa bihar 167.32 107.88 0.64475257 ## 9 1974 saharsa bihar 192.74 147.14 0.76341185 ## 10 1975 saharsa bihar 195.28 16.41 0.08403318 ## geometry ## 1 MULTIPOLYGON (((87.04089 26... ## 2 MULTIPOLYGON (((87.04089 26... ## 3 MULTIPOLYGON (((87.04089 26... ## 4 MULTIPOLYGON (((87.04089 26... ## 5 MULTIPOLYGON (((87.04089 26... ## 6 MULTIPOLYGON (((87.04089 26... ## 7 MULTIPOLYGON (((87.04089 26... ## 8 MULTIPOLYGON (((87.04089 26... ## 9 MULTIPOLYGON (((87.04089 26... ## 10 MULTIPOLYGON (((87.04089 26... Saharsa’s rice production is around 130 tonnes in most years. But it only takes one good year of rice growth and, bang, it goes to the top of the league. So a standard map of rates is bound to be noisy. There is the instability that is introduced by virtue of having areas that may be sparsely populated and in which one single event, like in this case a bumper crop, will produce a very noticeable change in the rate. In fact, if you look at the counties with the highest yields in the yield dataset you will notice all of them are places like Borden, areas that have small areas of rice farmland (RICE_TA). So they have high yields not because they produce massive amounts of rice, but because of the instability of rates. This is a problem that was first noted by epidemiologists doing disease mapping. But a number of other disciplines have now noted this and used some of the approaches developed by public health researchers that confronted this problem when producing maps of disease (techniques and approaches used by spatial epidemiologists are very similar to those used by criminologists -in case you ever think of changing careers or need inspiration for how to solve a crime analysis problem). 5.2.2 Activity 5: Smoothing as a way to address insensitivity to sample size One way of dealing with this is by smoothing the rates. This basically as the word implies aims for a smoother representation that avoids hard spikes associated with random noise. There are different ways of doing that. Some ways use a non-spatial approach to smoothing, using something called a empirical bayesian smoother. How does this work? This approach takes the raw rates and tries to “shrink” them towards the overall average. The formal logic behind the idea of smoothing is situated in a Bayesian framework, in which the distribution of a random variable is updated after observing data. In the Empirical Bayes approach, values for \\(\\alpha\\) and \\(\\beta\\) of the prior Gamma distribution are estimated from the actual data. The smoothed rate is then expressed as a weighted average of the crude rate and the prior estimate. What does this mean? Essentially, we compute a weighted average between the raw rate for each area and the global average across all areas, with weights proportional to the underlying population at risk. What this procedure does is to have the rates of smaller areas (those with a small population at risk) to have their rates adjusted considerably (brought closer to the global average), whereas the rates for the larger areas will barely change. Here we are going to introduce the approach implemented in DCluster, a package developed for epidemiological research and detection of clusters of disease. Specifically, we use the empbaysmooth(Observed, Expected) function, where we specify two values, the expected value (the global average) and the observed value (the rate we actually see). (note for the next part iwht neighbours to work we have to remove the one polygon with the empty geometry with st_is_empty() bihar_1971 &lt;- bihar_1971 %&gt;% filter(!st_is_empty(.)) res &lt;- empbaysmooth(bihar_1971$rice_yield, bihar_1971$RICE_TA*(sum(bihar_1971$rice_yield)/sum(bihar_1971$RICE_TA))) In the new object we generate, which is a list, you have an element which contains the computed rates. We can add those to our dataset: bihar_1971$rice_yieldEBS &lt;- res$smthrr Alternatively instead of shrinking to the global rate, we can shrink to a rate based on the neighbours of each county. In this case, instead of shrinking to a global rate, we shrink to a local rate. We then may be able to take unobserved heterogeneity into account. For this we need the list of neighbours (we will discuss this code in a later session, so for now just trust us we are computing the rate of the areas that surround each counry): bihar_1971 &lt;- bihar_1971 %&gt;% mutate(INTERSECT = st_intersects(.)) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar as.nb.sgbp &lt;- function(x, ...) { attrs &lt;- attributes(x) x &lt;- lapply(x, function(i) { if(length(i) == 0L) 0L else i } ) attributes(x) &lt;- attrs class(x) &lt;- &quot;nb&quot; x } w_sf &lt;- as.nb.sgbp(bihar_1971$INTERSECT) And now that we have the list of neighbours, we can use the EBlocal() function, and here specofy the count of cases (observed), the population at risk (or rice_TA in the case of our crop yields) and then finally the neighbours list we just created above (w_sf): eb2 &lt;- EBlocal(bihar_1971$rice_yield, bihar_1971$RICE_TA, w_sf) bihar_1971$rice_yieldEBSL &lt;- eb2$est bihar_1971$INTERSECT &lt;- NULL We can now plot the maps and compare them : current_style &lt;- tmap_style(&quot;col_blind&quot;) ## tmap style set to &quot;col_blind&quot; ## other available styles are: &quot;white&quot;, &quot;gray&quot;, &quot;natural&quot;, &quot;cobalt&quot;, &quot;albatross&quot;, &quot;beaver&quot;, &quot;bw&quot;, &quot;classic&quot;, &quot;watercolor&quot; map1 &lt;- tm_shape(bihar_1971) + tm_fill(&quot;rice_yield&quot;, style=&quot;quantile&quot;, title = &quot;Raw rate&quot;, palette = &quot;Reds&quot;) + tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) map2&lt;- tm_shape(bihar_1971) + tm_fill(&quot;rice_yieldEBS&quot;, style=&quot;quantile&quot;, title = &quot;EB Smooth&quot;, palette = &quot;Reds&quot;) + tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) map3&lt;- tm_shape(bihar_1971) + tm_fill(&quot;rice_yieldEBSL&quot;, style=&quot;quantile&quot;, title = &quot;Local Smooth&quot;, palette = &quot;Reds&quot;) + tm_layout(legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.title.size = 0.8, legend.text.size = 0.5) tmap_arrange(map1, map2, map3) Notice that the quantiles are not the same, so that will make your comparison difficult! 5.3 Binning points We’re going to move away from area-level data now, and go back to point-level data. In GIS it is often difficult to present point-based data because in many instances there are several different points and data symbologies that need to be shown. As the number of different data points grows they can become complicated to interpret and manage which can result in convoluted and sometimes inaccurate maps. This becomes an even larger problem in web maps that are able to be depicted at different scales because smaller scale maps need to show more area and more data. This makes the maps convoluted if multiple data points are included. In many maps there are so many data points included that little can be interpreted from them. In order to reduce congestion on maps many GIS users and cartographers have turned to a process known as binning. Binning is defined as the process of grouping pairs of locations based on their distance from one another. These points can then be grouped as categories to make less complex and more meaningful maps. Researchers and practitioners often require a way to systematically divide a region into equal-sized portions. As well as making maps with many points easier to read, binning data into regions can help identify spatial influence of neighbourhoods, and can be an essential step in developing systematic sampling designs. This approach to binning generates an array of repeating shapes over a user-specified area. These shapes can be hexagons, squares, rectangles, triangles, circles or points, and they can be generated with any directional orientation. 5.3.1 The Binning Process Binning is a data modification technique that changes the way data is shown at small scales. It is done in the pre-processing stage of data analysis to convert the original data values into a range of small intervals, known as a bin. These bins are then replaced by a value that is representative of the interval to reduce the number of data points. Spatial binning (also called spatial discretization) discretizes the location values into a small number of groups associated with geographical areas or shapes. The assignment of a location to a group can be done by any of the following methods: - Using the coordinates of the point to identify which “bin” it belongs to. - Using a common variable in the attribute table of the bin and the point layers. 5.3.2 Different Binning Techniques Binning itself is a general term used to describe the grouping of a dataset’s values into smaller groups (Johnson, 2011). The bins can be based on a variety of factors and attributes such as spatial and temporal and can thus be used for many different projects. 5.3.2.1 Choropleth maps You might be thinkging, “grouping points into a larger spatial unit, haven’t we already done this when making choropleth maps?” In a way you are right. Choropleth maps are another type of map to that uses binning. Proportional symbol and choropleth maps group similar data points together to show a range of data instead of many individual points. We’ve covered this extensively, and is generally the best approch to consider spatial grouping of your point variables, because the polygons (shapes) to which you are aggregating your points are meaningful. You can group into LSOAs because you want to show variation in neighbourhoods. Or you can group into police force areas because you want to look at differences between those units of analysis. But sometimes there is just not a geography present to meet your needs. Let’s say you are conducting some days of action in Manchester city centre, focusing on antisocial behaviour. You are going to put up some information booths and staff them with officers to engage with the local population about antiscoail behaviour. For these to be most effective, as an analyst you decide that they should go into the areas with the highest count of antisocial beaviour. You want to be very specific about where you put these as well, and so LSOA level would be too broad, you want to zoom in more. One approach can be to split central Manchester into some smaller polygons, and just calculate the number of antisocial behaviour incidents recorded in each. That way you can then decide to put your information booths somewhere inside the top 5 highest count bins. 5.3.2.2 Rectangular binning The aggregation of incident point data to regularly shaped grids is used for many reasons such as normalizing geography for mapping or to mitigate the issues of using irregularly shaped polygons created arbitrarily (such as county boundaries or block groups that have been created from a political process). Regularly shaped grids can only be comprised of equilateral triangles, squares, or hexagons, as these three polygon shapes are the only three that can tessellate (repeating the same shape over and over again, edge to edge, to cover an area without gaps or overlaps) to create an evenly spaced grid. Rectangular binning is the simplest binning method and as such it heavily used. However, there are some reasons why rectangular bins are less preferable over hexagonal bins. Before we cover this, let’s have a look at hexagonal bins. 5.3.2.3 Hexagonal binning In many applications binning is done using a technique called hexagonal binning. This technique uses hexagon shapes to create a grid of points and develops a spatial histogram that shows different data points as a range or group of pairs with common distances and directions. In hexagonal binning the number of points falling within a particular rectangular or hexagon in a gridded surface is what makes the different colors to easily visualize data (Smith, 2012). Hexagonnal binning was first developed in 1987 and today “hexbinning” is conducted by laying a hexagonal grid on top of 2-dimensional data (Johnson, 2011). Once this is done users can conduct data point counts to determine the number of points for each hexagon (Johnson, 2011). The bins are then symbolized differently to show meaningful patterns in the data. 5.3.2.4 Activity 6: Hexbinning So how can we use hexbinning to solve our antisocial behaviour days of action task? Well let’s say we split Manchester city centre into hexagons, and count the number of antisocial behaviour instances in these. We can then identify the top hexagons, and locate our booths somewhere within these. Also let’s get some data. You could go and get this data yourself from police.uk, we’ve been through all the steps for downloading data from there a few times now. But for now, I have a tidied set of data ready for you. This data is one year’s worth of antisocial behaviour from the police.uk data, from May 2016 to May 2017, for the borough of Manchester. We can take our GMP crimes data, and select only the cases from ASB using the crime.type variable. If you want, however, I have already done this, so you can also download from my dropbox using the link here, (or get it from Blackboard): manchester_asb &lt;- read_csv(&quot;https://www.dropbox.com/s/4tk0aps3jfd9nh4/manchester_asb.csv?dl=1&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## X = col_double(), ## Crime.ID = col_logical(), ## Month = col_character(), ## Reported.by = col_character(), ## Falls.within = col_character(), ## Longitude = col_double(), ## Latitude = col_double(), ## Location = col_character(), ## LSOA.code = col_character(), ## LSOA.name = col_character(), ## Crime.type = col_character(), ## Last.outcome.category = col_logical(), ## Context = col_logical(), ## borough = col_character() ## ) This is currently just a text dataframe, so we need to let R know that actually this is a spatial object, who’s geometry can be find in its longitude and latitude coordinates. As we have long/lat we can assure it’s in WGS 84 projection. ma_spatial &lt;- st_as_sf(manchester_asb, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326, agr = &quot;constant&quot;) Now one thing that this does is it consumes our Long and Lat columns into a geometry attribute. This is generally OK, but for the binning we will do, we would like to have them as separate coordinates. To do this, we can use a bespoke function, sfc_as_cols() created by Josh M. London in response to this issue opened on github for the sf package. To create this function here, run the below code: sfc_as_cols &lt;- function(x, names = c(&quot;x&quot;,&quot;y&quot;)) { stopifnot(inherits(x,&quot;sf&quot;) &amp;&amp; inherits(sf::st_geometry(x),&quot;sfc_POINT&quot;)) ret &lt;- sf::st_coordinates(x) ret &lt;- tibble::as_tibble(ret) stopifnot(length(names) == ncol(ret)) x &lt;- x[ , !names(x) %in% names] ret &lt;- setNames(ret,names) dplyr::bind_cols(x,ret) } As we are not covering making your own function much here, don’t worry too much about the above code, but if you’re curious, raise your hand in the labs and we can come around and talk through it. Now finally, let’s use this function we just created to extract the coords to some columns. So in this function, we have to pass as parameters the name of the dataframe, ma_spatial in our case, and also what we want the columnd to be called in a concatenated list (created with the c() function). Let’s call these “lng” and “lat”: ma_spatial &lt;- sfc_as_cols(ma_spatial, c(&quot;lng&quot;, &quot;lat&quot;)) As a first step, we can plot asb in the borough of Manchester using simple ggplot! Remember the data visualisation session from weeks ago? We discussed how ggplot is such a great tool for building visualisations, because you can apply whatever geometry best suits your data. So for us to just have a look at the hexbinned version of our point data of antisocial behaviour, we can use the stat_binhex() function. We can also recreate the thematic map element, as we can use the frequency of points in each hex to shade each hexbin from white (least number of incidents) to red (most nuber of incidents). So let’s have a go: ggplot(ma_spatial, aes(lng, lat)) + #define data and variables for x and y axes stat_binhex() + #add binhex layer (hexbin) scale_fill_gradientn(colours = c(&quot;white&quot;,&quot;red&quot;), name = &quot;Frequency&quot;) #add shading based on number of ASB incidents Neat, but doesn’t quite tell us where that really dark hexbon actually is. So it would be much better if we could do this with a basemap as the backrgound, rather than our grey ggplot theme. Now, we can apply the same code as we used above, for the ggplot, to this ggmap, to add our hexbins on top of this basemap: ggplot(ma_spatial, aes(x = lng, y = lat)) + annotation_map_tile() + stat_binhex(alpha=0.7) + #add binhex layer (hexbin) scale_fill_gradientn(colours = c(&quot;white&quot;,&quot;red&quot;), name = &quot;Frequency&quot;) #add shading based on number of ASB incidents ## Zoom: 10 Now this should give you some more context! Woo! So I mentioned we’d go over some reasons why you should consider aggregating into a hexagon grid rather than other shape: Hexagons reduce sampling bias due to edge effects of the grid shape. The edge effects of bounded space refers to the problem of truncated data that can skew the results of subsequent analyses (we’ll get to this in the next section). This is related to the low perimeter-to-area ratio of the shape of the hexagon. A circle has the lowest ratio but cannot tessellate to form a continuous grid. Hexagons are the most circular-shaped polygon that can tessellate to form an evenly spaced grid. This circularity of a hexagon grid allows it to represent curves in the patterns of your data more naturally than square grids. When comparing polygons with equal areas, the more similar to a circle the polygon is, the closer to the centroid the points near the border are (especially points near the vertices). This means that any point inside a hexagon is closer to the centroid of the hexagon than any given point in an equal-area square or triangle would be (this is due to the more acute angles of the square and triangle versus the hexagon). Hexagons are preferable when your analysis includes aspects of connectivity or movement paths. Due to the linear nature of rectangles, fishnet grids can draw our eyes to the straight, unbroken, parallel lines which may inhibit the underlying patterns in the data. Hexagons tend to break up the lines and allow any curvature of the patterns in the data to be seen more clearly and easily. This breakup of artificial linear patterns also diminishes any orientation bias that can be perceived in fishnet grids. If you are working over a large area, a hexagon grid will suffer less distortion due to the curvature of the earth than the shape of a fishnet grid. Finding neighbors is more straightforward with a hexagon grid. Since the edge or length of contact is the same on each side, the centroid of each neighbor is equidistant. However, with a fishnet grid, the Queen’s Case (above/below/right/left) neighbor’s centroids are N units away, while the centroids of the diagonal (Rook) neighbors are farther away (exactly the square root of 2 times N units away). Since the distance between centroids is the same in all six directions with hexagons, if you are using a distance band to find neighbors or are using the Optimized Hot Spot Analysis, Optimized Outlier Analysis or Create Space Time Cube By Aggregating Points tools, you will have more neighbors included in the calculations for each feature if you are using hexagonal grid as opposed to a fishnet grid. 5.3.2.5 Activity 6: Rectangular binning Now, to again illustrate the differences of different approaches, let’s see what this map would look like with: rectangular binning: ggplot(ma_spatial, aes(x = lng, y = lat)) + annotation_map_tile() + stat_bin2d(alpha=0.7) + scale_fill_gradientn(colours = c(&quot;white&quot;,&quot;red&quot;), name = &quot;Frequency&quot;) ## Zoom: 10 hexagonal binning: ggplot(ma_spatial, aes(x = lng, y = lat)) + annotation_map_tile() + stat_binhex(alpha=0.7) + scale_fill_gradientn(colours = c(&quot;white&quot;,&quot;red&quot;), name = &quot;Frequency&quot;) ## Zoom: 10 a simple “heatmap” (we will discuss these more thoroughly next week): ggplot(ma_spatial, aes(x = lng, y = lat)) + annotation_map_tile() + stat_density2d(aes(fill = ..level.., # value corresponding to discretized density estimates alpha = ..level..), geom = &quot;polygon&quot;) + # creates the bands of differenc colors ## Configure the colors, transparency and panel scale_fill_gradientn(colours = c(&quot;white&quot;,&quot;red&quot;), name = &quot;Frequency&quot;) ## Zoom: 11 Look at the difference between the three maps (hex, rectangle, and density). How would your conclusions change if you were given these maps? Would you make different decisions about where to place your booths for the days of action? Why or why not? Discuss. 5.3.3 Multivariate binning Multivariate binning is another binning method that lets you visualise slightly more complex data. In this method there can be many different variables consisting of different types of data. Like other binning methods the data is typically grouped with the sum or average of the data. Different types of symbology (such as size, shape and color) can also be used to represent this data as well. We won’t be covering this here but just so you can have a look at some examples here. 5.3.4 Benefits of Binning Because of the plethora of data types available and the wide variety of projects being done in GIS, binning is a popular method for mapping complex data and making it meaningful. Binning is a good option for map makers as well as users because it makes data easy to understand and it can be both static and interactive on many different map scales. If every different point were shown on a map it would have to be a very large scale map to ensure that the data points did not overlap and were easily understood by people using the maps. According to Kenneth Field, an Esri Research Cartographer, “Data binning is a great alternative for mapping large point-based data sets which allows us to tell a better story without interpolation. Binning is a way of converting point-based data into a regular grid of polygons so that each polygon represents the aggregation of points that fall within it.” By using binning to create categories of data maps are easier to understand, more accurate and more visually appealing. Hexbin plots can be viewed as an alternative to scatter plots. The hexagon-shaped bins were introduced to plot densely packed sunflower plots. They can be used to plot scatter plots with high-density data. 5.4 A note of caution: MAUP Now that we’ve shown you how to do a lot of spatial crime analysis, we wanted to close with some words of caution. Remember that everything you’ve learned here are just tools that you will be applying to data you are working with, but it’s up to you, the researcher, the analyst, the domain expert, to apply and use these with careful consideration and cautions. This discussion is very much part of spatial crime analysis, and an important field of thought. I borrow here from George Renghert and Brian Lockwood: When spatial analysis of crime is conducted, the analyst should not ignore the spatial units that data are aggregated into and the impact of this choice on the interpretation of findings. Just as several independent variables are considered to determine whether they have statistical significance, a consideration of multiple spatial units of analysis should be made as well, in order to determine whether the choice of aggregation level used in a spatial analysis can result in biased findings. In particular, they highlight four main issues inherent in most studies of space: issues associated with politically bounded units of aggregation, edge effects of bounded space the modifiable aerial unit problem (MAUP) and ways in which the results of statistical analyses can be manipulated by changes in the level of aggregation. In this lab we will focus on MAUP, but if you are interested in this kind of work, you should definitely read their paper to consider the other issues as well. There are techniques that can be used to alleviate each of the methodological difficulties, and they are described in accessible detail in their paper: Rengert, George F., and Brian Lockwood. “Geographical units of analysis and the analysis of crime.” Putting crime in its place. Springer, New York, NY, 2009. 109-122. 5.4.1 What is MAUP? The Modifiable Areal Unit Problem (MAUP) is an important issue for those who conduct spatial analysis using units of analysis at aggregations higher than incident level. It is one of the better-known problems in geography and spatial analysis. This phenomenon illustrates both the need for considering space in one’s analysis, and the fundamental uncertainties that accompany real-world analysis. The MAUP is \"a problem arising from the imposition of artificial units of spatial reporting on continuous geographical phenomena, leading to artifacts or errors are created when one groups data into units for analysis. The classic text on MAUP is the 1983 paper Openshaw, Stan. “The modifiable areal unit problem. CATMOG (Concepts and techniques in modern geography) 38.” Geo Abstracts, Norwich. 1984.. There are two distinct types of MAUP: Scale (i.e. determining the appropriate size of units for aggregation) and zone (i.e. drawing boundaries or grouping). 5.4.1.1 Scale The scale problem involves results that change based on data that are analyzed at higher or lower levels of aggregation (Changing the number of units). For example, evaluating data at the state level vs. Census tract level. The scale problem has moved to the forefront of geographical criminology as a result of the recent interest in small-scale geographical units of analysis. It has been suggested that smaller is better since small areas can be directly perceived by individuals and are likely to be more homogenous than larger areas. - Gerell, Manne. “Smallest is better? The spatial distribution of arson and the modifiable areal unit problem.” Journal of quantitative criminology 33.2 (2017): 293-318. 5.4.1.2 Zone The zonal problem involves keeping the same scale of research (say, at the state level) but changing the actual shape and size of those areas. The basic issue with the MAUP is that aggregate units of analysis are often arbitrarily produced by whom ever is in charge of creating the aggregate units. A classic example of this problem is known as Gerrymandering. Gerrymandering involves shaping and re-shaping voting districts based on the political affiliations of the resident citizenry. The inherent problem with the MAUP and with situations such as Gerrymandering is that units of analysis are not based on geographic principles, and instead are based on political and social biases. For researchers and practitioners the MAUP has very important implications for research findings because it is possible that as arbitrarily defined units of analysis change shape findings based on these units will change as well. When spatial data are derived from counting or averaging data within areal units, the form of those areal units affects the data recorded, and any statistical measures derived from the data. Modifying the areal units therefore changes the data. Two effects are involved: a zoning effect arising from the particular choice of areas at a given scale; and an aggregation effect arising from the extent to which data are aggregated over smaller or larger areas. The modifiable areal unit problem arises in part from edge effect. If you’re interested, in particular about politics and voting, you can read this interesting piece to learn more about gerrymandering 5.4.2 Why does MAUP matter? The practical implications of MAUP are immense for almost all decision-making processes involving GIS technology, since with the availability of aggregated maps, policy could easily focus on issues and problems which might look different if the aggregation scheme used were changed . All studies based on geographical areas are susceptible to MAUP. The implications of the MAUP affect potentially any area level data, whether direct measures or complex model-based estimates. Here are a few examples of situations where the MAUP is expected to make a difference: The special case of the ecological fallacy is always present when Census area data are used to formulate and evaluate policies that address problems at individual level, such as deprivation. Also, it is recognised that a potential source of error in the analysis of Census data is ‘the arrangement of continuous space into defined regions for purposes of data reporting’ The MAUP has an impact on indices derived from areal data, such as measures of segregation, which can change significantly as a result of using different geographical levels of analysis to derive composite measures . The choice of boundaries for reporting mortality ratios is not without consequences: when the areas are too small, the values estimated are unstable, while when the areas are too large, the values reported may be over-smoothed, i.e. meaningful variation may be lost . Gerell, Manne. “Smallest is better? The spatial distribution of arson and the modifiable areal unit problem.” Journal of quantitative criminology 33.2 (2017): 293-318. 5.4.3 What can we do? Most often you will just have to remain aware of the MAUP and it’s possible effects. There are some techniques, that can help you address these issues, and the chapter pointed out at the beginning of this section is a great place to start to explore these. It is possible to use also an alternative, zone-free approach to mapping these crime patterns, perhaps by using kernel density estimation. Here we model the relative density of the points as a density surface - essentially a function of location (x,y) representing the relative likelihood of occurrence of an event at that point. We have covered KDE elsewhere in this course. For the purposes of this course, it’s enough that you know of, and understand the MAUP and its implications. Always be smart when choosing your appropriate spatial unit of analysis, and when you use binning of any form, make sure you consider how and if your conclusions might change compared to another possible approach. 5.4.4 Activity 7: Considering MAUP This is a theoretical activity, think about your answers carefully, discuss if you are in a chatty room, and any questions, do ask! Look at the question for homework 5.1 about the three maps of binning and the hotspot map. Answer this question again, but now in light of what you have learned about MAUP. 5.5 Replacing polygons with grid or hex shapes When you have meaningful spatial units of analysis in your polygons, for example you are interested specifically in Local Autorities, it might make sense to stick with what we did last week, and aggregate the points into these polygons to cheare thematic maps. However, while thematic maps are an accessible and visually appealing method for displaying spatial information, they can also be highly misleading. Irregularly shaped polygons and large differences in the size of areas being mapped can introduce misrepresentation. The message researchers want to get across might be lost, or even worse, misdirect the viewers to erroneous conclusions. This article provides a helpful discussion of the problem illustrating the case with UK election maps. It is worth reading. Fortunately, there are many methods in R to enhance the legibility of geographic information and the interpretability of what it is trying to be communicated. Broadly, the options are: cartogram hexmap grid Selecting the appropriate method might depend on the research question being posed (e.g. clustering) and the data itself. Even once a method has been selected, there are different ways of operationalising them. Let’s explore this using the example of the results of the 2016 EU referendum at Local Authority level, where remain areas clustered in London. A simple thematic map does not necessarily communicate this well because Local Authorities are both small and densely populated in London. You can download the full set of EU referendum result data as a csv from the Electoral Commission webside. Let’s read it straight into R: eu_ref &lt;- read_csv(&quot;https://www.electoralcommission.org.uk/sites/default/files/2019-07/EU-referendum-result-data.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Region_Code = col_character(), ## Region = col_character(), ## Area_Code = col_character(), ## Area = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. OKAY, now we need a shapefile to join it to. Remember when we got the Manchester lsoa shapefile with the boundary selector? Let’s go back, and this time get Local Authority Districts for England. In this case that means select “English Districts, UAs and London Boroughs, 2011”: Once you have the file, download, extract (unzip) and put the folder in your working directory. Mine is in a subfolder in my working directory called data, so I point R inside that folder to find my shape file. las &lt;- st_read(&quot;data/England_lad_2011_gen/england_lad_2011_gen.shp&quot;) ## Reading layer `england_lad_2011_gen&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/England_lad_2011_gen/england_lad_2011_gen.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 326 features and 4 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: 82644.8 ymin: 5349.399 xmax: 655976.9 ymax: 657599.5 ## projected CRS: OSGB 1936 / British National Grid We can now join the EU referendum data, as we have learned in the past weeks: eu_sf &lt;- left_join(las, eu_ref, by = c(&quot;name&quot; = &quot;Area&quot;)) Now we can have a look at these data: ggplot() + geom_sf(data = eu_sf, aes(fill = Pct_Leave)) We can see that in smaller LAs we don’t even really see the result, as the boundary lines pretty much cover everything. Hmm. Now what we can do is transform the shapes. 5.5.0.1 Activity 8: Cartograms The last thing we will do today is make some cartograms! Cartogram types of maps distort reality to convey information. They resize and exaggerate any variable on an attribute value. There are different types of cartograms. Density-equalizing (contiguous) cartograms are your traditional cartograms. In density-equalizing cartograms, map features bulge out a specific variable. Even though it distorts each feature, it remains connected during its creation. On the other hand, you can have Non-Contiguous Cartograms, where features in non-contiguous cartograms don’t have to stay connected. Finally, Dorling Cartogram (named after professor Danny Dorling) uses shapes like circles and rectangles to depict area. These types of cartograms make it easy to recognize patterns. For example, here is a Dorling Cartogram I made of FixMyStreet reports in London: Now we can make our own as well, using the cartogram package. library(cartogram) Within that there is the cartogram() function, which takes 2 arguments, 1 - the shape file (it can be a SpatialPolygonDataFrame or an sf object), and 2 - the variable which it should use to distort the polygon by. In our data set we have a variable “electorate” which refers to the total number of registered electors # construct a cartogram using the percentage voting leave eu_cartogram &lt;- cartogram(eu_sf, &quot;Electorate&quot;) Again this is some labour intensive work, much like the grid making, you have some time to chill now. Maybe read up on the maths behind this tranformation as well, in the paper Dougenik, J. A., Chrisman, N. R., &amp; Niemeyer, D. R. (1985). An Algorithm To Construct Continuous Area Cartograms. In The Professional Geographer, 37(1), 75-81.. I do have a tip for you if you want to make sure the process does not take too long. You can set a parameter in the cartogram function which is the “itermax” parameter. This specifies the maximum number of iterations we are happy with. If you don’t specify it’s set to 15. Let’s set to 5 for the sake of speed: # construct a cartogram using the percentage voting leave eu_cartogram &lt;- cartogram_cont(eu_sf, &quot;Electorate&quot;, itermax = 5) ## Mean size error for iteration 1: 4.11884641547245 ## Mean size error for iteration 2: 21.1096266805133 ## Mean size error for iteration 3: 3.05848955329872 ## Mean size error for iteration 4: 2.51480273537651 ## Mean size error for iteration 5: 2.92812701653567 And if your cartogram has been created, you can now plot again the referendum results, but using the electorate to change the size of the locad authority: ggplot() + geom_sf(data = eu_cartogram, aes(fill = Pct_Leave)) We can now see London much better, and see that darker coloured cluster where much smaller percentage of people voted leave. Okay that’s probably enough for the day. Nice work crime mappers! 5.6 References and further reading 5.6.1 Binning Johnson, Zachary Forest. (18 October 2011). “Hexbins!” Retrieved from: http://indiemaps.com/blog/2011/10/hexbins/ (8 August 2014). Smith, Nate. (25 May 2012). “Binning: An Alternative to Point Maps.” Mapbox. Retrieved from: https://www.mapbox.com/blog/binning-alternative-point-maps/ (8 August 2014). Claudia A Engel’s fantastic R-pub on Making Maps in R. Hexbin Graph Gallery US Drought Hexmap Hexbin with ggplot2 5.6.2 MAUP Gerell, Manne. “Smallest is better? The spatial distribution of arson and the modifiable areal unit problem.” Journal of quantitative criminology 33.2 (2017): 293-318. Openshaw, Stan. “The modifiable areal unit problem. CATMOG (Concepts and techniques in modern geography) 38.” Geo Abstracts, Norwich. 1984.. Rengert, George F., and Brian Lockwood. “Geographical units of analysis and the analysis of crime.” Putting crime in its place. Springer, New York, NY, 2009. 109-122. 5.6.3 Transforming polygons Waldo Tobler (2004) Thirty Five Years of Computer Cartograms, Annals of the Association of American Geographers, 94:1, 58-73, DOI: 10.1111/j.1467-8306.2004.09401004.x Langton, S.H. &amp; Solymosi, R. (2018) ‘Visualising geographic information: examining methods of improving the thematic map.’ RPubs. Available: https://rpubs.com/langton_/visual_geography_study "],["studying-spatial-point-patterns.html", "Chapter 6 Studying spatial point patterns 6.1 What we’ll do today 6.2 Getting the data 6.3 Getting the data into spatstat: the problem with duplicates 6.4 Inspecting our data with spatstat 6.5 Density estimates 6.6 Adding some context 6.7 Spatial point patterns along networks 6.8 Recap", " Chapter 6 Studying spatial point patterns 6.1 What we’ll do today We have now covered quite a bit! You’ve learnt about spatial objects and various formats in which they come and are stored by R, how to produce maps using a variety of packages, and also provided you with a brief introduction to common spatial operations. In what remains of the semester we are going to shift the emphasis and start focusing a bit more on spatial statistics. First we will focus on techniques that are used to explore and analyse points in a geographical space and in subsequent sessions we will cover techniques that are used to analyse spatial data when our unit of analysis are polygons (e.g., postal code areas, census areas, police beats, etc). We will introduce a new R package called spatstat, that was developed for spatial point pattern analysis and modelling. It was written by Adrian Baddeley and Rolf Turner. There is a webpage dedicated to this package. The thickest book in my library, at 810 pages, is dedicated to this package. So as you can imagine the theory and practice of spatial pattern analysis is something one could devote an entire course to. You can get a pdf document used in a course the authors of this package develop here. In our course we are only going to provide you with an introductory practical entry into this field of techniques. If this package is not installed in your machine, make sure you install it before we carry on. We will be using the following packages today: - readxl - janitor - sf - ggplot2 - dplyr - spatstat - raster - leaflet Let’s load them now: library(readxl) library(janitor) library(sf) library(ggplot2) library(dplyr) library(spatstat) library(raster) library(leaflet) 6.2 Getting the data This week we will work with some slightly larger data sets than before: we will have one year’s worth of police-recorded crime data for Greater Manchester Police from the police.uk website. Within that, we will focus on burglary in the Fallowfield area. The code below has already been explained and used in previous sessions, so we won’t go over the detail again. But rather than cut and paste automatically, try to remember what each line of code is doing. If you have any questions about it, do ask in the lab, remember - practice makes perfect! By the way, the police data for Manchester we have used in previous sessions correspond to only one month of the year. Here we are using a full year worth of data, so the data import will take a bit longer. We’ll also be using a geojson shape file of wards. From this we will select Fallowfield ward specifically. You can get both the geojson file of wards and the excel file of crimes from Blackboard. Save to your computer, and remember to put it in the right place for the project! Here I saved mine into the “data” subfolder. #Read a geojson file with Manchester wards (remember we learned about geojson files in week 4) manchester_ward &lt;- st_read(&quot;data/wards.geojson&quot;) ## Reading layer `wards&#39; from data source `/Users/reka/Desktop/crime_mapping_textbook/data/wards.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 215 features and 12 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 351664 ymin: 381168.6 xmax: 406087.5 ymax: 421039.8 ## projected CRS: OSGB 1936 / British National Grid #Create a new object that only has the fallowfield ward df1 &lt;- manchester_ward %&gt;% filter(wd16nm == &quot;Fallowfield&quot;) #Change coordinate systems fallowfield &lt;- st_transform(df1, 4326) #Get rid of objects we no longer need rm(manchester_ward) rm(df1) #Read Greater Manchester police data crimes &lt;- read_xlsx(&quot;data/gmp_crimes_2021.xlsx&quot;) %&gt;% clean_names() burglary &lt;- filter(crimes, crime_type == &quot;Burglary&quot;) #Transform the dataframe with crime information into a sf object burglary_spatial &lt;- st_as_sf(burglary, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326, agr = &quot;constant&quot;) #Select only the crimes that take place within the space defined by the Ward boundaries # intersection bur_fal &lt;- st_intersects(fallowfield, burglary_spatial) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar # subsetting bur_fal &lt;- burglary_spatial[unlist(bur_fal),] #again remove things we don&#39;t need rm(crimes) rm(burglary) Now we have all our data cleaned and all our files prepared. Let’s see the results! ggplot() + geom_sf(data = fallowfield) + geom_sf(data = bur_fal) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.background = element_blank()) In the point pattern analysis literature each point is often referred to as an event and these events can have marks, attributes or characteristics that are also encoded in the data. In our spatial object one of these marks is the type of crime (although in this case it’s of little interest since we have filtered on it). 6.3 Getting the data into spatstat: the problem with duplicates So let’s start using spatstat.The first thing we need to do is to transform our sf object into a ppp object which is how spatstat likes to store its point patterns. Unfortunately, spatstat and many other packages for analysis of spatial data precede sf, so the transformation is a bit awkard. Also before we do that, it is important to realise that a point pattern is defined as a series of events in a given area, or window, of observation. It is therefore extremely important to precisely define this window. In spatstat the function owin() is used to set the observation window. However, the standard function takes the coordinates of a rectangle or of a polygon from a matrix, and therefore it may be a bit tricky to use. Luckily the package maptools provides a way to transform a SpatialPolygons into an object of class owin, using the function as.owin(). Here are the steps: First we transform the CRS of our Falllowfield polygon into projected coordinates (British National Grid) as opposed to geographic coordinates (WGS84) : fallowfield_proj &lt;- st_transform(fallowfield, 27700) Then we use the as.owin function to define the window. window &lt;- as.owin(fallowfield_proj) Now, use the class function and print the window object to check that this worked: class(window) ## [1] &quot;owin&quot; window ## window: polygonal boundary ## enclosing rectangle: [382951.5, 385869.8] x [393616.3, 394988.8] units Now that we have created the window as an owin object let’s get the points. First we will extract the coordinates from our sf point data into a matrix: bur_fal &lt;- st_transform(bur_fal, 27700) #we must transform these too to match our window in BNG sf_bur_fal_coords &lt;- matrix(unlist(bur_fal$geometry), ncol = 2, byrow = T) Then we use the ppp function to create the object using the information from our matrix and the window that we created. bur_ppp &lt;- ppp(x = sf_bur_fal_coords[,1], y = sf_bur_fal_coords[,2], window = window, check = T) ## Warning: data contain duplicated points plot(bur_ppp) Notice the warning message about duplicates. In spatial point pattern analysis an issue of significance is the presence of duplicates. The statistical methodology used for spatial point pattern processes is based largely on the assumption that processes are simple, that is, that the points cannot be coincident. That assumption may be unreasonable in many contexts (for example, the literature on repeat victimisation indeed suggests that we should expect the same households to be at a higher risk of being hit again). Even so the point (no pun intended) is that “when the data has coincidence points, some statistical procedures will be severely affected. So it is always strongly advisable to check for duplicate points and to decide on a strategy for dealing with them if they are present” (Baddeley et al., 2016: 60). We can check the duplication in a ppp object with the following syntax: any(duplicated(bur_ppp)) ## [1] TRUE To count the number of coincidence points we use the multiplicity() function. This will return a vector of integers, with one entry for each observation in our dataset, giving the number of points that are identical to the point in question (including itself). multiplicity(bur_ppp) If you want to know how many locations have more than one event you can use: sum(multiplicity(bur_ppp) &gt; 1) ## [1] 255 That’s quite something. 255 points here share coordinates. ggplot() + geom_sf(data = fallowfield) + geom_sf(data = bur_fal, alpha = 0.4) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.background = element_blank()) In the case of crime, as we have hinted some of this may be linked to the nature of crime itself. Hint: repeat victimisation. However, this pattern of duplication is fairly obvious across all crime categories in the police.uk website. This is due to the way in which spatial anonymisation of police.uk data is carried out. This is done using geomasking, whereby there exist a pre-determined list of points that each crime event gets “snapped” to its nearest one. So, the coordinates provided in the open data are not the exact locations of crimes, but they come from a list of points generated for purposes of data publication. You can see the details here. This process is likely inflating the amount of duplication we observe, because each snap point might have many crimes near it, resulting in those crimes being geo-coded to the same exact location. So keep in mind when analysing and working with this data set that it is not the same as working with the real locations. If you are interested in the effects of this read the paper Lisa Tompson, Shane Johnson, Matthew Ashby, Chloe Perkins &amp; Phillip Edwards (2015) UK open source crime data: accuracy and possibilities for research, Cartography and Geographic Information Science, 42:2, 97-111, DOI: 10.1080/15230406.2014.972456. What to do about duplicates in spatial point pattern analysis is not always clear. You could simply delete the duplicates, but of course that may ignore issues such as repeat victimisation. You could also use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space. Which again, may ignore things like repeat victimisation. Another alternative is to make each point “unique” and then attach the multiplicites of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks. If you were to be doing this for real you would want access to the real thing, not this public version of the data and then go for the latter solution suggested above. We don’t have access to the source data, so for the sake of simplicity and so that we can illustrate how spatstat works we will instead add some jittering to the data. The first argument for the function is the object, retry asks whether we want the algorithm to have another go if the jittering places a point outside the window (we want this so that we don’t loose points), and the drop argument is used to ensure we get a ppp object as a result of running this function (which we do). jitter_bur &lt;- rjitter(bur_ppp, retry=TRUE, nsim=1, drop=TRUE) plot(jitter_bur) Notice the difference with the original plot. Can you see how the circumferences do not overlap perfectly now? 6.4 Inspecting our data with spatstat This package supports all kind of exploratory point pattern analysis. One example of this is quadrant counting. One could divide the window of observation into quadrants and count the number of points into each of these quadrants. For example, if we want four quadrants along the X axis and 3 along the Y axis we could used those parameters in the quadratcount() function. Then we just use standard plotting functions from R base. Q &lt;- quadratcount(jitter_bur, nx = 4, ny = 3) plot(jitter_bur) plot(Q, add = TRUE, cex = 2) In the video lectures for this week, Luc Anselin introduced the notion of complete spatial randomness (CSR for short). When we look at a point pattern process the first step in the process is to ask whether it has been generated in a random manner. Under CSR, points are independent of each other and have the same propensity to be found at any location. We can generate data that conform to complete spatial randomness using the rpoispp() function. The r at the beginning is used to denote we are simulating data (you will see this is common in R) and we are using a Poisson point process, a good probability distribution for these purposes. Let’s generate 223 points in a random manner: plot(rpoispp(223)) You will notice that the points in a homogeneous Poisson process are not ‘uniformly spread’: there are empty gaps and clusters of points. Run the previous command a few times. You will see the map generated is different each time. In classical literature, the homogeneous Poisson process (CSR) is usually taken as the appropriate ‘null’ model for a point pattern. Our basic task in analysing a point pattern is to find evidence against CSR. We can run a Chi Square test to check this. So, for example: quadrat.test(jitter_bur, nx = 3, ny = 2) ## ## Chi-squared test of CSR using quadrat counts ## ## data: jitter_bur ## X2 = 248.71, df = 5, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## ## Quadrats: 6 tiles (irregular windows) Observing the results we see that the p value is well below convential standards for rejection of the null hypothesis. Observing our data of burglary in Fallowfield would be extremely rare if the null hypothesis was true. We can then conclude that the burglary data is not randomly distributed in the observed space. But no cop nor criminologist would really question this. They would rarely be surprised by your findings! We do know that crime is not randomly distributed in space. 6.5 Density estimates In the presentations by Luc Anselin and the recommended reading materials we introduced the notion of density maps. Kernel density estimation involves applying a function (known as a “kernel”) to each data point, which averages the location of that point with respect to the location of other data points. The surface that results from this model allows us to produce isarithmic maps, also referred to in common parlor as heatmaps. Beware though, cartographers really dislike this common parlor. We saw this kind of maps when covering the various types of thematic maps. Kernel density estimation maps are very popular among crime analysts. According to Chainey (2012), 9 out of 10 intelligence professionals prefer it to other techniques for hot spot analysis. As compared to visualisations of crime that relies on point maps or thematic maps of geographic administrative units (such as LSOAs), kernel density estimation maps are considered best for location, size, shape and orientation of the hotspot (Chainey, 2012). Spencer Chainey and his colleagues (2008) have also suggested that this method produces some of the best prediction accuracy. The areas identified as hotspots by KDE (using historical data) tend to be the ones that better identify the areas that will have high levels of crime in the future. Yet, producing these maps (as with any map, really) requires you to take a number of decisions that will significantly affect the resulting product and the conveyed message. Like any other data visualisation technique they can be powerful, but they have to be handled with great care. Essentially this method uses a statistical technique (kernel density estimation) to generate a smooth continuous surface aiming to represent the density or volume of crimes across the target area. The technique, in one of its implementations (quartic kernel), is described in this way by Eck and colleagues (2005): “a fine grid is generated over the point distribution; a moving three-dimensional function of a specified radius visits each cell and calculates weights for each point within the kernel’s radius. Points closer to the centre will receive a higher weight, and therefore contribute more to the cell’s total density value; and final grid cell values are calculated by summing the values of all kernel estimates for each location” (Reproduced from Eck et al. 2012) The values that we attribute to the cells in crime mapping will typically refer to the number of crimes within the area’s unit of measurement. We don’t have the time to elaborate further on this technique now, but if you did the required reading you should have at least a notion of how this works. Let’s produce one of this density maps: ds &lt;- density(jitter_bur) class(ds) ## [1] &quot;im&quot; plot(ds, main=&#39;Burglary density in Fallowfield&#39;) The density function is estimating a kernel density estimate. Density is nothing but the number of points per unit area. This method computes the intensity continuously across the study area and the object returns a raster image. To perform this analysis in R we need to define the bandwidth of the density estimation, which basically determines the area of influence of the estimation. There is no general rule to determine the correct bandwidth; generally speaking if the bandwidth is too small the estimate is too noisy, while if bandwidth is too high the estimate may miss crucial elements of the point pattern due to oversmoothing (Scott, 2009). The key argument to pass to the density method for point patterm objects is sigma=, which determines the bandwidth of the kernel. In spatstat the functions bw.diggle(), bw.ppl(), and bw.scott() can be used to estimate the bandwidth according to difference methods. The helpfiles recommend the use of the first two. These functions run algorithms that aim to select an appropriate bandwith. bw.diggle(jitter_bur) ## sigma ## 3.021559 bw.ppl(jitter_bur) ## sigma ## 16.77736 bw.scott(jitter_bur) ## sigma.x sigma.y ## 244.40391 86.18499 You can see the Diggle algorithm gives you the narrower bandwith. We can test how they work with our dataset using the following code: par(mfrow=c(2,2)) plot(density.ppp(jitter_bur, sigma = bw.diggle(jitter_bur),edge=T), main = paste(&quot;h = 0.000003&quot;)) plot(density.ppp(jitter_bur, sigma = bw.ppl(jitter_bur),edge=T), main=paste(&quot;h =0.0005&quot;)) plot(density.ppp(jitter_bur, sigma = bw.scott(jitter_bur)[2],edge=T), main=paste(&quot;h = 0.0008&quot;)) plot(density.ppp(jitter_bur, sigma = bw.scott(jitter_bur)[1],edge=T), main=paste(&quot;h = 0.004&quot;)) Baddeley et (2016) suggest the use of the bw.ppl() algorithm because in their experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if your purpose it to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best. Apart from selecting the bandwidth we also need to specify the particular kernel we will use. In density estimation there are different types of kernel (as illustrated below): Source: wikepedia You can read more about kernel types in the Wikipedia entry. This relates to the type of kernel drawn around each point in the process of counting points around each point. The use of these functions will result in slightly different estimations. They relate to the way we weight points within the radius: “The normal distribution weighs all points in the study area, though near points are weighted more highly than distant points. The other four techniques use a circumscribed circle around the grid cell. The uniform distribution weighs all points within the circle equally. The quartic function weighs near points more than far points, but the fall off is gradual. The triangular function weighs near points more than far points within the circle, but the fall off is more rapid. Finally, the negative exponential weighs near points much more highly than far points within the circle and the decay is very rapid.” (Levine, 2013: 10.10). Which one to use? Levine (2013) produces the following guidance: “The use of any of one of these depends on how much the user wants to weigh near points relative to far points. Using a kernel function which has a big difference in the weights of near versus far points (e.g., the negative exponential or the triangular) tends to produce finer variations within the surface than functions which weight more evenly (e.g., the normal distribution, the quartic, or the uniform); these latter ones tend to smooth the distribution more. However, Silverman (1986) has argued that it does not make that much difference as long as the kernel is symmetrical. Chainey (2013) suggest that in his experience most crime mappers prefer the quartic function, since it applies greater weight to crimes closer to the centre of the grid. The authors of the CrimeStat workbook (Smith and Bruce, 2008), on the other hand, suggest that the choice of the kernel should be based in our theoretical understanding of the data generating mechanisms. By this they mean that the processes behind spatial autocorrelation may be different according to various crime patterns and that this is something that we may want to take into account when selecting a particular function. They provide a table with some examples that may help you to understand what they mean: (Source: Smith and Bruce, 2008.) The default kernel in density.ppp() is the gaussian. But there are other options. We can use the epanechnikov, quartic or disc. There are also further options for customisation. We can compare these kernels: par(mfrow=c(2,2)) plot(density.ppp(jitter_bur, sigma = bw.ppl(jitter_bur),edge=T), main=paste(&quot;Gaussian&quot;)) plot(density.ppp(jitter_bur, kernel = &quot;epanechnikov&quot;, sigma = bw.ppl(jitter_bur),edge=T), main=paste(&quot;Epanechnikov&quot;)) plot(density.ppp(jitter_bur, kernel = &quot;quartic&quot;, sigma = bw.ppl(jitter_bur),edge=T), main=paste(&quot;Quartic&quot;)) plot(density.ppp(jitter_bur, kernel = &quot;disc&quot;, sigma = bw.ppl(jitter_bur),edge=T), main=paste(&quot;Disc&quot;)) When reading these maps you need to understand you are only looking at counts of crime in a smooth surface. Nothing more, nothing less. Unlike with choropleth maps we are not normalising the data. We are simply showing the areas where there is more crime, but we are not adjusting for anything (like number of people in the area, or number of houses to burgle). So, it is important you keep this in the back of your mind. As this comic suggests you may end up reading too much into it if you don’t remember this. There are ways to produce density maps adjusting for a second variable, such as population size, but we do not have the time to cover this. There are also general considerations to keep in mind. Hot spots of crime are a simply a convenient perceptual construct. As Ned Levine (2013: 7.1) highlights “Hot spots do not exist in reality, but are areas where there is sufficient clustering of certain activities (in this case, crime) such that they get labeled such. There is not a border around these incidents, but a gradient where people draw an imaginary line to indicate the location at which the hot spot starts.” Equally, there is not a unique solution to the identification of hot spots. Different techniques and algorithms will give you different answers. As Levine (2013: 7.7) emphasises: “It would be very naive to expect that a single technique can reveal the existence of hot spots in a jurisdiction that are unequivocally clear. In most cases, analysts are not sure why there are hot spots in the first place. Until that is solved, it would be unreasonable to expect a mathematical or statistical routine to solve that problem.” So, as with most data analysis exercises one has to try different approaches and use professional judgement to select a particular representation that may work best for a particular use. Equally, we should not reify what we produce and, instead, take the maps as a starting point for trying to understand the underlying patterns that are being revealed. Critically you want to try several different methods. You will be more persuaded a location is a hot spot if several methods for hot spot analysis point to the same location. 6.6 Adding some context Often it is convenient to use a basemap to provide context. In order to do that we first need to turn the image object generated by the spatstat package into a raster object, a more generic format for raster image used in R. Remember rasters from the first week? Now we finally get to use them a bit! library(raster) dmap1 &lt;- density.ppp(jitter_bur, sigma = bw.ppl(jitter_bur),edge=T) r1 &lt;- raster(dmap1) #remove very low density values r1[r1 &lt; 0.0001 ] &lt;- NA class(r1) ## [1] &quot;RasterLayer&quot; ## attr(,&quot;package&quot;) ## [1] &quot;raster&quot; Now that we have the raster we can add it to a basemap. Two-dimensional RasterLayer objects (from the raster package) can be turned into images and added to Leaflet maps using the addRasterImage() function. The addRasterImage() function works by projecting the RasterLayer object to EPSG:3857 and encoding each cell to an RGBA color, to produce a PNG image. That image is then embedded in the map widget. It’s important that the RasterLayer object is tagged with a proper coordinate reference system. Many raster files contain this information, but some do not. Here is how you’d tag a raster layer object “r1” which contains WGS84 data: library(leaflet) #make sure we have right CRS, which in this case is British National Grid epsg27700 &lt;- &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs&quot; crs(r1) &lt;- sp::CRS(epsg27700) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Airy 1830 ellipsoid in Proj4 ## definition #we also create a colour palet pal &lt;- colorNumeric(c(&quot;#0C2C84&quot;, &quot;#41B6C4&quot;, &quot;#FFFFCC&quot;), values(r1), na.color = &quot;transparent&quot;) #and then make map! leaflet() %&gt;% addTiles() %&gt;% addRasterImage(r1, colors = pal, opacity = 0.8) %&gt;% addLegend(pal = pal, values = values(r1), title = &quot;Burglary map&quot;) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded ellps WGS 84 in Proj4 definition: +proj=merc +a=6378137 ## +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null ## +wktext +no_defs +type=crs ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded datum World Geodetic System 1984 in Proj4 definition ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded ellps WGS 84 in Proj4 definition: +proj=merc +a=6378137 ## +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null ## +wktext +no_defs +type=crs ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded datum World Geodetic System 1984 in Proj4 definition And there you have it. Perhaps those familiar with Fallowfield have some guesses as to what may be going on there? 6.7 Spatial point patterns along networks Have a look at these maps. Can we say that the spatial point process is random here? Can you identify the areas where we have hotspots of crime? Think about these questions for a little while. (Source: Okabe and Sugihara, 2012) Ok, so most likely you concluded that the process wasn’t random, which it isn’t in truth. It is also likely that you identified a number of potential hotspots? Now, look at the two maps below: (Source: Okabe and Sugihara, 2012) We are representing the same spatial point pattern process in each of them. But we do have additional information in map B. We now know the street layout. The structure we observed in the map is accounted by the street layout. So what look like a non random spatial point process when we considered the full two dimensional space, now looks less random when we realise that the points can only appear alongside the linear network. This problem is common in criminal justice applications. Crime is geocoded alongside a linear street network. Even if in physical space crime can take place along a spatial continuum, once crime is geocoded it will only be possible alongside the street network used for the geocoding process. For exploring this kind of spatial point pattern processes along networks we need special techniques. Some researchers have developed special applications, such as SANET. The spatstat package also provides some functionality for this kind of data structures. In spatstat a point pattern on a linear network is represented by an object of class lpp. The functions lpp() and as.lpp() convert raw data into an object of class lpp (but they require a specification of the underlying network of lines, which is represented by an object of class linnet). For simplicity and illustration purposes we will use the chicago dataset that is distributed as part of the spatstat package. The chicago data is of class lpp and contains information on crime in an area of Chicago. data(&quot;chicago&quot;) plot(chicago) summary(chicago) ## Multitype point pattern on linear network ## 116 points ## Linear network with 338 vertices and 503 lines ## Total length 31150.21 feet ## Average intensity 0.003723891 points per foot ## Types of points: ## frequency proportion intensity ## assault 21 0.18103450 0.0006741528 ## burglary 5 0.04310345 0.0001605126 ## cartheft 7 0.06034483 0.0002247176 ## damage 35 0.30172410 0.0011235880 ## robbery 4 0.03448276 0.0001284100 ## theft 38 0.32758620 0.0012198950 ## trespass 6 0.05172414 0.0001926151 ## Enclosing window: rectangle = [0.3894, 1281.9863] x [153.1035, 1276.5602] feet An lpp object contains the linear network information, the spatial coordinates of the data points, and any number of columns of marks (in this case the mark is telling us the type of crime we are dealing with). It also contains the local coordinates seg and tp for the data points. The local coordinate seg is an integer identifying the particular street segment the data point is located in. A segment is each of the sections of a street between two vertices (marking the intersection with another segment). The local coordinate tp is a real number between 0 and 1 indicating the position of the point within the segement: tp=0 corresponds to the first endpoint and tp=1 correspond to the second endpoint. The visual inspection of the map suggest that the intensity of crime along the network is not spatially uniform. Crime seems to be concentrated in particular segments. Like we did before we can estimate the density of data points along the networks using Kernel estimation (with the density.lpp() function), only now we only look at the street segments (rather than areas of the space that are outside the segments). The authors of the package are planning to introduce methods for automatic bandwidth selection but for now this is not possible, so we have to select a bandwidth. We could for example select 60 feet. d60 &lt;- density.lpp(unmark(chicago), 60) We use unmark() to ignore the fact the data points are marked (that is they provide marks with informtation, in this case about the crime type). By using unmark() in this example we will run density estimation for all crimes (rather than by type of crime). We can see the results below: plot(d60) If rather than colour you want to use the thickness of the street segment to identify hotpspots you would need to modify the code as shown below: plot(d60, style=&quot;width&quot;, adjust=2.5) This is very important for crime research, as offending will be constrained by all sorts of networks. Traditionally, hotspot analysis has been directed at crimes that are assumed to be situated across an infinite homogeneous environment (e.g., theft of motor vehicle), we must develop an increased awareness of perceptible geographical restrictions. There has been increasing recognition in recent years that the spatial existence of many phenomena is constrained by networks. These networks may be roads or rail networks, but there may be many more: Environmental crimes could exist along waterways such as streams, canals, and rivers; and thefts of metal could occur along utility networks such as pipelines. Those sociologically inclined might be able to offer more examples in the way of interpersonal networks. Tompson, Lisa, Henry Partridge, and Naomi Shepherd. “Hot routes: Developing a new technique for the spatial analysis of crime.” Crime Mapping: A Journal of Research and Practice 1, no. 1 (2009): 77-96. For an additional tutorial on creating hot routes in R you can read my blog post here: rekadata.net/blog/hot-routes-tutorial/ While sometimes there may be issues with linking points to routes due to problems such as bad geocoding, as we had discusses in great detail in week 4, there are obvious advantages to considering crime as distributed along networks, rather than continuous space. Now we have some ways of identifying hot spots under our belts! 6.8 Recap Today we learned about spatial point patterns. Specifically we explored how to do the following: we looked at dividing our polygon into quadrants, and counting the number of crimes in each one we used kernel density estimation to look for concentration of crimes in euclidean space saved our kernel density heat map as a raster layer and presented on an interactive map looked at spatial clustering along networks "]]
